title: 'MIT 6.824 - GFS'
tags:
- MIT 6.824
- GFS
---

## 介绍
在 [MapReduce: Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf) 中提到，`MapReduce` 任务的输入输出构建在 `GFS` 之上，`GFS` 是 `Google` 内部开发的一个分布式文件系统，用于应对大型的数据密集型系统。在 `GFS` 之前，业界已经存在了一些分布式文件系统的实现，为什么 `Google` 还要再实现一套？因为基于 `Google` 内部应用的特点，有别于传统的分布式文件系统，除了考虑性能、可扩展性、可靠性和可用性之外，`GFS` 在设计时还考虑了以下三个方面：

1. 组件异常经常出现而不是偶尔出现。`GFS` 构建在成百上千台廉价的机器上，并同时被同等数量的客户端访问。在这个量级规模下，在任何时候某个组件都有可能发生异常以及发生异常后无法自动恢复。这里的异常不只包括硬件的异常，还包括软件的异常以及人为的错误。因此，对于异常的监控和检测，容错，以及异常的自动恢复是系统不可或缺的一个部分。
2. 以传统的分布式文件系统的视角来看，`Google` 要处理的都是大文件，几个G的文件随处可见。
3. 对于 `Google` 的数据应用来说，大部分对文件的写操作是追加操作而不是覆盖操作。对文件的随机写几乎可以说是不存在。文件一旦写入完成后，基本上就不会被再次修改，剩下的都是读操作，且大部分场景下是顺序读。`MapReduce` 系统就是这个应用场景的典型例子，`map` 任务持续顺序追加生成中间结果文件，`reduce` 任务不断的从中间结果文件中顺序读取。根据这个特点，追加写就成为了系统性能优化以及写操作原子性保证的主要设计方向。
4. 将应用程序和文件系统的 `API` 协同设计有利于增加系统的灵活性。例如，`GFS` 提供了原子性的追加写操作，多个客户端可以并发追加写，而无需在应用程序层面进行加锁。

## 设计
### 假设
本节主要描述了 `GFS` 设计的几个出发点：

* 整个系统构建在一批廉价且经常出现异常的硬件上，所以设计上必须考虑对异常的监测和容错，以及异常发生后能够恢复到一个合适的程度。
* 需要能够高效管理几个G的大文件，系统也支持存储小文件，不过不会对其作特殊的优化。
* 系统需要支持两种文件读取方式：大量的流式读和少量的随机读。在大量的流式读场景下，每次读取大小一般在几百KB，或者1MB或更多。来自同一个客户端的连续读取一般是读取文件的某一段连续内容。而少量的随机读一般是在文件的任意位置读取几KB。对于性能敏感的应用程序，一般会将多个随机读根据读取的位置排序后批量读取，避免磁盘来回的随机寻址。
* 系统需要支持大量连续的文件追加写操作。对文件追加写的单次大小一般和流式读的单次大小差不多。文件一旦写入完成后就几乎不会再被修改。系统同样需要支持少量的随机写，不过和少量的随机读类似，随机写也不强调性能。
* 当有多个客户端对同一个文件进行追加写的时候，系统必须能高效的实现清晰明确的执行语义，例如是保证每个客户端至少成功追加写一次，还是至多一次或者其他。`GFS` 中的文件经常会充当某个生产者消费者场景下的队列，一边会有多个客户端不断往文件中追加写，一边也会有一个客户端同时进行流式读取（或在写入完成后读取），所以系统必须保证追加写操作的原子性。
* 整个系统的吞吐的重要性大于延迟，大部分 `Google` 的应用程序主要是大批量的文件处理，只有少量的程序会对单次的读或写有性能要求，所以系统的吞吐是第一位。

### 接口
虽然 `GFS` 并未完全实现标准的文件系统 `API`，如 `POSIX`，但仍提供了常见的文件系统接口，如创建（`create`）、删除（`delete`）、打开（`open`）、关闭（`close`）、读取（`read`）和写入（`write`）文件。类似于本地文件系统，文件在 `GFS` 内通过树状目录的形式组织，每个文件通过文件路径来唯一确定，不过这里的目录是逻辑上的概念，并不会映射到某个物理文件系统目录上。

除此之外，`GFS` 还支持快照（`snapshot`）和追加写（`record append`）的操作。快照能够低成本的复制一个文件或一个目录。追加写允许多个客户端并发的对同一个文件追加写入，并保证每个客户端写入的原子性。

### 架构
一个 `GFS` 集群由一个主节点（`master`）和多个 `chunkserver` 组成，并被多个客户端（`client`）访问。不管是主节点还是 `chunkserver` 或客户端，都运行在廉价的 `Linux` 机器上。可以轻易的将 `chunkserver` 和客户端运行在同一台机器上，如果系统资源允许或者能够容忍客户端代码潜在的不可靠性的话。

每个文件在存入 `GFS` 时会被切分为固定大小的块（`chunk`），每个 `chunk` 在创建时会被主节点分配一个全局不可变的64位 `chunk handle`。`chunkserver` 将 `chunk` 保存在本地文件系统中，每个 `chunk` 对应着本地的一个 `Linux` 文件，客户端通过指定 `chunk handle` 和文件偏移范围来读取或者写入 `chunk`。为了保证可靠性，每个 `chunk` 会复制到多台 `chunkserver` 上。`GFS` 默认为每个 `chunk` 生成3份备份，不过用户也可以为某些命名空间下的文件指定不同的备份数量。

主节点保存了全部的系统元数据，包括文件命名空间、访问控制信息、每个文件和对应 `chunk` 的映射、每个 `chunk` 所在的位置等。同时主节点还负责 `chunk` 的租约管理、不再使用的 `chunk` 的垃圾回收、`chunkserver` 间的 `chunk` 迁移等。主节点也会定期的向 `chunkserver` 发送心跳用于向 `chunkserver` 发送指令和收集 `chunkserver` 当前的状态。

`GFS` 的客户端代码会集成到用户应用程序中，它负责实现文件系统 `API` 以及和主节点、 `chunkserver` 通信来读取或写入文件。`GFS` 客户端通过主节点获取文件的元数据信息，然而文件的读取和写入都只和 `chunkserver` 通信，从而减少了主节点的负担。

不管是 `GFS` 客户端还是 `chunkserver` 都不会缓存文件数据。客户端缓存收益不大是因为 `Google` 大部分的应用程序是对大文件的流式读取或者文件内容过大无法被缓存。不考虑缓存简化了客户端和整个系统的实现，因为引入缓存就要考虑缓存一致性问题。不过客户端会缓存文件的元数据信息，例如某个 `chunk` 的位置信息。`chunkserver` 不缓存是因为 `chunk` 是作为 `Linux` 文件保存在本地文件系统中，操作系统本身已经提供了一层文件访问缓存，没有必要再加一层。

### 单主节点

参考：

* [The Google File System](https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)