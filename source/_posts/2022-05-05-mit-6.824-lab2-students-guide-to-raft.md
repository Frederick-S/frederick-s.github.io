title: "MIT 6.824 - Lab 2 (1): Students' Guide to Raft"
tags:
- MIT 6.824
- Go
- Raft
- Distributed Systems
---

[Students' Guide to Raft](https://thesquareplanet.com/blog/students-guide-to-raft/) 是 [MIT 6.824: Distributed Systems](https://pdos.csail.mit.edu/6.824/) 之前的助教写给学生看的实验生存指南。

## 实现 Raft
论文中的 `Figure 2` 和 `Figure 13` 描述了实现 `Raft` 的主要接口，以及各节点需要维护的状态和响应接口时的行为逻辑，所以在做 `Lab 2` 之前需要先充分理解这部分的内容。文中的建议是**必须**严格落实图中的每一句话，而不是仅仅实现了功能，否则就有可能遇到奇怪的问题或不正确的系统行为。

文中举了三个例子。第一，每次收到 `AppendEntries` 或 `RequestVote` 请求就重置 `election timeout`，因为收到这两个请求说明存在主节点或者正在选主。不过，图中还有描述：

> If election timeout elapses without receiving AppendEntries RPC from current leader or granting vote to candidate: convert to candidate.

重置 `electino timeout` 有两个关键条件：`AppendEntries` 要求发送请求的主节点是当前任期的主节点；`RequestVote` 要求成功投出选票。

先看第一个条件，假设当前任期的主节点异常，此时还有一个之前任期的主节点存活在不断发送 `AppendEntries` 请求，假设此时某个从节点马上要进入选主状态，但由于收到了 `AppendEntries` 请求又没有校验任期就重置了 `election timeout`，造成无主状态时间延长。这个场景下如果过期的主节点不实现下面的逻辑杀伤力倍增：

> If RPC request or response contains term T > currentTerm: set currentTerm = T, convert to follower

如果不实现上面的逻辑，那么过期的主节点会一直是主节点，然后一直重置其他本来可以成为候选节点的从节点。

再来看第二个条件，假设当前任期的主节点异常，各从节点准备进入选主状态，此时一个不可能成为主节点的从节点先变成了候选节点（该节点的日志相对于其他任何一个从节点来说都不够新，即最后一条日志的任期不够大，或者任期相同但是日志条数不够多），`RequestVote` 永远不会成功，如果各从节点一收到 `RequestVote` 就重置 `election timeout`，就会造成始终没有主节点的情况。

第二个例子，从节点收到心跳 `AppendEntries` 后就直接返回 `success = true` 和重置 `election timeout`。重置 `election timeout` 的危害上面已经说过，充当心跳的 `AppendEntries` 是不带日志的请求，文中提到学生在实现时容易将心跳 `AppendEntries` 特殊对待，而不进行 `prevLogIndex` 和 `prevLogTerm` 的校验。从节点返回 `true` 时主节点就会认为从节点的日志和自己匹配，从而错误的认为某些日志已经复制到了从节点上，从而可以提交日志。第一眼看到这里的时候可能会问，为什么主节点收到心跳返回后会提交日志，提交日志不应该是在非心跳 `AppendEntries` 的结果中处理吗？因为主节点会异常，假设主节点将某条日志复制到了过半数的节点，没有来得及提交就异常了，该节点马上恢复后再次被选为主节点，但是它并不知道哪些日志已经被提交了，只能通过 `AppendEntries` 心跳的 `prevLogIndex` 和 `prevLogTerm` 来交换信息，从而知道哪些日志可以提交。所以，如果从节点收到心跳不校验 `prevLogIndex` 和 `prevLogTerm`，那么主节点就会错误的将不该提交的日志提交。

引申开来，假设系统中此时 `x = 1`，有个客户端发送了 `write x = 2` 的请求，主节点成功将日志复制到了过半数的节点上，但是还没有响应客户端就异常了。系统重新选主后，此时有个客户端发送请求 `read x`，`x` 应该返回多少？应该是2，因为在 `Raft` 层面，一条日志是否已提交只取决于是否复制到了过半数的节点上，而不是取决于客户端是否收到响应，而只要日志提交了就可以被应用到状态机。类似的场景可以想象一下平时填了某个表单，提交时提示系统异常，但是刷新页面后表单信息已更新。

第三个例子，学生在实现 `AppendEntries` 时，可能将 `prevLogIndex` 之后的日志都替换为请求中的日志。同样的，图中也有描述：

> If an existing entry conflicts with a new one (same index but different terms), delete the existing entry and all that follow it.

因为，当前的 `AppendEntries` 可能是个过期的请求，假设请求中的日志只是当前从节点已提交的日志的某一段子集，那么如果从节点将 `prevLogIndex` 之后的日志都替换为请求中的日志就会丢失已提交的日志。

## 调试 Raft
调试是实验中最花时间的一个部分，文中列举了四个常见的问题：活锁，不正确或不完整的 `RPC` 实现，没有遵循论文中的 `Rules for Servers` 以及任期混淆。

### 活锁
出现活锁时，等同于各节点都在做无用功。选主是最容易出现活锁的场景，例如始终无法选出一个主节点，或者某个候选节点获得了足够的选票后，马上有另外一个节点发起选主，使得刚成为主节点的节点重新退回到从节点（某个节点成为主节点后，还没来得及发送心跳，刚投了票的某个节点 `election timeout` 到期，发起新的选主，由于它的任期加了1比当前的主节点的任期大，主节点收到 `RequestVote` 后发现自己的任期小，从而转为从节点）。

常见的原因有以下几种：

第一，没有严格按照 `Figure 2` 的要求重置 `election timeout`。重置 `election timeout` 仅限于几种情况：

1. 收到当前任期的主节点的 `AppendEntries` 请求（如果 `AppendEntries` 中的任期比自身的任期小，则忽略该请求）。
2. 开启一轮新的选主，这个属于 `election timeout` 到期，选主的同时需要重置一个新的随机值。
3. 在某轮选举中，将票投给了某个候选节点。

第二，没有按照 `Fiture 2` 的要求开启选主。特别的，如果当前节点是候选节点，但是 `election timeout` 到期了，那么需要开启新的一轮选主。

第三，没有遵循 `Rules for Servers` 的第二条：

> If RPC request or response contains term T > currentTerm: set currentTerm = T, convert to follower (§5.1)

例如，当前节点已投票，按照 `Raft` 的要求，一个从节点在一个任期内只能投票一次，此时又收到了一个 `RequestVote` 请求，请求中的任期大于自身的任期，那么就需要再次投票并更新自身的任期，避免新的候选节点拿不到足够的选票。

### 不正确的 `RPC` 实现
这里总结了学生们过往在实现 `RPC` 接口时容易犯错的点：

* 如果某一步要求 `reply false`，那么这说明需要立即回复 `false` 并返回，不要再执行后续的步骤。
* 如果收到了一条 `AppendEntries` 请求，但是请求中的 `prevLogIndex` 比当前日志的最后一条的索引还要大，也要返回 `false`，这也属于日志不匹配。
* 如前面描述，即使 `AppendEntries` 没有包含任何日志，接收方也需要校验 `prevLogIndex` 和 `prevLogTerm`。
* `AppendEntries` 中的第五步的 `min` 是有必要的，而且是和新日志中的最后一条日志的索引相比较，而不是当前日志的最后一条索引。
  * 首先来看 `leaderCommit < index of last new entry` 的情况，假设 `S1` 为主节点，日志为 `[1, 2, 3, 4, 5]`，`leaderCommit` 为3（索引按从1开始算），`S2` 为从节点，日志为 `[1, 2]`，那么 `S1` 需要将 `[3, 4, 5]` 发给 `S2`，`S2` 收到后 `index of last new entry` 是5，但是 `commitIndex` 只能更新到 `3`，也就是 `leaderCommit`，因为 `[4, 5]` 还没有提交。
  * 然后来看 `leaderCommit > index of last new entry` 的情况，乍看之下好像不可能，因为对于主节点来说，在添加一条新日志的那个时刻，`leaderCommit` 肯定小于新的日志的索引，复制日志到从节点时这个关系也不会变。这里只能想到的是，`leaderCommit` 被并发修改了，因为主节点不会删除日志，所以 `index of last new entry` 不会变，当主节点和从节点的日志不匹配时，主从节点需要来回的就 `prevLogIndex` 和 `prevLogTerm` 达成一致（或者单纯就是因为执行的慢），在这个期间，可能过半数的节点已经达成了完成日志复制，主节点就可以提交这条日志，并更新 `leaderCommit`，所以在原先的 `AppendEntries` 中，如果 `leaderCommit` 引用的是全局变量，而 `entries[]` 是固定的，就会造成 `leaderCommit` 的值比 `entries[]` 的最后一条日志的索引还要大，这个时候从节点的 `commitIndex` 就需要取 `index of last new entry`。文中举了个例子说明为什么这时候不能取 `leaderCommit`，假设 `S1` 为主节点，日志为 `[1, 2, 3, 6]`，`leaderCommit` 为3，`S2` 为从节点，日志为 `[1, 2, 3, 4, 5]`，那么 `S1` 需要将 `[6]` 发给 `S2`，注意这里 `prevLogIndex` 和 `prevLogTerm` 校验通过，而论文中只提到校验不通过时才删除从节点的日志，所以这里就不会删除 `S2` 的 `[4, 5]`，又假设此时 `S1` 通过其他节点的交互将日志成功提交到了 `[1, 2, 3, 6, 7]`，`leaderCommit` 变为了第5个位置，`S2` 收到的 `AppendEntries` 中 `leaderCommit = 5`，`entries = [6]`，如果按照最新的 `leaderCommit` 更新就会造成把 `S2` 日志中的5归为已提交。
* 严格按照论文5.4节的描述来比较两个节点的日志哪个较新，不要只比较日志的长度。

### 未遵循 Rules for Servers
* 如果 `commitIndex > lastApplied`，说明可以应用新的日志到状态机中。应用到状态机的顺序必须严格按照日志的顺序，可以由一个单独的线程顺序执行，也可以由多个线程并行执行，但各线程间必须确保不互相干扰，避免执行覆盖。

## 参考

* [Students' Guide to Raft](https://thesquareplanet.com/blog/students-guide-to-raft/)