title: '【读】RocksDB: Evolution of Development Priorities in a Key-value Store Serving Large-scale Applications'
tags:
- Paper
- RocksDB
---

本文是 `Facebook` 对 `RocksDB` 8年开发历程的回顾，重点讨论了为支持大规模分布式系统所做的开发优先级取舍与演进，以及在生产环境中运行大规模应用的经验。

## 介绍
`RocksDB` 是 `Facebook` 在2012年创建的高性能、`KV` 持久存储引擎，代码衍生自 `Google` 的 `LevelDB`。它针对 `SSD` 的某些特性进行了优化，目标是服务于大型（分布式）应用，在使用上则以类库的方式和上层应用集成。每个 `RocksDB` 实例是个单机版程序，本身不提供跨主机间的操作，例如副本管理和负载均衡，同时也不提供高阶 `API`，例如不支持 `checkpoint`，这些都留给上层应用自行实现。

`RocksDB` 及其高度可定制的组件设计使其能够从容应对不同的业务需求和工作负载。除了作为数据库系统的存储引擎外，`RocksDB` 还被用于如下不同类型的服务：
* 流式处理：典型代表如 `Flink` 借助 `RocksDB` 保存 `checkpoint` 的状态数据
* 日志/队列服务：依托于 `RocksDB` 可定制化的压缩策略，这些服务能够以不亚于追加写单个文件的效率实现高吞吐的写入，同时有着较低的写放大，以及享受内置索引带来的便利
* 索引服务：`RocksDB` 的 `bulk loading` 特性能够为索引服务提供大规模加载离线数据的能力，同时也有着高效的查询性能
* 基于 `SSD` 的二级缓存：因为 `RocksDB` 针对 `SSD` 进行了优化，所以某些内存式的缓存服务会在内存不够时将部分数据置换到 `RocksDB`。这些服务往往要求存储引擎有着足够高的写入速度和优秀的点查询性能

## 背景
`RocksDB` 的设计极大的受到了 `SSD` 特性的影响，`SSD` 不对称的读写性能和有限的耐用性给 `RocksDB` 的数据结构设计和系统架构带来了机遇和挑战。

### 基于 SSD 的嵌入式存储
相比于机械硬盘，`SSD` 读写的 `IOPS` 可以达到十万至百万，读写速度可以达到几百至几千 `MB/s`。一方面，这给如何设计软件从而能充分利用 `SSD` 的性能带来了挑战；另一方面，受限于 `SSD` 有限的擦除次数，同时也需要考虑如何设计合理的数据结构。

正因为 `SSD` 出色的性能，在大多数情况下，应用的性能瓶颈也从设备 `I/O` 转向了网络；应用架构设计时也更倾向于将数据存储在本地 `SSD` 而不是远程存储服务，因此，能够内嵌在应用中的本地 `KV` 存储引擎的需求就日渐上涨。

在这个背景下，`Facebook` 实现了 `RocksDB`，其中 `LSM` 树扮演了重大的角色。

### RocksDB 的架构和 `LSM` 树的使用
`RocksDB` 使用 `LSM` 树作为主要的数据结构来保存数据并支持以下核心的操作。

#### 写
写入时会先将数据写入到名为 `MemTable` 的内存写缓冲中，同时也会在磁盘上记录 `Write Aghead Log (WAL)`。`MemTable` 由跳表（`skiplist`）实现，插入和查询的时间复杂度都是 `O(logn)`。`WAL` 可按需开启，用于 `RocksDB` 从崩溃后恢复数据。当 `MemTable` 的大小达到所配置的值时：
1. 当前接受写入的 `MemTable` 和 `WAL` 变为只读
2. 后续新的写入由新创建的 `MemTable` 和 `WAL` 服务
3. 系统会将变为只读的 `MemTable` 和 `WAL` 的内容落盘到 `Sorted String Table (SSTable)` 内
4. 已落盘的 `MemTable` 和 `WAL` 则可以丢弃

`SSTable` 中的数据按序存储，并以等大小的块（`block`）组织。`SSTable` 生成后同样变为只读，同时，其内部会维护一个索引块并给每个数据块维护一条索引，从而能借助二分查找快速搜索。

#### 压缩
![alt](/images/rocksdb-1.png)

如上图所示，一个 `LSM` 树分为多层。最新的 `SSTable` 由 `MemTable` 刷盘生成，并放置在 `Level-0`。其他层的 `SSTable` 则统一由压缩进程维护。当第 `L` 层的 `SSTable` 大小触及了配置值，压缩进程会选择该层的部分 `SSTable`，并将其和第 `L + 1` 层内键的范围存在重合的 `SSTable` 进行合并，从而在第 `L + 1` 层生成一个新的 `SSTable`。通过这个操作，`RocksDB` 就可以将已删除的数据和过时的数据清除，同时新生成的 `SSTable` 也进行了瘦身，节省了磁盘空间，最终写入的数据会逐渐从 `Level-0` 迁移到最后一层。整个压缩过程的 `IO` 效率也比较高，一方面不同层的压缩可以并行执行，另一方面 `IO` 操作只涉及整个 `SSTable` 文件的批量读和写。

`MemTable` 和 `Level-0` 层的 `SSTable` 键的范围可能会存在重合，而 `Level-1` 及其之后的每一层内，`RocksDB` 会确保各个 `SSTable` 之间键的范围不会重合（但是不同层之间的 `SSTable` 键的范围是有可能重合的）。

#### 读

## 参考
* [RocksDB: Evolution of Development Priorities in a Key-value Store Serving Large-scale Applications](https://research.facebook.com/publications/rocksdb-evolution-of-development-priorities-in-a-key-value-store-serving-large-scale-applications/)
