title: '【读】RocksDB: Evolution of Development Priorities in a Key-value Store Serving Large-scale Applications'
tags:
- Paper
- RocksDB
---

本文是 `Facebook` 对 `RocksDB` 8年开发历程的回顾，重点讨论了为支持大规模分布式系统所做的开发优先级取舍与演进，以及在生产环境中运行大规模应用的经验。

## 介绍
`RocksDB` 是 `Facebook` 在2012年创建的高性能 `KV` 持久存储引擎，代码衍生自 `Google` 的 `LevelDB`。它针对 `SSD` 的某些特性进行了优化，目标是服务于大型（分布式）应用，在使用上则以类库的方式和上层应用集成。每个 `RocksDB` 实例是个单机版程序，本身不提供跨主机间的操作，例如副本管理和负载均衡，同时也不提供高阶 `API`，例如不支持 `checkpoint`，这些都留给上层应用自行实现。

`RocksDB` 及其高度可定制的组件设计使其能够从容应对不同的业务需求和工作负载。除了作为数据库系统的存储引擎外，`RocksDB` 还被用于以下不同类型的服务：
* 流式处理：典型代表如 `Flink` 借助 `RocksDB` 保存 `checkpoint` 的状态数据
* 日志/队列服务：依托于 `RocksDB` 可定制化的合并策略，这些服务能够以不亚于追加写单个文件的效率实现高吞吐的写入，同时有着较低的写放大，以及享受内置索引带来的便利
* 索引服务：`RocksDB` 的 `bulk loading` 特性能够为索引服务提供大规模加载离线数据的能力，同时也有着高效的查询性能
* 基于 `SSD` 的二级缓存：因为 `RocksDB` 针对 `SSD` 进行了优化，所以某些内存式的缓存服务会借助 `RocksDB` 在内存不够时将部分数据置换到 `SSD`。这些服务往往要求存储引擎有着足够高的写入速度和优秀的点查询性能

## 背景
`RocksDB` 的设计极大的受到了 `SSD` 特性的影响，`SSD` 不对称的读写性能和有限的耐用性给 `RocksDB` 的数据结构设计和系统架构带来了机遇和挑战。

### 基于 SSD 的嵌入式存储
相比于机械硬盘，`SSD` 读写的 `IOPS` 可以达到十万至百万，读写速度可以达到几百至几千 `MB/s`。一方面，这给如何设计软件从而能充分利用 `SSD` 的性能带来了挑战；另一方面，受限于 `SSD` 有限的擦除次数，同时也需要考虑如何设计合理的数据结构。

正因为 `SSD` 出色的性能，在大多数情况下，应用的性能瓶颈也从设备 `I/O` 转向了网络；应用架构设计时也更倾向于将数据存储在本地 `SSD` 而不是远程存储服务，因此，能够内嵌在应用中的本地 `KV` 存储引擎的需求就日渐上涨。

在这个背景下，`Facebook` 实现了 `RocksDB`，其中 `LSM` 树扮演了重大的角色。

### RocksDB 的架构和 LSM 树的使用
`RocksDB` 使用 `LSM` 树作为主要的数据结构来保存数据并支持以下核心的操作。

#### 写
写入时会先将数据写入到名为 `MemTable` 的内存写缓冲中，同时也会在磁盘上记录 `Write Aghead Log (WAL)`。`MemTable` 由跳表（`skiplist`）实现，插入和查询的时间复杂度都是 `O(logn)`。`WAL` 可按需开启，用于 `RocksDB` 从崩溃后恢复数据。当 `MemTable` 的大小达到所配置的阈值时：
1. 当前接受写入的 `MemTable` 和 `WAL` 变为只读
2. 后续新的写入由新创建的 `MemTable` 和 `WAL` 服务
3. 系统会将变为只读的 `MemTable` 和 `WAL` 的内容落盘到 `Sorted String Table (SSTable)` 内
4. 已落盘的 `MemTable` 和 `WAL` 则可以丢弃

`SSTable` 中的数据按序存储，并以等大小的块（`block`）组织。`SSTable` 生成后同样变为只读，同时，其内部会维护一个索引块并给每个数据块维护一条索引，从而能借助二分查找快速搜索。

#### 合并
![alt](/images/rocksdb-1.png)

如上图所示，一个 `LSM` 树分为多层。最新的 `SSTable` 由 `MemTable` 刷盘生成，并放置在 `Level-0`。其他层的 `SSTable` 则统一由合并进程维护。当第 `L` 层的 `SSTable` 大小触及了配置值，合并进程会选择该层的部分 `SSTable`，并将其和第 `L + 1` 层内键的范围存在重合的 `SSTable` 进行合并，从而在第 `L + 1` 层生成一个新的 `SSTable`。通过这个操作，`RocksDB` 就可以将已删除的数据和过时的数据清除，同时新生成的 `SSTable` 也进行了瘦身，节省了磁盘空间，最终写入的数据会逐渐从 `Level-0` 迁移到最后一层。整个合并过程的 `I/O` 效率也比较高，一方面不同层的合并可以并行执行，另一方面 `I/O` 操作只涉及整个 `SSTable` 文件的批量读和写。

`MemTable` 和 `Level-0` 层的 `SSTable` 键的范围可能会存在重合，而 `Level-1` 及其之后的每一层内，`RocksDB` 会确保各个 `SSTable` 之间键的范围不会重合（但是不同层之间的 `SSTable` 键的范围是有可能重合的）。

`RocksDB` 支持不同类型的合并：

* `Leveled Compaction`：借鉴自 `LevelDB` 并加以改进。每一层可容纳的文件大小呈指数级放大。系统会积极的触发合并以确保每层的文件大小不会超过指定阈值
* `Tiered Compaction`：在 `RocksDB` 也被称为 `Universal Compactioin`，与 `Apache Cassandra` 或 `HBase` 采取的合并模式类似。当 `Level-0` 层文件的个数或者非 `Level-0` 层的个数超过指定的阈值，又或者整个数据库的大小和最深层文件大小之比超过指定的阈值时，则会触发合并多个 `SSTable`。有别于 `Leveled Compaction`，`Tiered Compaction` 是惰性合并，实际的合并会推迟到读性能或者空间效率发生衰减时进行，从而能够一次性合并更多的数据
* `FIFO Compaction`：当数据库大小触及到指定阈值时，丢弃最老的 `SSTable`，且只进行轻量级的合并。适合于基于内存的缓存应用

`RocksDB` 的读写性能在不同的合并策略下有着不同的表现，应用开发者需要结合自身服务的工作负载选择合适的合并策略。

#### 读
读取时，`RocksDB` 首先在所有的 `MemTable` 中查找，如果没有找到则继续在位于 `Level-0` 层的所有 `SSTable` 中查找，如果还没有找到，则继续向下一层中键的范围包含要查找的键的 `SSTable` 中查找，所有的查找都借助了二分搜索。另外还有两项辅助查找的优化：
1. 频繁被访问的 `SSTable` 块会在内存中缓存从而减少文件 `I/O`，以及解压缩的开销
2. 布隆过滤器用于快速排除一定不包含要查找的键的 `SSTable`

#### Column Family
`RocksDB` 在2014年引入了 `column family` 功能，不同的 `column family` 下可以包含相同的键，每个 `column family` 有独立的 `MemTable` 和 `SStable`，但是共享 `WAL`。其优势在于：

1. 每个 `column family` 可独立配置，如合并，压缩，`merge operators` 以及 `compaction filters`
2. 共享的 `WAL` 能够保证原子写入到多个 `column family`
3. `column family` 可动态高效的删除和创建

## 资源优化目标的演进
### 写放大
`RocksDB` 最初的资源优化目标在于减少 `SSD` 的擦除周期以及写放大，写放大包含两方面：
1. `SSD` 本身的写放大，`SSD` 不能直接覆盖已有的数据，需要先将其擦除，再写入，写入的粒度为 `page`，但是擦除的粒度是 `block`，一个 `block` 包含多个 `page`；同时 `SSD` 的垃圾回收也会造成数据移动和擦除；最后 `SSD` 的 `Wear Leveling` 特性会保证各个 `memory cell` 均衡的写入，也引入了数据移动
2. 数据库软件带来的写放大

在这两个因素下有时候写放大能达到100倍。

`Leveled Compaction` 的写放大倍数基本在10到30，在大多数情况能够数倍优于 `B` 树的实现。更进一步，`Tiered Compaction` 能将写放大倍数降至4到10，不过缺点是读性能会有一定的下降。一般来说，当应用的写负载较高时，可以配合写放大较低的合并策略，而当写负载不高时，则可以采用更激进的合并策略，从而有更好的空间效率和读性能。

### 空间放大
经过了多年的开发后，`RocksDB` 团队认为对于大多数应用来说，空间使用率远比写放大重要，因为这些场景下还没有到触及 `SSD` 本身的限制，不恰当的比喻来说就是：

> 以大多数应用程序的稳定性来说，还远没有到拼操作系统稳定性的地步。

而实际上，应用本身也没有充分利用 `SSD` 提供的读写吞吐，因此这一阶段的优化重心就转移到了磁盘空间上。

由于 `LSM` 树无碎片的数据组织，天然的避免了由于数据碎片带来的磁盘空间浪费。另一方面，`RocksDB` 也引入新的合并策略 `Dynamic Leveled Compaction`，其中 `LSM` 树每一层的大小上限会动态的根据最深层文件的大小调整，而不是固定值。这么做的原因是为了减少 `LSM` 树中无效的数据（已删除和已被覆盖），而和最深层文件大小的比值则可作为无效数据的度量指标。最终的结果也表明相比于 `Leveled Compaction`，`Dynamic Leveled Compaction` 有着更稳定的空间效率。

### CPU 利用率
随着 `SSD` 的发展，一种潜在的担忧是应用程序已不能完全充分利用 `SSD` 的潜能。因此，系统的瓶颈也从设备 `I/O` 转移到了 `CPU`。不过，`RocksDB` 的开发人员不这么看，因为：

1. 只有少部分的应用受限于 `SSD` 的 `IOPS`，大部分应用受限于磁盘空间
2. 一个高端 `CPU` 足够服务于一个高端 `SSD`。在 `Facebook` 的生产环境中还没有遇到 `RocksDB` 不能充分利用 `SSD` 能力的情况。当然，如果一个 `CPU` 服务多个 `SSD` 还是有可能会有 `CPU` 瓶颈的，不过这属于系统配置层面的资源不均衡问题。另一方面，写密集型的应用也有可能存在 `CPU` 瓶颈的问题，不过这可以通过使用更轻量级的合并策略解决。而在这之外的场景，其工作负载则可能不适合使用 `SSD`，因为有可能提前让 `SSD` 的寿命完结

不过，优化 `CPU` 利用率也不等于说是无用功，因为空间放大的优化余地已经不多了。优化了 `CPU` 也等同于省钱，毕竟 `CPU` 和内存的价格也在节节攀升。一些对 `RocksDB` 的 `CPU` 优化的尝试包括前缀布隆过滤器，在查找索引前先用布隆过滤器判断，以及其他的一些布隆过滤器优化。

### 适配新技术
一些 `SSD` 的新技术例如 `open-channel SSDs`，`multi-stream SSDs`，`ZNS` 能让 `SSD` 有着更低的查询延迟及更少的擦除周期损耗。不过，如前面所述，`RocksDB` 的开发团队认为大部分应用的瓶颈在于磁盘空间，适配这些新技术反而会给 `RocksDB` 的一致性体验带来挑战，所以这项的优先级不高。

`In-storage computing` 可能会给应用带来巨大的提升，不过 `RocksDB` 的开发团队目前还不确定 `RocksDB` 能从这项技术中受益多少，而且对 `API` 的改动可能也比较大。

`Disaggregated (remote) storage` 则更具吸引力，并且也是当前的一个优化重点。前文的优化背景都是应用直接访问本地 `SSD`，不过，如今更快的网络带宽使得远程访问 `SSD` 成为了可能，因此，如何优化 `RocksDB` 使其更好的适配远程 `SSD` 也变得有意义。在远程存储模式下，`CPU` 和 `SSD` 资源可以同时做到充分利用以及独立扩展，相反本地 `SSD` 的模式则较难实现。目前 `RocksDB` 的开发团队正在优化远程模式下的 `I/O` 延迟。

最后，`non-volatile memory (NVM)` （它相比于 `SSD` 有着更高的 `IO` 读写吞吐）这项技术也在考量中：

1. 将 `NVM` 作为 `DRAM` 的扩展
   1. 如何实现核心数据结构（`block cache` 还是 `MemTable`）从而结合 `NVM` 和 `DRAM` 使用
   2. 会引入哪些额外的开销
2. 将 `NVM` 作为数据库的主要存储：不过实践表明 `RocksDB` 的瓶颈主要在于磁盘空间或者 `CPU`，而不是 `I/O`
3. 用 `NVM` 保存 `WAL`：其成本是否值得有待考虑，毕竟 `WAL` 中的数据量不大，并且会刷盘到 `SSD`

## 参考
* [RocksDB: Evolution of Development Priorities in a Key-value Store Serving Large-scale Applications](https://research.facebook.com/publications/rocksdb-evolution-of-development-priorities-in-a-key-value-store-serving-large-scale-applications/)
* [Write amplification](https://en.wikipedia.org/wiki/Write_amplification)
