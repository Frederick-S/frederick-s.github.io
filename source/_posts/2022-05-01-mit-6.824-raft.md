title: 'MIT 6.824 - In Search of an Understandable Consensus Algorithm (Extended Version)'
tags:
- MIT 6.824
- Paper
---

## 介绍
共识算法使得一批机器作为一个整体对外提供服务，同时当部分机器异常时系统仍能保证可用性。因此，共识算法在构建可靠的大型软件系统中扮演着至关重要的角色。而在所有的共识算法中，`Paxos` 占据了主导的地位：大部分共识算法的实现都是基于 `Paxos` 或者受其影响，并且它也成为了教授学生们共识算法的第一选择。

然而，尽管人们为了让 `Paxos` 易于理解做了大量的尝试，`Paxos` 依然非常难以理解。另外，如果要将 `Paxos` 应用到实际的系统中需要涉及复杂的修改。因此，系统开发人员和学生都受困于 `Paxos`。

在受困于 `Paxos` 之后，`Raft` 的作者开始尝试设计一种新的共识算法，从而能够为系统构建和教学提供一个更好的基础。和常见的共识算法的设计目标不同，这个新的共识算法的首要设计目标是可理解性：能否为实际的系统设计一个共识算法，并且远比 `Paxos` 易于理解？另外，对于系统构建者来说这个算法需要能易于实现。这个算法仅仅是正确的还不够，重要的是能显而易见的被人们理解为什么是正确的。

这个新的共识算法就是 `Raft`。在设计 `Raft` 时为了提高可理解性作者做了些特定的处理，包括解耦（`Raft` 分离了选主，日志复制和安全性的处理）和减少了状态机的状态（相比于 `Paxos`，`Raft` 减少了非确定性的状态以及各服务器间不一致的情况）。根据两所大学共43名学生的调查反馈，`Raft` 明显比 `Paxos` 易于理解：在学习了两种共识算法后，有33名学生在回答关于 `Raft` 的问题时比回答关于 `Paxos` 的问题表现的更好。

`Raft` 和现今存在的共识算法有很多相之处（特别是 `Oki` 和 `Liskov` 的 `Viewstamped Replication`），不过也有几个方面的创新：

* 强主节点：相比于其他共识算法，`Raft` 使用了更严格的主节点要求。例如，日志只会从主节点发往其他服务器。这简化了日志复制的管理，同时也使得 `Raft` 更易于理解。
* 选主：`Raft` 使用随机计时器来进行选主（后面会提到主节点和各从节点间会维持一个心跳，如果在一段时间内从节点没有收到心跳，那么就可以认为主节点异常，可以重新发起选主，对于每个从节点来说，这个等待的时间不是固定的，而是随机的）。因为心跳本身在共识算法中是一个必不可少的技术，使用随机计时器仅仅在这之上增加了一些额外的机制，却能简单快速的解决选主冲突问题（例如有两个节点瓜分了存活着的节点的全部选票，却没有任何一个节点获得了超过半数的选票，需要重新选主）。
* 节点变更：当集群中的节点需要变更时，`Raft` 使用 `joint consensus` 机制来保证在调整时新旧两套集群下过半数的机器同时属于两套集群。这就保证了整个集群在节点变更时依然正常对外提供服务。

`Raft` 的作者认为不管是出于教学还是实现目的，`Raft` 都优于 `Paxos` 和其他共识算法。它相比于其他共识算法更简单和易于理解；也完全覆盖了实现一个实际系统的需求；它也有了一些开源的实现并且已经被一些公司所使用；它的安全性也已被正式定义和证明；其性能也不输给其他共识算法。

## 复制状态机
谈论共识算法时一般离不开复制状态机（`replicated state machines`）。在这个模型下，集群中的每台机器上的状态机能产生有着相同状态的副本，并且在某些机器异常时整个系统依然能对外提供服务。复制状态机被用于解决分布式系统中的一些列容错问题。例如，对于 `GFS`、`HDFS` 和 `RAMCloud` 这样的单主节点的大型系统来说，一般会用一个独立的复制状态机来管理选主，以及存储某些配置信息，并且主节点发生异常时这些信息也不会丢失。复制状态机的应用包括 `Chubby` 和 `ZooKeeper`。

![alt](/images/raft-1.png)

复制状态机一般通过复制日志来实现。在上图中，每台机器保存的日志中包含了一系列命令，这些命令会被状态机按顺序执行。每份日志中以相同的顺序保存着相同的命令，所以每台状态机能以相同的顺序执行这些命令。因为状态机是确定性的，所以最终所有状态机的状态和输出的结果都是相同的。

共识算法的任务就是要保证这些日志数据的一致性。服务器上的共识模块收到客户端的命令后会将其添加到日志中。然后它会和其他服务器上的共识模块通信来保证即使在某些服务器异常的情况下，各服务器也会以相同的顺序记录下所有的请求日志。当客户端命令被正确复制后，每台服务器上的状态机会以日志中的顺序执行这些命令，然后将结果返回给客户端。从整体上来说，所有的服务器对外组成了一个独立，高可用的状态机。

针对实际系统的共识算法来说一般有以下几个特性：

* 保证在所有非拜占庭情况下的正确性（永远不会返回一个错误的结果给客户端），这里的场景包括网络延迟，网络分区，网络包丢失、重复和重排序等等。
* 只要系统中过半数的服务器依然存活并且相互间以及和客户端间可以通信，整个系统对外来说依然是可用的。因此，一个由五台服务器组成的集群可以容忍任意两台服务器的异常。如果某台服务器停止了则认为是异常，它可能之后会自动恢复并加载持久化存储上的状态然后重新加入到集群中。
* 不依赖时间来保证日志的一致性：错误的时钟和极大的消息延迟在最坏的情况下会造成可用问题。
* 在一般情况下，当集群中过半数的服务器在一轮 `RPC` 请求中成功响应时，这次的客户端请求就被视为完成，剩下少数响应缓慢的服务器不会影响整个系统的性能。

## `Paxos` 的问题
`Leslie Lamport` 的 `Paxos` 协议几乎成为了共识算法的代名词：它是在课堂上被教授的最多的协议，以及大部分的共识算法的实现都以此为出发点。`Paxos` 首先定义了一个协议能够对单一决策达成共识，例如复制某一条日志。这个被称之为 `single-decree Paxos`。然后，`Paxos` 能组合多个单一决策以达成对一系列决策的共识（`multi-Paxos`），例如一整个日志文件。`Paxos` 保证了安全性和存活性，同时支持集群中的节点变更。它的正确性已经被证明而且在常规使用中已足够高效。

不幸的是，`Paxos` 有两个重大的缺点。第一个缺点是 `Paxos` 非常难以理解。它的完整的解释是众所周知的晦涩难懂，只有少数人拼尽全力后才能正确理解。因此，人们尝试用通俗易懂的方式来解释 `Paxos`。不过这些尝试主要针对的是 `single-decree Paxos`，虽然也足够具有挑战性。根据 `NSDI 2012` 与会者的一项非正式调查显示，即使在经验老到的研究者中，也只有少数人能掌握 `Paxos`。`Raft` 的作者自身也受困于 `Paxos`，直到它们读了某些简化的 `Paxos` 的解释和实现了 `Raft` 之后才理解了 `Paxos` 的完整的协议，而这花了几乎一年的时间。

`Raft` 的作者假定 `Paxos` 的晦涩难懂来源于 `Paxos` 选择 `single-decree` 作为其协议的基础。`single-decree Paxos` 难以理解：它被分为两阶段但是又缺少简单直白的解释来单独理解每个阶段。鉴于此，人们也很难理解为什么整个 `single-decree` 协议是正确的。而由 `single-decree Paxos` 组合而来的 `multi-Paxos` 则更添加了额外的复杂性。`Raft` 的作者相信多决策共识的问题能够以更直白的方式拆解。

`Paxos` 的第二个问题是没有为构建实际的系统提供坚实的基础。其中一个原因是还没有一个被广泛认可的 `multi-Paxos` 算法。`Lamport` 的论文中主要描述的是 `single-decree Paxos`；他只是概括性的描述了 `multi-Paxos`，但是缺少很多细节。虽然人们有很多尝试来补充和优化 `Paxos`，但是它们相互之间以及和 `Lamport` 的描述都各有不同。虽然有些系统如 `Chubby` 实现了类似 `Paxos` 的算法，但是实现的算法细节并没有公开。

另外，`Paxos` 的架构对于实际系统的构建来说不够友好，这也是一个将 `single-decree` 分为两阶段后造成的后果。例如，没有必要独立的选择一些日志然后再将其合并为有序的日志，这只会增加复杂性。相比而言，设计一个以日志为中心的系统并且只允许按照指定的顺序来追加写日志会更简单和高效。另一方面，`Paxos` 的核心实现采用了对等点对点的方式（虽然它最终建议一种弱主节点的方式来作为一种性能优化的手段）。这种设计适合于单一决策共识的场景，不过很少有实际的系统采用这种方式。当需要对一系列决策达成共识时，首先选择一个领导者然后由领导者协调做决策会更简单和高效。

因此，实际的系统很少有采用 `Paxos` 的方式。每一种实现都基于 `Paxos`，然后在实现时遇到了困难，接着就演变出了大不相同的架构。这既费时又容易出错，`Paxos` 的难以理解又加剧了这个问题。`Paxos` 的描述可能非常适合证明其正确性，不过实际系统的实现却大相径庭，`Paxos` 的证明也没有什么太大的帮助。来自 `Chubby` 的实现者的评论一针见血的指出了这个问题：

> `Paxos` 的描述和现实世界的系统的实现间存在巨大的鸿沟...最终的系统将会构建在一个没有被证明的协议上。

鉴于以上的问题，`Paxos` 即没有为系统构建也没有为教学提供一个坚实的基础。考虑到共识算法在构建大型软件系统中的重要性，`Raft` 的作者决定尝试能否设计成比 `Paxos` 更优秀的共识算法，`Raft` 因此应运而生。

## 为可理解性设计
作者在设计 `Raft` 时有几个目标：必须为系统构建提供完整坚实的基础，从而能大大减少开发人员的设计工作；必须在任何场景下保证安全性以及在特定操作场景下保证可用性；大多数的操作必须高效。不过最重要也是最困难的是可理解性。它必须能让大部分的受众易于理解。另外，这个算法必须能让人形成直观的认识，系统构建者就可以在实现时进行必要的扩展。

在设计 `Raft` 时有很多方面需要在多种方案中做选择。在选择哪种方案时基于的是可理解性：解释每种方案难度有多大（例如，其内部状态有多复杂），读者完全理解这个方案需要付出多大的努力？

虽然做这样的分析有很大的主观性，作者使用了两方面的手段来解决这个问题。第一个手段是众所周知的问题分解：只要有可能，作者都会先将一个问题分解为一系列独立可解决，可解释，可相对的单独理解的子问题。例如，在 `Raft` 中选主，日志复制，安全性，集群节点变更被分解为独立的模块。

第二个手段是通过减少系统状态的数量来简化系统状态，这样就使得系统更具一致性并尽可能的消除非确定性。特别的，系统中的日志不允许有空洞，`Raft` 也限制了各节点间日志不一致的场景。虽然在大多数情况下会尽可能的消除非确定性，不过在某些场景下非确定性却更有助于理解。特别是随机化会带来非确定性，不过它能以相似的手段来处理所有可能的场景来降低系统状态的复杂性。`Raft` 使用随机化来简化了选主算法。

## `Raft` 共识算法
`Raft` 是一种管理第二节中所描述的复制日志的算法。它在实现共识时会先进行选主，然后完全交由主节点来管理日志的复制。主节点接受来自客户端的日志请求，然后将日志复制到其他从节点上，最后在合适的时机告诉各从节点将日志中的内容应用到自身的状态机中。使用主节点的方式简化了复制日志的管理。例如，主节点可自行决定在哪里插入新的日志而不用和其他服务器交互，其他数据流也是类似的只会从主节点流向从节点。当主节点异常或无法和其他从节点连通时，系统会选举一个新的主节点。

通过主节点的方式，`Raft` 将共识问题分解成了三个相对独立的子问题：

* 选主：当主节点异常时，系统必须选举一个新的主节点。
* 日志复制：主节点必须从客户端接受日志请求，然后将其复制到其他从节点上，并强制要求其他从节点的日志以主节点的为准。
* 安全性：如果任意一台服务器已经将某条日志应用到了自身的状态机中，那么其他任何服务器都不能在这条日志对应的索引上应用不同的命令到状态机中。

### `Raft` 基础
一个 `Raft` 集群包含若干台服务器，5是一个常见的配置，这允许系统最多能容忍两台服务器的异常。在任一时刻，每台服务器只会处于其中一个状态：主节点（`leader`），从节点（`follower`），或者候选节点（`candidate`）。在正常操作下，系统中只有一个主节点，剩下的都是从节点。从节点是被动的：它们不会主动发起任何请求，只是简单的响应来自主节点和候选节点的请求。主节点会处理所有来自客户端的请求（如果客户端将请求发给了一个从节点，从节点会将其转发给主节点）。候选节点这个状态会在选举新的主节点时用到。下图展示了各节点状态间的转换：

![alt](/images/raft-2.png)

如下图所示，`Raft` 将时间切分为任意长度的任期（`term`）。

![alt](/images/raft-3.png)

参考：

* [In Search of an Understandable Consensus Algorithm (Extended Version)](https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf)
* [Raft: Understandable Distributed Consensus](http://thesecretlivesofdata.com/raft/)
* [Students' Guide to Raft](https://thesquareplanet.com/blog/students-guide-to-raft/)