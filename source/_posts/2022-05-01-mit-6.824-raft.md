title: 'MIT 6.824 - In Search of an Understandable Consensus Algorithm (Extended Version)'
tags:
- MIT 6.824
- Paper
---

## 介绍
共识算法使得一批机器作为一个整体对外提供服务，同时当部分机器异常时系统仍能保证可用性。因此，共识算法在构建可靠的大型软件系统中扮演着至关重要的角色。而在所有的共识算法中，`Paxos` 占据了主导的地位：大部分共识算法的实现都是基于 `Paxos` 或者受其影响，并且它也成为了教授学生们共识算法的第一选择。

然而，尽管人们为了让 `Paxos` 易于理解做了大量的尝试，`Paxos` 依然非常难以理解。另外，如果要将 `Paxos` 应用到实际的系统中需要涉及复杂的修改。因此，系统开发人员和学生都受困于 `Paxos`。

在受困于 `Paxos` 之后，`Raft` 的作者开始尝试设计一种新的共识算法，从而能够为系统构建和教学提供一个更好的基础。和常见的共识算法的设计目标不同，这个新的共识算法的首要设计目标是可理解性：能否为实际的系统设计一个共识算法，并且远比 `Paxos` 易于理解？另外，对于系统构建者来说这个算法需要能易于实现。这个算法仅仅是正确的还不够，重要的是能显而易见的被人们理解为什么是正确的。

这个新的共识算法就是 `Raft`。在设计 `Raft` 时为了提高可理解性作者做了些特定的处理，包括解耦（`Raft` 分离了选主，日志复制和安全性的处理）和减少了状态机的状态（相比于 `Paxos`，`Raft` 减少了非确定性的状态以及各服务器间不一致的情况）。根据两所大学共43名学生的调查反馈，`Raft` 明显比 `Paxos` 易于理解：在学习了两种共识算法后，有33名学生在回答关于 `Raft` 的问题时比回答关于 `Paxos` 的问题表现的更好。

`Raft` 和现今存在的共识算法有很多相之处（特别是 `Oki` 和 `Liskov` 的 `Viewstamped Replication`），不过也有几个方面的创新：

* 强主节点：相比于其他共识算法，`Raft` 使用了更严格的主节点要求。例如，日志只会从主节点发往其他服务器。这简化了日志复制的管理，同时也使得 `Raft` 更易于理解。
* 选主：`Raft` 使用随机计时器来进行选主（后面会提到主节点和各从节点间会维持一个心跳，如果在一段时间内从节点没有收到心跳，那么就可以认为主节点异常，可以重新发起选主，对于每个从节点来说，这个等待的时间不是固定的，而是随机的）。因为心跳本身在共识算法中是一个必不可少的技术，使用随机计时器仅仅在这之上增加了一些额外的机制，却能简单快速的解决选主冲突问题（例如有两个节点瓜分了存活着的节点的全部选票，却没有任何一个节点获得了超过半数的选票，需要重新选主）。
* 节点变更：当集群中的节点需要变更时，`Raft` 使用 `joint consensus` 机制来保证在调整时新旧两套集群下过半数的机器同时属于两套集群。这就保证了整个集群在节点变更时依然正常对外提供服务。

`Raft` 的作者认为不管是出于教学还是实现目的，`Raft` 都优于 `Paxos` 和其他共识算法。它相比于其他共识算法更简单和易于理解；也完全覆盖了实现一个实际系统的需求；它也有了一些开源的实现并且已经被一些公司所使用；它的安全性也已被正式定义和证明；其性能也不输给其他共识算法。

## 复制状态机
谈论共识算法时一般离不开复制状态机（`replicated state machines`）。在这个模型下，集群中的每台机器上的状态机能产生有着相同状态的副本，并且在某些机器异常时整个系统依然能对外提供服务。复制状态机被用于解决分布式系统中的一些列容错问题。例如，对于 `GFS`、`HDFS` 和 `RAMCloud` 这样的单主节点的大型系统来说，一般会用一个独立的复制状态机来管理选主，以及存储某些配置信息，并且主节点发生异常时这些信息也不会丢失。复制状态机的应用包括 `Chubby` 和 `ZooKeeper`。

![alt](/images/raft-1.png)

复制状态机一般通过复制日志来实现。在上图中，每台机器保存的日志中包含了一系列命令，这些命令会被状态机按顺序执行。每份日志中以相同的顺序保存着相同的命令，所以每台状态机能以相同的顺序执行这些命令。因为状态机是确定性的，所以最终所有状态机的状态和输出的结果都是相同的。

共识算法的任务就是要保证这些日志数据的一致性。服务器上的共识模块收到客户端的命令后会将其添加到日志中。然后它会和其他服务器上的共识模块通信来保证即使在某些服务器异常的情况下，各服务器也会以相同的顺序记录下所有的请求日志。当客户端命令被正确复制后，每台服务器上的状态机会以日志中的顺序执行这些命令，然后将结果返回给客户端。从整体上来说，所有的服务器对外组成了一个独立，高可用的状态机。

针对实际系统的共识算法来说一般有以下几个特性：

* 保证在所有非拜占庭情况下的正确性（永远不会返回一个错误的结果给客户端），这里的场景包括网络延迟，网络分区，网络包丢失、重复和重排序等等。
* 只要系统中过半数的服务器依然存活并且相互间以及和客户端间可以通信，整个系统对外来说依然是可用的。因此，一个由五台服务器组成的集群可以容忍任意两台服务器的异常。如果某台服务器停止了则认为是异常，它可能之后会自动恢复并加载持久化存储上的状态然后重新加入到集群中。
* 不依赖时间来保证日志的一致性：错误的时钟和极大的消息延迟在最坏的情况下会造成可用问题。
* 在一般情况下，当集群中过半数的服务器在一轮 `RPC` 请求中成功响应时，这次的客户端请求就被视为完成，剩下少数响应缓慢的服务器不会影响整个系统的性能。

## `Paxos` 的问题
`Leslie Lamport` 的 `Paxos` 协议几乎成为了共识算法的代名词：它是在课堂上被教授的最多的协议，以及大部分的共识算法的实现都以此为出发点。`Paxos` 首先定义了一个协议能够对单一决策达成共识，例如复制某一条日志。这个被称之为 `single-decree Paxos`。然后，`Paxos` 能组合多个单一决策以达成对一系列决策的共识（`multi-Paxos`），例如一整个日志文件。`Paxos` 保证了安全性和存活性，同时支持集群中的节点变更。它的正确性已经被证明而且在常规使用中已足够高效。

不幸的是，`Paxos` 有两个重大的缺点。第一个缺点是 `Paxos` 非常难以理解。它的完整的解释是众所周知的晦涩难懂，只有少数人拼尽全力后才能正确理解。因此，人们尝试用通俗易懂的方式来解释 `Paxos`。不过这些尝试主要针对的是 `single-decree Paxos`，虽然也足够具有挑战性。根据 `NSDI 2012` 与会者的一项非正式调查显示，即使在经验老到的研究者中，也只有少数人能掌握 `Paxos`。`Raft` 的作者自身也受困于 `Paxos`，直到它们读了某些简化的 `Paxos` 的解释和实现了 `Raft` 之后才理解了 `Paxos` 的完整的协议，而这花了几乎一年的时间。

`Raft` 的作者假定 `Paxos` 的晦涩难懂来源于 `Paxos` 选择 `single-decree` 作为其协议的基础。`single-decree Paxos` 难以理解：它被分为两阶段但是又缺少简单直白的解释来单独理解每个阶段。鉴于此，人们也很难理解为什么整个 `single-decree` 协议是正确的。而由 `single-decree Paxos` 组合而来的 `multi-Paxos` 则更添加了额外的复杂性。`Raft` 的作者相信多决策共识的问题能够以更直白的方式拆解。

`Paxos` 的第二个问题是没有为构建实际的系统提供坚实的基础。其中一个原因是还没有一个被广泛认可的 `multi-Paxos` 算法。`Lamport` 的论文中主要描述的是 `single-decree Paxos`；他只是概括性的描述了 `multi-Paxos`，但是缺少很多细节。虽然人们有很多尝试来补充和优化 `Paxos`，但是它们相互之间以及和 `Lamport` 的描述都各有不同。虽然有些系统如 `Chubby` 实现了类似 `Paxos` 的算法，但是实现的算法细节并没有公开。

另外，`Paxos` 的架构对于实际系统的构建来说不够友好，这也是一个将 `single-decree` 分为两阶段后造成的后果。例如，没有必要独立的选择一些日志然后再将其合并为有序的日志，这只会增加复杂性。相比而言，设计一个以日志为中心的系统并且只允许按照指定的顺序来追加写日志会更简单和高效。另一方面，`Paxos` 的核心实现采用了对等点对点的方式（虽然它最终建议一种弱主节点的方式来作为一种性能优化的手段）。这种设计适合于单一决策共识的场景，不过很少有实际的系统采用这种方式。当需要对一系列决策达成共识时，首先选择一个领导者然后由领导者协调做决策会更简单和高效。

因此，实际的系统很少有采用 `Paxos` 的方式。每一种实现都基于 `Paxos`，然后在实现时遇到了困难，接着就演变出了大不相同的架构。这既费时又容易出错，`Paxos` 的难以理解又加剧了这个问题。`Paxos` 的描述可能非常适合证明其正确性，不过实际系统的实现却大相径庭，`Paxos` 的证明也没有什么太大的帮助。来自 `Chubby` 的实现者的评论一针见血的指出了这个问题：

> `Paxos` 的描述和现实世界的系统的实现间存在巨大的鸿沟...最终的系统将会构建在一个没有被证明的协议上。

鉴于以上的问题，`Paxos` 即没有为系统构建也没有为教学提供一个坚实的基础。考虑到共识算法在构建大型软件系统中的重要性，`Raft` 的作者决定尝试能否设计成比 `Paxos` 更优秀的共识算法，`Raft` 因此应运而生。

## 为可理解性设计
作者在设计 `Raft` 时有几个目标：必须为系统构建提供完整坚实的基础，从而能大大减少开发人员的设计工作；必须在任何场景下保证安全性以及在特定操作场景下保证可用性；大多数的操作必须高效。不过最重要也是最困难的是可理解性。它必须能让大部分的受众易于理解。另外，这个算法必须能让人形成直观的认识，系统构建者就可以在实现时进行必要的扩展。

在设计 `Raft` 时有很多方面需要在多种方案中做选择。在选择哪种方案时基于的是可理解性：解释每种方案难度有多大（例如，其内部状态有多复杂），读者完全理解这个方案需要付出多大的努力？

虽然做这样的分析有很大的主观性，作者使用了两方面的手段来解决这个问题。第一个手段是众所周知的问题分解：只要有可能，作者都会先将一个问题分解为一系列独立可解决，可解释，可相对的单独理解的子问题。例如，在 `Raft` 中选主，日志复制，安全性，集群节点变更被分解为独立的模块。

第二个手段是通过减少系统状态的数量来简化系统状态，这样就使得系统更具一致性并尽可能的消除非确定性。特别的，系统中的日志不允许有空洞，`Raft` 也限制了各节点间日志不一致的场景。虽然在大多数情况下会尽可能的消除非确定性，不过在某些场景下非确定性却更有助于理解。特别是随机化会带来非确定性，不过它能以相似的手段来处理所有可能的场景来降低系统状态的复杂性。`Raft` 使用随机化来简化了选主算法。

## `Raft` 共识算法
`Raft` 是一种管理第二节中所描述的复制日志的算法，下表描述了该算法的关键特性：

* `Election Safety`：在任一任期内最多只有一个节点被选为主节点。
* `Leader Append-Only`：主节点永远不会覆盖或者删除某些日志项，它只会追加写新的日志项。
* `Log Matching`：如果两份日志在同一索引处的日志项对应相同的任期，那么双方在这个索引之前的日志项都相同。
* `Leader Completeness`：如果某个日志项在某个任期内被提交了，那么这条日志会出现在后续所有新任期下的主节点的日志中。
* `State Machine Safety`：如果某台服务器将某个索引位置的日志应用到了自身的状态机中，那么不会有任何一台服务器在相同索引位置应用了一条不同的日志到状态机中。

`Raft` 在实现共识时会先进行选主，然后完全交由主节点来管理日志的复制。主节点接受来自客户端的日志请求，然后将日志复制到其他从节点上，最后在合适的时机告诉各从节点将日志中的内容应用到自身的状态机中。使用主节点的方式简化了复制日志的管理。例如，主节点可自行决定在哪里插入新的日志而不用和其他服务器交互，其他数据流也是类似的只会从主节点流向从节点。当主节点异常或无法和其他从节点连通时，系统会选举一个新的主节点。

通过主节点的方式，`Raft` 将共识问题分解成了三个相对独立的子问题：

* 选主：当主节点异常时，系统必须选举一个新的主节点。
* 日志复制：主节点必须从客户端接受日志请求，然后将其复制到其他从节点上，并强制要求其他从节点的日志以主节点的为准。
* 安全性：如果任意一台服务器已经将某条日志应用到了自身的状态机中，那么其他任何服务器都不能在这条日志对应的索引上应用不同的命令到状态机中。

### `Raft` 基础
一个 `Raft` 集群包含若干台服务器，5是一个常见的配置，这允许系统最多能容忍两台服务器的异常。在任一时刻，每台服务器只会处于其中一个状态：主节点（`leader`），从节点（`follower`），或者候选节点（`candidate`）。在正常操作下，系统中只有一个主节点，剩下的都是从节点。从节点是被动的：它们不会主动发起任何请求，只是简单的响应来自主节点和候选节点的请求。主节点会处理所有来自客户端的请求（如果客户端将请求发给了一个从节点，从节点会将其转发给主节点）。候选节点这个状态会在选举新的主节点时用到。下图展示了各节点状态间的转换：

![alt](/images/raft-2.png)

如下图所示，`Raft` 将时间切分为任意长度的任期（`term`）。任期会以连续的整数来标记。每个任期以选举作为开始，一个或多个候选节点会尝试成为主节点。如果某个候选节点赢得了选举，那么在这个任期剩下的时间里它将作为主节点。在某些情况下，多个候选节点可能会分票，造成没有一个候选节点赢得半数的选票。在这种情况下，当前任期会以没有主节点的状态结束，接着系统会马上对新的任期发起新的一轮选举。`Raft` 保证在任一任期内至多只有一个主节点。

![alt](/images/raft-3.png)

不同的服务器可能会观测到不同次数的任期转变，在某些情况下一台服务器可能观测不到某次选举的发生或者感知不到整个任期。任期在 `Raft` 中扮演了逻辑时钟的角色，它能允许服务器侦测过期的信息例如过期的主节点。每台服务器都会保存一个当前任期（`current term`）的数字，这个数字会随时间递增。在服务器间通信时双方会附加上当前任期，如果某台服务器的当前任期小于另外一台服务器，那么这个服务器会将当前任期更新为另一台服务器的值。如果某个候选节点或者主节点发现自己的当前任期过期了，那么它会马上转为从节点状态。如果某台服务器收到的请求中的任期过期了，那么它会拒绝这个请求。

`Raft` 服务器间通过 `RPC` 进行通信，基础的共识算法只需要两种 `RPC`。`RequestVote` 用于候选节点在选主期间获取选票，`AppendEntries` 用于主节点向各从节点复制日志以及充当心跳的作用。如果某个 `RPC` 请求在一段时间内没有响应，那么服务器会重新发起请求，同时服务器也会并行发送 `RPC` 请求来达到最佳性能。

### 选主
`Raft` 通过心跳机制来触发选主。当服务器启动时，它们的初始状态是从节点。只要服务器能收到来自候选节点或者主节点的有效请求，就会一直出于从节点状态。主节点会定期的向所有从节点发生心跳（不带任何日志的 `AppendEntries` 请求）来维持自己主节点的地位。如果在某段时间内某台从节点没有收到心跳，那么它就会认为此时没有存活的主节点，则会发起选主来选择一个新的主节点，这个等待时间就叫做 `election timeout`。

开始新的一轮选主时，从节点会先将当前任期递增然后转换为候选节点。接着，它会给自己投一票然后并行发起 `RequestVote` 请求给其他从节点来获取选票。一个候选节点会保持当前的状态直到下面三种情况之一发生：

1. 当前候选节点赢得了选举
2. 有另外一个候选节点赢得了选举
3. 一段时间之后没有一个候选节点赢得选举

当候选节点获得了集群中针对某个任期的过半数节点的选票时就赢得了选举。在某个任期内，每台服务器只会最多给一个候选节点投票，以先来后到为准。过半数的选票保证了在某个任期内最多只会有一个候选节点被选举为主节点（`Election Safety` 特性）。当某个候选节点赢得选举时，它就成为了主节点。然后它就开始向其他服务器发送心跳来维持主节点的状态并阻止其他节点继续发起选主。

候选节点在等待选票时有可能收到其他自认为是主节点的 `AppendEntries` 的请求。如果请求中的任期号不小于当前候选节点记录的任期号，则该候选节点会将此主节点作为主节点并转为从节点状态。如果请求中的任期号小于当前候选节点记录的任期号，则该候选节点会拒绝此请求并继续处于候选节点状态。

第三种情况是没有一个候选节点赢得了选举：当很多从节点在同一时间转变为候选节点时，会分散选票，最终造成没有一个候选节点赢得过半数的选票。当发生这种情况时，候选节点会将此次选主作超时处理，然后将当前任期自增，重新发起新的任期的选主，并向其他从节点继续发起一轮 `RequestVote` 请求。不过，如果缺少额外机制，分票可能会一直持续下去。

`Raft` 通过随机的 `election timeout` 来确保分票极少会发生并且在发生时能快速解决。为了避免分票，首先 `election timeout` 的值会在一个固定区间内随机选择（例如150-300ms）。这就分散了各从节点的选主启动时机，使得在大多数的情况下只有一个从节点会进入选主状态；在其他从节点进入选主状态之前，这个节点就已经赢得了选举并向其他从节点发送了心跳，这就大大扼杀了分票的可能性。同样的机制也被用来解决当分票确实发生的场景，每个候选节点在启动新的选主时会重新设置一个随机的 `election timeout`，然后在这段期间内等待其他从节点的选票，或者新的主节点的心跳，假设第一轮选主发生了分票，那么由于随机 `election timeout` 的存在，不同的候选节点进入第二轮选主的时机也不会相同，这就降低了第二轮选主继续发生分票的可能性。

选主是一个很好的例子展示了可理解性这个设计目标如何来指导在不同设计方案中做出选择。在最初的方案中作者打算使用一个排序系统：每一个候选节点被分配了一个唯一的权重，用来在多个候选节点中选择最终的主节点。如果某个候选节点发现其他候选节点的权重比自己高，那么这个候选节点就会退回到从节点状态，这就使得有着更高权重的候选节点能更容易的赢得下一轮的选举。不过这种实现可能会造成难以察觉的可用性问题（在不采用随机 `election timeout` 的情况下，当某个高权重的候选节点异常时，由于低权重的候选节点已经退回到了从节点状态，它需要再等待一个 `election timeout` 周期才能再次转变为候选节点，而在正常的情况下本身各节点间的信息交换速度较快，此时高权重的候选节点异常可能会造成系统没有候选节点，而距离各从节点进入选主状态又还有较长时间，从而造成系统在这段期间的不可用）。虽然作者对该算法进行了多次调整，但是每次调整后都会出现新的边界问题。最终作者认为随机化的 `election timeout` 更胜一筹和易于理解。

### 日志复制
当某个候选节点被选为主节点后，它就开始处理客户端请求。每个客户端请求中包含了复制状态机需要执行的命令。主节点将这个命令以日志的形式追加到自己的日志文件中，然后并行的给所有从节点发送 `AppendEntries` 请求来复制日志。当日志在各从节点上被安全的复制后，主节点就将日志对应的命令应用到自身的状态机中，并将结果返回给客户端。如果某个从节点异常或者运行缓慢，或者网络包丢失，主节点会一直重发 `AppendEntries` 请求（即使主节点已经将结果返回给了客户端），直到所有从节点保存了所有的日志。

日志内容的组织如下图所示，每一条日志包含了状态机需要执行的命令以及主节点收到该请求时对应的任期。日志中的任期信息用来检测日志间的不一致性以及保证前面所提到的 `Raft` 的几个关键特性。每条日志同时有一个索引信息来标记这条日志在日志文件中的位置。

![alt](/images/raft-4.png)

参考：

* [In Search of an Understandable Consensus Algorithm (Extended Version)](https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf)
* [Raft: Understandable Distributed Consensus](http://thesecretlivesofdata.com/raft/)
* [Students' Guide to Raft](https://thesquareplanet.com/blog/students-guide-to-raft/)