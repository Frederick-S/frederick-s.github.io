title: 'MIT 6.824 - GFS'
tags:
- MIT 6.824
- GFS
---

## 介绍
在 [MapReduce: Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf) 中提到，`MapReduce` 任务的输入输出构建在 `GFS` 之上，`GFS` 是 `Google` 内部开发的一个分布式文件系统，用于应对大型的数据密集型系统。在 `GFS` 之前，业界已经存在了一些分布式文件系统的实现，为什么 `Google` 还要再实现一套？因为基于 `Google` 内部应用的特点，有别于传统的分布式文件系统，除了考虑性能、可扩展性、可靠性和可用性之外，`GFS` 在设计时还考虑了以下三个方面：

1. 组件异常经常出现而不是偶尔出现。`GFS` 构建在成百上千台廉价的机器上，并同时被同等数量的客户端访问。在这个量级规模下，在任何时候某个组件都有可能发生异常以及发生异常后无法自动恢复。这里的异常不只包括硬件的异常，还包括软件的异常以及人为的错误。因此，对于异常的监控和检测，容错，以及异常的自动恢复是系统不可或缺的一个部分。
2. 以传统的分布式文件系统的视角来看，`Google` 要处理的都是大文件，几个G的文件随处可见。
3. 对于 `Google` 的数据应用来说，大部分对文件的写操作是追加操作而不是覆盖操作。对文件的随机写几乎可以说是不存在。文件一旦写入完成后，基本上就不会被再次修改，剩下的都是读操作，且大部分场景下是顺序读。`MapReduce` 系统就是这个应用场景的典型例子，`map` 任务持续顺序追加生成中间结果文件，`reduce` 任务不断的从中间结果文件中顺序读取。根据这个特点，追加写就成为了系统性能优化以及写操作原子性保证的主要设计方向。
4. 将应用程序和文件系统的 `API` 协同设计有利于增加系统的灵活性。例如，`GFS` 提供了原子性的追加写操作，多个客户端可以并发追加写，而无需在应用程序层面进行加锁。

## 设计
### 假设
本节主要描述了 `GFS` 设计的几个出发点：

* 整个系统构建在一批廉价且经常出现异常的硬件上，所以设计上必须考虑对异常的监测和容错，以及异常发生后能够恢复到一个合适的程度。
* 需要能够高效管理几个G的大文件，系统也支持存储小文件，不过不会对其作特殊的优化。
* 系统需要支持两种文件读取方式：大量的流式读和少量的随机读。在大量的流式读场景下，每次读取大小一般在几百KB，或者1MB或更多。来自同一个客户端的连续读取一般是读取文件的某一段连续内容。而少量的随机读一般是在文件的任意位置读取几KB。对于性能敏感的应用程序，一般会将多个随机读根据读取的位置排序后批量读取，避免磁盘来回的随机寻址。
* 系统需要支持大量连续的文件追加写操作。对文件追加写的单次大小一般和流式读的单次大小差不多。文件一旦写入完成后就几乎不会再被修改。系统同样需要支持少量的随机写，不过和少量的随机读类似，随机写也不强调性能。
* 当有多个客户端对同一个文件进行追加写的时候，系统必须能高效的实现清晰明确的执行语义，例如是保证每个客户端至少成功追加写一次，还是至多一次或者其他。`GFS` 中的文件经常会充当某个生产者消费者场景下的队列，一边会有多个客户端不断往文件中追加写，一边也会有一个客户端同时进行流式读取（或在写入完成后读取），所以系统必须保证追加写操作的原子性。
* 整个系统的吞吐的重要性大于延迟，大部分 `Google` 的应用程序主要是大批量的文件处理，只有少量的程序会对单次的读或写有性能要求，所以系统的吞吐是第一位。

### 接口
虽然 `GFS` 并未完全实现标准的文件系统 `API`，如 `POSIX`，但仍提供了常见的文件系统接口，如创建（`create`）、删除（`delete`）、打开（`open`）、关闭（`close`）、读取（`read`）和写入（`write`）文件。类似于本地文件系统，文件在 `GFS` 内通过树状目录的形式组织，每个文件通过文件路径来唯一确定，不过这里的目录是逻辑上的概念，并不会映射到某个物理文件系统目录上。

除此之外，`GFS` 还支持快照（`snapshot`）和追加写（`record append`）的操作。快照能够低成本的复制一个文件或一个目录。追加写允许多个客户端并发的对同一个文件追加写入，并保证每个客户端写入的原子性。

### 架构
一个 `GFS` 集群由一个主节点（`master`）和多个 `chunkserver` 组成，并被多个客户端（`client`）访问。不管是主节点还是 `chunkserver` 或客户端，都运行在廉价的 `Linux` 机器上。可以轻易的将 `chunkserver` 和客户端运行在同一台机器上，如果系统资源允许或者能够容忍客户端代码潜在的不可靠性的话。

每个文件在存入 `GFS` 时会被切分为固定大小的块（`chunk`），每个 `chunk` 在创建时会被主节点分配一个全局不可变的64位 `chunk handle`。`chunkserver` 将 `chunk` 保存在本地文件系统中，每个 `chunk` 对应着本地的一个 `Linux` 文件，客户端通过指定 `chunk handle` 和文件偏移范围来读取或者写入 `chunk`。为了保证可靠性，每个 `chunk` 会复制到多台 `chunkserver` 上。`GFS` 默认为每个 `chunk` 生成3份备份，不过用户也可以为某些命名空间下的文件指定不同的备份数量。

主节点保存了全部的系统元数据，包括文件命名空间、访问控制信息、每个文件和对应 `chunk` 的映射、每个 `chunk` 所在的位置等。同时主节点还负责 `chunk` 的租约管理、不再使用的 `chunk` 的垃圾回收、`chunkserver` 间的 `chunk` 迁移等。主节点也会定期的向 `chunkserver` 发送心跳用于向 `chunkserver` 发送指令和收集 `chunkserver` 当前的状态。

`GFS` 的客户端代码会集成到用户应用程序中，它负责实现文件系统 `API` 以及和主节点、 `chunkserver` 通信来读取或写入文件。`GFS` 客户端通过主节点获取文件的元数据信息，然而文件的读取和写入都只和 `chunkserver` 通信，从而减少了主节点的负担。

不管是 `GFS` 客户端还是 `chunkserver` 都不会缓存文件数据。客户端缓存收益不大是因为 `Google` 大部分的应用程序是对大文件的流式读取或者文件内容过大无法被缓存。不考虑缓存简化了客户端和整个系统的实现，因为引入缓存就要考虑缓存一致性问题。不过客户端会缓存文件的元数据信息，例如某个 `chunk` 的位置信息。`chunkserver` 不缓存是因为 `chunk` 是作为 `Linux` 文件保存在本地文件系统中，操作系统本身已经提供了一层文件访问缓存，没有必要再加一层。

### 单主节点
单主节点同样简化了整个系统的设计，方便主节点高效管理 `chunk`，因为所有需要的信息都保存在主节点内存中。为了避免对文件的读写使得主节点成为瓶颈，客户端读写文件时直接和 `chunkserver` 通信而不会经过主节点中转，不过在开始读写前，客户端会先和主节点通信获取需要的 `chunk` 对应的 `chunkserver` 列表，并将这个数据缓存一段时间来减少后续和主节点的通信。

结合下面的流程图我们来看一下一次读操作是如何执行的：

![alt](/images/gfs1.png)

1. 客户端根据固定的每个 `chunk` 的大小和想要读取的文件内容偏移量，计算出 `chunk` 的索引。
2. 客户端将文件名和想要读取的 `chunk` 的索引发给主节点，主节点收到请求后返回对应的 `chunk handle` 和保存了该份 `chunk` 的 `chunkserver` 列表。
3. 客户端收到主节点返回结果后，以文件名和 `chunk` 的索引组合作为键，将返回结果缓存起来。
4. 客户端将 `chunk handle` 和想要读取的文件内容偏移范围发给其中一台 `chunkserver`，一般来说，客户端会选择距离最近的 `chunkserver`，由于缓存了 `chunkserver` 列表信息，后续对同一个 `chunk` 的读请求就不需要再次和主节点通信，直到缓存过期或者文件被再次打开。
5. 一般来说，客户端和主节点通信时会在一次请求中要求多个 `chunk` 的信息，主节点同样也可在一次响应中返回多个 `chunk` 的信息，这也有利于减少客户端和主节点的通信次数。

### `chunk` 大小
每个 `chunk` 的大小是一个关键的设计选择。`GFS` 的 `chunk` 大小为64MB，远大于一般文件系统的数据块的大小。每个 `chunk` 以 `Linux` 文件的形式保存在 `chunkserver` 上，但其实际占用的大小是动态增长的，从而避免了大量的文件碎片（例如实际只需要十几K的文件却固定分配了64MB的大小）。

那么，`GFS` 为什么要选择64MB这样较大的数呢？

1. `chunk` 的大小越大，每个文件拆分成的 `chunk` 的数量就越少，客户端需要读取或写入的 `chunk` 数量也就越少，和主节点通信的次数也就越少。对于 `Google` 的数据应用来说，大部分都是顺序读写，客户端和主节点交互的次数是线性的时间复杂度，减少 `chunk` 的总个数有助于降低整个的时间复杂度。即使是对于少数的随机写，由于 `chunk` 大小足够大，客户端也能够将TB级别的数据对应的所有 `chunk` 的信息缓存起来。
2. 一个 `chunk` 的大小越大，那么客户端对其的可操作次数就越多，客户端就可以和某个 `chunkserver` 维持一段较长时间的 `TCP` 连接，减少和不同的 `chunkserver` 建立连接的次数。
3. 主节点需要维护每个 `chunk` 对应的 `chunkserver` 列表，这个也是个线性的空间复杂度，较大的 `chunk` 大小能减少 `chunk` 的总个数，从而减少元数据的大小，主节点就可以将所有的元数据放在内存中。

另一方面，即使将 `chunk` 的大小设置的较大，以及采用了懒分配的策略，也依然存在缺点。对于一个小文件来说，它可能只有几个甚至是一个 `chunk`，如果这个文件被访问的频率较大，它就有可能成为热点数据，保存了对应的 `chunk` 的 `chunkserver` 就要承受更大的流量。不过在 `Google` 的实际应用场景中，热点数据并没有成为一个大问题，因为大部分的系统面向的是大文件的流式读取，流量能较平均的分发到各个 `chunkserver`。

不过，`Google` 确实有遇到过一个热点数据问题，有个批处理系统会先将一个可执行文件写入到 `GFS` 中，这个可执行文件的大小对应了一个 `chunk`，然后所有客户端会同时读取这个可执行文件并执行，这就造成了几台 `chunkserver` 同时承接了大量的请求。`Google` 通过两个方面来解决这个问题，第一针对这个文件设置一个较大的备份数量，让更多的 `chunkserver` 来分散流量，即水平扩展；第二交替启动批处理系统的客户端，避免同一时间的大量请求。另一种长期的解决方案是在这种场景下允许客户端从其他已读取完毕的客户端那读取文件。

参考：

* [The Google File System](https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)