title: 'MIT 6.824 - GFS'
tags:
- MIT 6.824
- GFS
---

## 介绍
在 [MapReduce: Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf) 中提到，`MapReduce` 任务的输入输出构建在 `GFS` 之上，`GFS` 是 `Google` 内部开发的一个分布式文件系统，用于应对大型的数据密集型系统。在 `GFS` 之前，业界已经存在了一些分布式文件系统的实现，为什么 `Google` 还要再实现一套？因为基于 `Google` 内部应用的特点，有别于传统的分布式文件系统，除了考虑性能、可扩展性、可靠性和可用性之外，`GFS` 在设计时还考虑了以下三个方面：

1. 组件异常经常出现而不是偶尔出现。`GFS` 构建在成百上千台廉价的机器上，并同时被同等数量的客户端访问。在这个量级规模下，在任何时候某个组件都有可能发生异常以及发生异常后无法自动恢复。这里的异常不只包括硬件的异常，还包括软件的异常以及人为的错误。因此，对于异常的监控和检测，容错，以及异常的自动恢复是系统不可或缺的一个部分。
2. 以传统的分布式文件系统的视角来看，`Google` 要处理的都是大文件，几个G的文件随处可见。
3. 对于 `Google` 的数据应用来说，大部分对文件的写操作是追加操作而不是覆盖操作。对文件的随机写几乎可以说是不存在。文件一旦写入完成后，基本上就不会被再次修改，剩下的都是读操作，且大部分场景下是顺序读。`MapReduce` 系统就是这个应用场景的典型例子，`map` 任务持续顺序追加生成中间结果文件，`reduce` 任务不断的从中间结果文件中顺序读取。根据这个特点，追加写就成为了系统性能优化以及写操作原子性保证的主要设计方向。
4. 将应用程序和文件系统的 `API` 协同设计有利于增加系统的灵活性。例如，`GFS` 提供了原子性的追加写操作，多个客户端可以并发追加写，而无需在应用程序层面进行加锁。

## 设计
### 假设
本节主要描述了 `GFS` 设计的几个出发点：

* 整个系统构建在一批廉价且经常出现异常的硬件上，所以设计上必须考虑对异常的监测和容错，以及异常发生后能够恢复到一个合适的程度。
* 需要能够高效管理几个G的大文件，系统也支持存储小文件，不过不会对其作特殊的优化。
* 系统需要支持两种文件读取方式：大量的流式读和少量的随机读。在大量的流式读场景下，每次读取大小一般在几百KB，或者1MB或更多。来自同一个客户端的连续读取一般是读取文件的某一段连续内容。而少量的随机读一般是在文件的任意位置读取几KB。对于性能敏感的应用程序，一般会将多个随机读根据读取的位置排序后批量读取，避免磁盘来回的随机寻址。
* 系统需要支持大量连续的文件追加写操作。对文件追加写的单次大小一般和流式读的单次大小差不多。文件一旦写入完成后就几乎不会再被修改。系统同样需要支持少量的随机写，不过和少量的随机读类似，随机写也不强调性能。
* 当有多个客户端对同一个文件进行追加写的时候，系统必须能高效的实现清晰明确的执行语义，例如是保证每个客户端至少成功追加写一次，还是至多一次或者其他。`GFS` 中的文件经常会充当某个生产者消费者场景下的队列，一边会有多个客户端不断往文件中追加写，一边也会有一个客户端同时进行流式读取（或在写入完成后读取），所以系统必须保证追加写操作的原子性。
* 整个系统的吞吐的重要性大于延迟，大部分 `Google` 的应用程序主要是大批量的文件处理，只有少量的程序会对单次的读或写有性能要求，所以系统的吞吐是第一位。

### 接口
虽然 `GFS` 并未完全实现标准的文件系统 `API`，如 `POSIX`，但仍提供了常见的文件系统接口，如创建（`create`）、删除（`delete`）、打开（`open`）、关闭（`close`）、读取（`read`）和写入（`write`）文件。类似于本地文件系统，文件在 `GFS` 内通过树状目录的形式组织，每个文件通过文件路径来唯一确定，不过这里的目录是逻辑上的概念，并不会映射到某个物理文件系统目录上。

除此之外，`GFS` 还支持快照（`snapshot`）和追加写（`record append`）的操作。快照能够低成本的复制一个文件或一个目录。追加写允许多个客户端并发的对同一个文件追加写入，并保证每个客户端写入的原子性。

### 架构
一个 `GFS` 集群由一个主节点（`master`）和多个 `chunkserver` 组成，并被多个客户端（`client`）访问。不管是主节点还是 `chunkserver` 或客户端，都运行在廉价的 `Linux` 机器上。可以轻易的将 `chunkserver` 和客户端运行在同一台机器上，如果系统资源允许或者能够容忍客户端代码潜在的不可靠性的话。

每个文件在存入 `GFS` 时会被切分为固定大小的块（`chunk`），每个 `chunk` 在创建时会被主节点分配一个全局不可变的64位 `chunk handle`。`chunkserver` 将 `chunk` 保存在本地文件系统中，每个 `chunk` 对应着本地的一个 `Linux` 文件，客户端通过指定 `chunk handle` 和文件偏移范围来读取或者写入 `chunk`。为了保证可靠性，每个 `chunk` 会复制到多台 `chunkserver` 上。`GFS` 默认为每个 `chunk` 生成3份备份，不过用户也可以为某些命名空间下的文件指定不同的备份数量。

主节点保存了全部的系统元数据，包括文件命名空间、访问控制信息、每个文件和对应 `chunk` 的映射、每个 `chunk` 所在的位置等。同时主节点还负责 `chunk` 的租约管理、不再使用的 `chunk` 的垃圾回收、`chunkserver` 间的 `chunk` 迁移等。主节点也会定期的向 `chunkserver` 发送心跳用于向 `chunkserver` 发送指令和收集 `chunkserver` 当前的状态。

`GFS` 的客户端代码会集成到用户应用程序中，它负责实现文件系统 `API` 以及和主节点、 `chunkserver` 通信来读取或写入文件。`GFS` 客户端通过主节点获取文件的元数据信息，然而文件的读取和写入都只和 `chunkserver` 通信，从而减少了主节点的负担。

不管是 `GFS` 客户端还是 `chunkserver` 都不会缓存文件数据。客户端缓存收益不大是因为 `Google` 大部分的应用程序是对大文件的流式读取或者文件内容过大无法被缓存。不考虑缓存简化了客户端和整个系统的实现，因为引入缓存就要考虑缓存一致性问题。不过客户端会缓存文件的元数据信息，例如某个 `chunk` 的位置信息。`chunkserver` 不缓存是因为 `chunk` 是作为 `Linux` 文件保存在本地文件系统中，操作系统本身已经提供了一层文件访问缓存，没有必要再加一层。

### 单主节点
单主节点同样简化了整个系统的设计，方便主节点高效管理 `chunk`，因为所有需要的信息都保存在主节点内存中。为了避免对文件的读写使得主节点成为瓶颈，客户端读写文件时直接和 `chunkserver` 通信而不会经过主节点中转，不过在开始读写前，客户端会先和主节点通信获取需要的 `chunk` 对应的 `chunkserver` 列表，并将这个数据缓存一段时间来减少后续和主节点的通信。

结合下面的流程图我们来看一下一次读操作是如何执行的：

![alt](/images/gfs1.png)

1. 客户端根据固定的每个 `chunk` 的大小和想要读取的文件内容偏移量，计算出 `chunk` 的索引。
2. 客户端将文件名和想要读取的 `chunk` 的索引发给主节点，主节点收到请求后返回对应的 `chunk handle` 和保存了该份 `chunk` 的 `chunkserver` 列表。
3. 客户端收到主节点返回结果后，以文件名和 `chunk` 的索引组合作为键，将返回结果缓存起来。
4. 客户端将 `chunk handle` 和想要读取的文件内容偏移范围发给其中一台 `chunkserver`，一般来说，客户端会选择距离最近的 `chunkserver`，由于缓存了 `chunkserver` 列表信息，后续对同一个 `chunk` 的读请求就不需要再次和主节点通信，直到缓存过期或者文件被再次打开。
5. 一般来说，客户端和主节点通信时会在一次请求中要求多个 `chunk` 的信息，主节点同样也可在一次响应中返回多个 `chunk` 的信息，这也有利于减少客户端和主节点的通信次数。

### `chunk` 大小
每个 `chunk` 的大小是一个关键的设计选择。`GFS` 的 `chunk` 大小为64MB，远大于一般文件系统的数据块的大小。每个 `chunk` 以 `Linux` 文件的形式保存在 `chunkserver` 上，但其实际占用的大小是动态增长的，从而避免了大量的文件碎片（例如实际只需要十几K的文件却固定分配了64MB的大小）。

那么，`GFS` 为什么要选择64MB这样较大的数呢？

1. `chunk` 的大小越大，每个文件拆分成的 `chunk` 的数量就越少，客户端需要读取或写入的 `chunk` 数量也就越少，和主节点通信的次数也就越少。对于 `Google` 的数据应用来说，大部分都是顺序读写，客户端和主节点交互的次数是线性的时间复杂度，减少 `chunk` 的总个数有助于降低整个的时间复杂度。即使是对于少数的随机写，由于 `chunk` 大小足够大，客户端也能够将TB级别的数据对应的所有 `chunk` 的信息缓存起来。
2. 一个 `chunk` 的大小越大，那么客户端对其的可操作次数就越多，客户端就可以和某个 `chunkserver` 维持一段较长时间的 `TCP` 连接，减少和不同的 `chunkserver` 建立连接的次数。
3. 主节点需要维护每个 `chunk` 对应的 `chunkserver` 列表，这个也是个线性的空间复杂度，较大的 `chunk` 大小能减少 `chunk` 的总个数，从而减少元数据的大小，主节点就可以将所有的元数据放在内存中。

另一方面，即使将 `chunk` 的大小设置的较大，以及采用了懒分配的策略，也依然存在缺点。对于一个小文件来说，它可能只有几个甚至是一个 `chunk`，如果这个文件被访问的频率较大，它就有可能成为热点数据，保存了对应的 `chunk` 的 `chunkserver` 就要承受更大的流量。不过在 `Google` 的实际应用场景中，热点数据并没有成为一个大问题，因为大部分的系统面向的是大文件的流式读取，流量能较平均的分发到各个 `chunkserver`。

不过，`Google` 确实有遇到过一个热点数据问题，有个批处理系统会先将一个可执行文件写入到 `GFS` 中，这个可执行文件的大小对应了一个 `chunk`，然后所有客户端会同时读取这个可执行文件并执行，这就造成了几台 `chunkserver` 同时承接了大量的请求。`Google` 通过两个方面来解决这个问题，第一针对这个文件设置一个较大的备份数量，让更多的 `chunkserver` 来分散流量，即水平扩展；第二交替启动批处理系统的客户端，避免同一时间的大量请求。另一种长期的解决方案是在这种场景下允许客户端从其他已读取完毕的客户端那读取文件。

### 元数据
主节点主要保存了三类元数据：每个文件和 `chunk` 所属的命名空间、每个文件到对应所有 `chunk` 的映射、每个 `chunk` 对应的 `chunkserver` 列表。所有的元数据都保存在主节点的内存中，前两种元数据被修改时还会以日志的形式保存在本地日志文件中，并备份到其他服务器上。通过日志文件的方式使得主节点能够轻松的修改元数据，并且在主节点崩溃恢复后能恢复数据，当然极端情况下主节点有可能没有保存最新的元数据到日志文件中就崩溃了，这里后文会有说明主节点会在持久化完成后才返回结果给客户端，所以客户端不会看到未持久化的元数据。主节点并不会对 `chunkserver` 列表进行持久化，而是在主节点启动时主动拉取所有 `chunkserver` 的信息，以及每当一个新的 `chunkserver` 加入到集群中时，更新元数据，因为 `chunkserver` 是个动态更新的数据，即使持久化了也需要和当前实时的数据作比对。

#### 数据结构
由于元数据保存在主节点内存中，所以涉及主节点的操作都是非常快的。另一方面，主节点也能够轻易的每隔一段时间遍历所有的元数据，这个主要有三个作用，第一是扫描不再需要的 `chunk` 进行垃圾回收；第二如果某个 `chunkserver` 失联了，能将其保存的 `chunk` 备份到其他 `chunkserver` 上；第三将某些流量失衡或者本地磁盘空间不够的 `chunkserver` 下的 `chunk` 转移到其他 `chunkserver` 下。

将元数据保存在内存中的一个缺陷是受限于主节点的内存容量，能够保存的 `chunk` 的元数据的个数是有限的，从而整个 `GFS` 的容量大小是有限的。不过在实践中，这个问题还没有成为瓶颈，每个 `chunk` 对应的元数据大小约小于64字节，由于 `GFS` 面向的主要是大文件，所以基本上每个文件也就最后一个 `chunk` 没有塞满，所以空间浪费较少。类似的，由于使用了前缀压缩，每个文件的命名空间元数据的大小也小于64字节。

如果需要 `GFS` 支撑更大的容量，相比于将元数据全部保存在内存中所带来的的简洁、可靠、性能和灵活上的收益，扩展主节点的内存所需要的成本不值一提。

#### `chunk` 的保存位置
主节点不会持久化每个 `chunk` 对应的 `chunkserver` 列表，它会在启动时主动拉取所有的 `chunkserver` 信息。在主节点运行后，它可以通过和 `chunkserver` 的心跳来实时更新 `chunkserver` 的状态信息。

在最初的设计中，`Google` 的工程师试过将 `chunk` 和 `chunkserver` 的映射关系进行持久化，但是最终还是采用了更简洁的设计，即在主节点启动时主动拉取所有的 `chunkserver` 信息并在之后定期更新内存中的数据。这就消除了频繁的数据同步，毕竟在集群环境下，`chunkserver` 加入或者离开集群，修改主机名，发生异常或重启等操作会经常发生。

另一方面，只有 `chunkserver` 才真正知道自己是否拥有某个 `chunk`，所以在主节点上持久化一份 `chunk` 和 `chunkserver` 的映射关系意义不大，因为这份数据很大概率是不一致的，毕竟 `chunkserver` 会发生异常然后失联或者运维重命名了一台 `chunkserver`。

#### 操作日志
操作日志记录了元数据的重要历史更新，它是 `GFS` 的关键。不仅因为它是元数据的唯一持久化备份，同时它也提供了系统在并发操作下各操作的执行顺序。各个版本的文件和其对应的 `chunk` 的操作记录都被唯一的标识在日志文件中。

所以操作日志必须保证可靠性存储，以及在元数据持久化完成之前不能将最新的元数据更新暴露给客户端。否则就有可能丢失最新的客户端操作，即使 `chunkserver` 还存活着。因此，`GFS` 会将操作日志备份在多台机器上，并且只有在所有的备份机器都持久化完成后才会返回结果给客户端。同时主节点会对操作日志进行批量持久化以降低频繁持久化对系统整体吞吐的影响。

当主节点崩溃重启后会通过重放操作日志来恢复崩溃前的状态，然而如果每次都从第一条日志开始重放，主节点崩溃重启到可用需要的时间会越来越久，因此当操作日志的大小增长到一定程度的时候，主节点会为当前的元数据创建一个检查点，当主节点崩溃恢复后，可用先加载最新的检查点数据，然后再重放在这个检查点之后生成的操作日志。检查点是一个类似于 `B` 树的数据结构，可以轻易的映射到内存数据结构中，并且方便根据文件的命名空间检索。这就加快了主节点崩溃恢复的速度和提高了系统的可用性。

因为构建检查点需要时间，所以在创建检查点时需要确保不影响正在进行的修改。主节点会先创建一个新的操作日志文件，然后由一个新的线程去创建检查点，这个线程会依据新的操作日志文件创建前的日志生成检查点。对于一个百万级别数量文件的系统来说，大概需要1分钟的时间创建检查点，检查点创建完成后同样会持久化到本地磁盘和备份机器上。

主节点崩溃恢复只需要最新完整的检查点文件以及后续的操作日志文件，在这之前的检查点文件和操作日志文件都可以删除，不过为了提高容错性还是会保留一部分文件。如果生成检查点的时候发生异常并不会影响崩溃恢复的正确性，因为恢复的代码会校验检查点文件的完整性并跳过损坏的检查点文件。

### 一致性模型
`GFS` 提供了弱一致性模型，在能较好的支撑 `Google` 内部各分布式应用的同时也兼顾了简洁和高效的实现。

#### `GFS` 的保证
文件命名空间的修改是原子的（例如创建一个文件）。对命名空间的修改会由主节点加锁执行，这就保证了修改的原子性和正确性，同时主节点的操作日志记录了这些修改的全局顺序。

而某块文件区域修改后的状态则取决于修改的类型，是否成功或失败，以及是否存在并发的修改。某块文件区域是一致的（`consistent`）表示不管客户端从哪个 `chunkserver` 读取这块的数据，每个客户端看到的内容始终是相同的。某块文件区域是已定义的（`defined`）表示经过一次修改后，首先这块文件区域是一致的，并且客户端读到的数据就是自己修改的。如果对某块文件区域的修改不受其他并发请求的客户端的影响，那么这块文件区域在修改成功后是已定义的（也是一致的），所有的客户端都能看到对应的修改。而如果有多个客户端并发的对同一块文件区域修改，那么最终的结果是未定义的，但是是一致的，所有的客户端读到的数据都是相同的，但是并不知道读到的数据是哪个客户端修改的，一般来说，最后这块文件区域的内容很可能是多个客户端并发修改后混合的结果。一次失败的修改会使得这块文件区域不一致（因此也是未定义的），不同的客户端在不同时间读取到的数据是不同的。

参考：

* [The Google File System](https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)
* [GFS，一致性模型里，“已定义”和“不一致”具体表示的什么含义？](https://www.zhihu.com/question/20485173)