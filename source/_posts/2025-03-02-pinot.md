title: '【读】Pinot: Realtime OLAP for 530 Million Users'
tags:
- Paper
- Pinot
- Distributed Systems
- Database
---

## 介绍
作者对大数据时代下近实时的 `OLAP` 服务提出了如下要求：

* 高性能：系统应当能够快速的返回用户的查询请求
* 可扩展性：为了能够在处理大量并发查询请求的同时做到近实时的消费大量数据，系统应当提供近线性的扩展性和容错性来满足大规模的服务部署
* 低成本：随着数据容量以及查询并发量的增加，系统的成本不能无限制的增长
* 低延迟的实时数据消费：用户期望能够近实时的查询到刚添加到系统中的数据
* 灵活性：系统应当能够支持用户查询时下钻到任意维度，而不受限于预聚合的数据；同时，系统也能够以零停机的方式在生产环境增加新的查询模式
* 容错性：系统异常时能够提供优雅的服务降级
* 非中断式运维：系统能够以零停机的方式进行服务升级或者表的模式变更
* 云服务友好的架构：系统应当能够轻易的部署到商用的云服务环境中

## 架构
`Pinot` 诞生自 `LinkedIn`，作为一款可扩展的分布式 `OLAP` 数据库，能够提供低延迟的实时数据分析。`Pinot` 构建在不可变的追加式数据存储之上，专门为数据分析查询场景作了优化，数据自开始接入到可被查询可以在几秒内完成。

在 `LinkedIn` 中，业务数据会先接入到 `Kafka` 中，然后经过 `ETL` 处理存储到 `HDFS`。`Pinot` 既支持近实时的从 `Kafka` 中消费数据，也支持从类似 `Hadoop` 的离线系统中导入数据。因此，`Pinot` 遵循 `lambda` 架构，能够自动合并从 `Kafka` 接入的流式数据和 `Hadoop` 导入的离线数据。

### 数据和查询模型
和常见的数据块一样，`Pinot` 也以表的方式管理数据，每个表背后由 `schema` 定义了有哪些列。支持的数据类型包括不同长度的整型，浮点数，字符串，布尔值，以及基于这些数据类型的数组。`Pinot` 中的列即可也是 `dimension`，也可以是 `metric`。

`Pinot` 还支持一个特殊的时间列，一方面在查询时基于该列合并流式数据和离线数据，另一方面作为数据过期判断的依据。

`Pinot` 的表以 `segment` 为单位存储，每个 `segment` 一般能存储几千万条记录，一张表能支持几万个 `segment`。`segment` 可以有副本，从而确保数据的高可用性。`segment` 中的数据是不可修改的，但是可以整个替换 `segment` 来更新数据（数据更新代价较大）。

`segment` 采用列存储保存数据，并支持多种编码策略来减少单个 `segment` 的大小。一个 `segment` 的大小一般在几百 `MB` 到几 `GB` 不等。下图展示了 `segment` 的数据存储方式：

![alt](/images/pinot-1.png)

`Pinot` 的查询语言为 `PQL`，为 `SQL` 的子集，支持 `select`，`projection`，`aggregation`，和 `top-n` 查询，不过不支持连接和嵌套查询。`PQL` 不支持单条记录级别的创建，更新或者删除。

### 组件
`Pinot` 有四个主要的组件用于数据存储，数据管理和查询：
* `controller`
* `broker`
* `server`
* `minion`

除此之外，`Pinot` 还依赖 `Zookeeper` 以及持久化的对象存储。`Pinot` 借助 `Apache Helix` 来管理集群，`Apache Helix` 是一个通用的集权管理框架，用于管理分布式系统内的分区和副本。

`server` 主要负责存储 `segment` 并处理针对所负责的 `segment` 的查询。每个 `segment` 在 `UNIX` 的文件系统上对应一个目录，目录中保存了 `segment` 的元数据和索引文件。`segment` 的元数据保存了列的信息，包括类型，`cardinality`，编码，各式各样的统计信息，以及支持的索引。索引文件保存了每列的索引。索引文件只能追加写入，从而支持按需创建 `inverted index`。`server` 可插拔的架构支持从多种不同的存储格式加载列索引，以及在于运行时生成衍生列（`synthetic column`）。同时也能轻易的扩展从类似 `HDFS` 或者 `S3` 这样的分布式存储系统读取数据。`Pinot` 会维护一个 `segment` 的多个副本，所有副本都会参与查询。

`controller` 负责管理 `segment` 到 `server` 的分配。`controller` 会根据运维需求或者 `server` 的可用性更新 `segment` 的分配。另外，`controller` 还负责一些列管理任务，例如查询所有可用的表、`segment`，添加或者删除表、`segment`。`Pinot` 的表可以设置过期时间，超过过期时间的 `segment` 会被 `controller` 删除。`segment` 的所有元数据及 `segment` 到 `server` 的映射都由 `Apache Helix` 管理。出于容错性的考虑，`LinkedIn` 一般会在每个数据中心部署三个 `controller` 实例，其中一个作为主节点，由 `Apache Helix` 管理，非主节点在大多数时间里是空闲的。

`broker` 负责处理查询请求，它首先将查询分发到负责的各个 `server` 上，然后合并各个 `server` 的查询结果，最后返回给客户端。客户端通过 `HTTP` 和 `broker` 交互，所以可以前置负载均衡器分摊 `broker` 的压力。

`minion` 负责运行一些计算密集型的任务，其任务由 `controller` 的调度器分配。另外，任务管理和调度支持扩展添加新的任务和调度类型以支持变化的业务需求。`minion` 的其中一个使用场景是数据清洗，出于数据合规的要求，`LinkedIn` 需要进行数据清洗。由于 `Pinot` 数据的不可变性，`minion` 运行时需要先下载 `segment`，然后清洗数据，接着重新生成 `segment` 以及重建索引，最后上传 `segment` 到 `Pinot` 覆盖旧的 `segment`。

`Zookeeper` 用于持久化存储元数据，并作为集群中各节点间通信的渠道。集群的状态，`segment` 的分配，以及元数据都通过 `Helix` 保存在 `Zookeeper` 中。`segment` 本身保存在持久化的对象存储中。在 `LinkedIn` 内部，`Pinot` 使用本地的 `NFS` 作为数据存储层，而运行在 `LinkedIn` 数据中心之外则借助 `Azure Disk`。

下图是 `Pinot` 的架构图：
![alt](/images/pinot-2.png)

### 常见操作
#### 加载 segment
`Helix` 借助状态机来描绘集群的状态，集群中的每个资源都有当前的状态以及期望的状态。当状态发生变更时，对应的节点就会执行状态变更流程。

## 参考
* [Pinot: Realtime OLAP for 530 Million Users](https://dl.acm.org/doi/10.1145/3183713.3190661)
