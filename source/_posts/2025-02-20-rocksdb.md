title: '【读】RocksDB: Evolution of Development Priorities in a Key-value Store Serving Large-scale Applications'
tags:
- Paper
- RocksDB
---

本文是 `Facebook` 对 `RocksDB` 8年开发历程的回顾，重点讨论了为支持大规模分布式系统所做的开发优先级取舍与演进，以及在生产环境中运行大规模应用的经验。

## 介绍
`RocksDB` 是 `Facebook` 在2012年创建的高性能 `KV` 持久存储引擎，代码衍生自 `Google` 的 `LevelDB`。它针对 `SSD` 的某些特性进行了优化，目标是服务于大型（分布式）应用，在使用上则以类库的方式和上层应用集成。每个 `RocksDB` 实例是个单机版程序，本身不提供跨主机间的操作，例如副本管理和负载均衡，同时也不提供高阶 `API`，例如不支持 `checkpoint`，这些都留给上层应用自行实现。

`RocksDB` 及其高度可定制的组件设计使其能够从容应对不同的业务需求和工作负载。除了作为数据库系统的存储引擎外，`RocksDB` 还被用于以下不同类型的服务：
* 流式处理：典型代表如 `Flink` 借助 `RocksDB` 保存 `checkpoint` 的状态数据
* 日志/队列服务：依托于 `RocksDB` 可定制化的合并策略，这些服务能够以不亚于追加写单个文件的效率实现高吞吐的写入，同时有着较低的写放大，以及享受内置索引带来的便利
* 索引服务：`RocksDB` 的 `bulk loading` 特性能够为索引服务提供大规模加载离线数据的能力，同时也有着高效的查询性能
* 基于 `SSD` 的二级缓存：因为 `RocksDB` 针对 `SSD` 进行了优化，所以某些内存式的缓存服务会借助 `RocksDB` 在内存不够时将部分数据置换到 `SSD`。这些服务往往要求存储引擎有着足够高的写入速度和优秀的点查询性能

## 背景
`RocksDB` 的设计极大的受到了 `SSD` 特性的影响，`SSD` 不对称的读写性能和有限的耐用性给 `RocksDB` 的数据结构设计和系统架构带来了机遇和挑战。

### 基于 SSD 的嵌入式存储
相比于机械硬盘，`SSD` 读写的 `IOPS` 可以达到十万至百万，读写速度可以达到几百至几千 `MB/s`。一方面，这给如何设计软件从而能充分利用 `SSD` 的性能带来了挑战；另一方面，受限于 `SSD` 有限的擦除次数，同时也需要考虑如何设计合理的数据结构。

正因为 `SSD` 出色的性能，在大多数情况下，应用的性能瓶颈也从设备 `I/O` 转向了网络；应用架构设计时也更倾向于将数据存储在本地 `SSD` 而不是远程存储服务，因此，能够内嵌在应用中的本地 `KV` 存储引擎的需求就日渐上涨。

在这个背景下，`Facebook` 实现了 `RocksDB`，其中 `LSM` 树扮演了重大的角色。

### RocksDB 的架构和 LSM 树的使用
`RocksDB` 使用 `LSM` 树作为主要的数据结构来保存数据并支持以下核心的操作。

#### 写
写入时会先将数据写入到名为 `MemTable` 的内存写缓冲中，同时也会在磁盘上记录 `Write Aghead Log (WAL)`。`MemTable` 由跳表（`skiplist`）实现，插入和查询的时间复杂度都是 `O(logn)`。`WAL` 可按需开启，用于 `RocksDB` 从崩溃后恢复数据。当 `MemTable` 的大小达到所配置的阈值时：
1. 当前接受写入的 `MemTable` 和 `WAL` 变为只读
2. 后续新的写入由新创建的 `MemTable` 和 `WAL` 服务
3. 系统会将变为只读的 `MemTable` 和 `WAL` 的内容落盘到 `Sorted String Table (SSTable)` 内
4. 已落盘的 `MemTable` 和 `WAL` 则可以丢弃

`SSTable` 中的数据按序存储，并以等大小的块（`block`）组织。`SSTable` 生成后同样变为只读，同时，其内部会维护一个索引块并给每个数据块维护一条索引，从而能借助二分查找快速搜索。

#### 合并
![alt](/images/rocksdb-1.png)

如上图所示，一个 `LSM` 树分为多层。最新的 `SSTable` 由 `MemTable` 刷盘生成，并放置在 `Level-0`。其他层的 `SSTable` 则统一由合并程序维护。当第 `L` 层的 `SSTable` 大小触及了配置值，合并程序会选择该层的部分 `SSTable`，并将其和第 `L + 1` 层内键的范围存在重合的 `SSTable` 进行合并，从而在第 `L + 1` 层生成一个新的 `SSTable`。通过这个操作，`RocksDB` 就可以将已删除的数据和过时的数据清除，同时新生成的 `SSTable` 也进行了瘦身，节省了磁盘空间，最终写入的数据会逐渐从 `Level-0` 迁移到最后一层。整个合并过程的 `I/O` 效率也比较高，一方面不同层的合并可以并行执行，另一方面 `I/O` 操作只涉及整个 `SSTable` 文件的批量读和写。

`MemTable` 和 `Level-0` 层的 `SSTable` 键的范围可能会存在重合，而 `Level-1` 及其之后的每一层内，`RocksDB` 会确保各个 `SSTable` 之间键的范围不会重合（但是不同层之间的 `SSTable` 键的范围是有可能重合的）。

`RocksDB` 支持不同类型的合并：

* `Leveled Compaction`：借鉴自 `LevelDB` 并加以改进。每一层可容纳的文件大小呈指数级放大。系统会积极的触发合并以确保每层的文件大小不会超过指定阈值
* `Tiered Compaction`：在 `RocksDB` 也被称为 `Universal Compactioin`，与 `Apache Cassandra` 或 `HBase` 采取的合并模式类似。当 `Level-0` 层文件的个数或者非 `Level-0` 层的个数超过指定的阈值，又或者整个数据库的大小和最深层文件大小之比超过指定的阈值时，则会触发合并多个 `SSTable`。有别于 `Leveled Compaction`，`Tiered Compaction` 是惰性合并，实际的合并会推迟到读性能或者空间效率发生衰减时进行，从而能够一次性合并更多的数据
* `FIFO Compaction`：当数据库大小触及到指定阈值时，丢弃最老的 `SSTable`，且只进行轻量级的合并。适合于基于内存的缓存应用

`RocksDB` 的读写性能在不同的合并策略下有着不同的表现，应用开发者需要结合自身服务的工作负载选择合适的合并策略。

#### 读
读取时，`RocksDB` 首先在所有的 `MemTable` 中查找，如果没有找到则继续在位于 `Level-0` 层的所有 `SSTable` 中查找，如果还没有找到，则继续向下一层中键的范围包含要查找的键的 `SSTable` 中查找，所有的查找都借助了二分搜索。另外还有两项辅助查找的优化：
1. 频繁被访问的 `SSTable` 块会在内存中缓存从而减少文件 `I/O`，以及解压缩的开销
2. 布隆过滤器用于快速排除一定不包含要查找的键的 `SSTable`

#### Column Family
`RocksDB` 在2014年引入了 `column family` 功能，不同的 `column family` 下可以包含相同的键，每个 `column family` 有独立的 `MemTable` 和 `SStable`，但是共享 `WAL`。其优势在于：

1. 每个 `column family` 可独立配置，如合并，压缩，`merge operators` 以及 `compaction filters`
2. 共享的 `WAL` 能够保证原子写入到多个 `column family`
3. `column family` 可动态高效的删除和创建

## 资源优化目标的演进
### 写放大
`RocksDB` 最初的资源优化目标在于减少 `SSD` 的擦除周期以及写放大，写放大包含两方面：
1. `SSD` 本身的写放大，`SSD` 不能直接覆盖已有的数据，需要先将其擦除，再写入，写入的粒度为 `page`，但是擦除的粒度是 `block`，一个 `block` 包含多个 `page`；同时 `SSD` 的垃圾回收也会造成数据移动和擦除；最后 `SSD` 的 `Wear Leveling` 特性会保证各个 `memory cell` 均衡的写入，也引入了数据移动
2. 数据库软件带来的写放大

在这两个因素下有时候写放大能达到100倍。

`Leveled Compaction` 的写放大倍数基本在10到30，在大多数情况能够数倍优于 `B` 树的实现。更进一步，`Tiered Compaction` 能将写放大倍数降至4到10，不过缺点是读性能会有一定的下降。一般来说，当应用的写负载较高时，可以配合写放大较低的合并策略，而当写负载不高时，则可以采用更激进的合并策略，从而有更好的空间效率和读性能。

### 空间放大
经过了多年的开发后，`RocksDB` 团队认为对于大多数应用来说，空间使用率远比写放大重要，因为这些场景下还没有到触及 `SSD` 本身的限制，不恰当的比喻来说就是：

> 以大多数应用程序的稳定性来说，还远没有到拼操作系统稳定性的地步。

而实际上，应用本身也没有充分利用 `SSD` 提供的读写吞吐，因此这一阶段的优化重心就转移到了磁盘空间上。

由于 `LSM` 树无碎片的数据组织，天然的避免了由于数据碎片带来的磁盘空间浪费。另一方面，`RocksDB` 也引入新的合并策略 `Dynamic Leveled Compaction`，其中 `LSM` 树每一层的大小上限会动态的根据最深层文件的大小调整，而不是固定值。这么做的原因是为了减少 `LSM` 树中无效的数据（已删除和已被覆盖），而和最深层文件大小的比值则可作为无效数据的度量指标。最终的结果也表明相比于 `Leveled Compaction`，`Dynamic Leveled Compaction` 有着更稳定的空间效率。

### CPU 利用率
随着 `SSD` 的发展，一种潜在的担忧是应用程序已不能完全充分利用 `SSD` 的潜能。因此，系统的瓶颈也从设备 `I/O` 转移到了 `CPU`。不过，`RocksDB` 的开发人员不这么看，因为：

1. 只有少部分的应用受限于 `SSD` 的 `IOPS`，大部分应用受限于磁盘空间
2. 一个高端 `CPU` 足够服务于一个高端 `SSD`。在 `Facebook` 的生产环境中还没有遇到 `RocksDB` 不能充分利用 `SSD` 能力的情况。当然，如果一个 `CPU` 服务多个 `SSD` 还是有可能会有 `CPU` 瓶颈的，不过这属于系统配置层面的资源不均衡问题。另一方面，写密集型的应用也有可能存在 `CPU` 瓶颈的问题，不过这可以通过使用更轻量级的合并策略解决。而在这之外的场景，其工作负载则可能不适合使用 `SSD`，因为有可能提前让 `SSD` 的寿命完结

不过，优化 `CPU` 利用率也不等于说是无用功，因为空间放大的优化余地已经不多了。优化了 `CPU` 也等同于省钱，毕竟 `CPU` 和内存的价格也在节节攀升。一些对 `RocksDB` 的 `CPU` 优化的尝试包括前缀布隆过滤器，在查找索引前先用布隆过滤器判断，以及其他的一些布隆过滤器优化。

### 适配新技术
一些 `SSD` 的新技术例如 `open-channel SSDs`，`multi-stream SSDs`，`ZNS` 能让 `SSD` 有着更低的查询延迟及更少的擦除周期损耗。不过，如前面所述，`RocksDB` 的开发团队认为大部分应用的瓶颈在于磁盘空间，适配这些新技术反而会给 `RocksDB` 的一致性体验带来挑战，所以这项的优先级不高。

`In-storage computing` 可能会给应用带来巨大的提升，不过 `RocksDB` 的开发团队目前还不确定 `RocksDB` 能从这项技术中受益多少，而且对 `API` 的改动可能也比较大。

`Disaggregated (remote) storage` 则更具吸引力，并且也是当前的一个优化重点。前文的优化背景都是应用直接访问本地 `SSD`，不过，如今更快的网络带宽使得远程访问 `SSD` 成为了可能，因此，如何优化 `RocksDB` 使其更好的适配远程 `SSD` 也变得有意义。在远程存储模式下，`CPU` 和 `SSD` 资源可以同时做到充分利用以及独立扩展，相反本地 `SSD` 的模式则较难实现。目前 `RocksDB` 的开发团队正在优化远程模式下的 `I/O` 延迟。

最后，`non-volatile memory (NVM)` （它相比于 `SSD` 有着更高的 `IO` 读写吞吐）这项技术也在考量中：

1. 将 `NVM` 作为 `DRAM` 的扩展
   1. 如何实现核心数据结构（`block cache` 还是 `MemTable`）从而结合 `NVM` 和 `DRAM` 使用
   2. 会引入哪些额外的开销
2. 将 `NVM` 作为数据库的主要存储：不过实践表明 `RocksDB` 的瓶颈主要在于磁盘空间或者 `CPU`，而不是 `I/O`
3. 用 `NVM` 保存 `WAL`：其成本是否值得有待考虑，毕竟 `WAL` 中的数据量不大，并且会刷盘到 `SSD`

### 再次审视 RocksDB 使用 LSM 树的合理性
`LSM` 树依然是最适合的，因为 `SSD` 还没有到白菜价的地步，对于大多数应用来说，其有限的寿命依然是无法忽略的因素。而另一方面，`RocksDB` 的开发团队也发现某些写密集型的应用会写大量的大对象，如果能分别存储键值对则能减少 `SSD` 的写入，其功能实现为 `BlobDB`。

## 运行大规模系统的经验总结
### 资源管理
大规模分布式数据服务往往会以 `shard` 粒度分区到多个节点上，一个节点可能会持有几十上百个 `shard`。`shard` 的大小有限，因为 `shard` 是负载均衡和副本的最小单位，需要在各节点之间进行拷贝。在 `Facebook` 的环境内，一个 `shard` 由一个 `RocksDB` 实例提供服务，因此一个节点会运行很多 `RocksDB` 实例，它们可能会共享一个地址空间，有可能会独享。

在上述背景下，就需要考虑如何进行资源管理，包括：
1. 分配给 `write buffer`，`MemTable`，`block cache` 的内存
2. 合并程序占用的 `I/O` 带宽
3. 合并程序线程数
4. 磁盘使用量
5. 文件删除速率

资源管理包括两个维度，全局（分配给每个节点的资源）和局部（分配给每个 `RocksDB` 实例的资源）。对后者来说，`RocksDB` 允许应用程序创建 `resource controller` （以 `C++` 对象实现并传递给多个 `RocksDB` 实例）来对上述提到的资源进行分配。例如，一个实现了对合并程序占用的 `I/O` 带宽限流的 `C++` 对象可以传递给多个 `RocksDB` 实例，从而保证任一时刻所有 `RocksDB` 实例的合并程序占用的 `I/O` 带宽之和不会超过指定值。另外，资源管理需要能够支持按优先级分配，使得最迫切需要资源的实例能优先获取资源。

另一个在一个进程内运行多个 `RocksDB` 实例的经验总结是将各实例中执行相似任务的线程统一以一个线程池进行管理，而不是每个实例各自维护线程池。这些线程执行的往往是后台任务，统一了线程池也变相的限制了后台任务执行时占用的 `I/O`，使得资源使用更具预测性。独立维护线程池的情况下有可能会有瞬时的 `CPU` 或者 `I/O` 毛刺，造成服务不稳定。不过，有得则有失，共享线程池的缺点就在于某些实例有可能无法及时的获取线程，从而阻塞后台任务，例如无法及时执行 `SSTable` 的合并，甚至造成写停顿（`write stall`）。

相比而言，当不同的 `RocksDB` 实例运行在多个进程时，全局的资源管理则更具有挑战性，毕竟各进程之间没有信息交互。文中提出了两种策略：
1. 为每个 `RocksDB` 实例配置较为保守的资源额度，缺点就是全局资源利用率不一定最优
2. 各进程间交换资源使用的情况，从而动态调整资源配比

### 支持副本和备份
`RocksDB` 本身不提供开箱即用的副本和备份的支持，需要应用自行实现，不过 `RocksDB` 为实现这两个功能提供了必要的支持。

#### 副本
从一个节点复制一个全新的副本节点有两种方式：
1. 逻辑复制（`logical copying`）：遍历源节点的所有键值对，然后写入到目标节点。在源节点端，借助 `RocksDB` 的快照功能保证了数据的读一致性。同时，`RocksDB` 支持 `scan` 操作从而在数据复制时减少对在线查询的影响。在目标节点端，`RocksDB` 提供了 `bulk loading` 的功能来批量加载数据
2. 物理复制（`physical copying`）：直接复制 `SSTable` 和其他辅助文件到目标节点。`RocksDB` 在复制时会确保没有文件被修改或删除

#### 备份
备份对于数据库来说至关重要，和副本复制一样，备份的实现同样有逻辑备份和物理备份两种。副本和备份的其中一个区别在于上层应用经常会需要同时管理多个备份。`RocksDB` 也内置了一个备份引擎针对简易的备份场景。

#### 更新副本面临的挑战
在多副本场景下，如何将主节点的更新以一致的顺序同步到各副本是一个挑战。直白的做法是依次按序向各个副本写入，当然缺点就是性能很差，无法利用多线程。另外，当某个副本停止同步很久之后，需要相应的机制能让其快速同步至最新的状态。

而无序写的问题在于读取时有可能数据不一致，一种解决方法是引入快照读，客户端读取时指定序列号，`RocksDB` 会返回截止到该序列号的数据快照，而不受当前正在进行中的写入的影响。

### WAL 处理
传统的数据库一般要求每次写入前先写 `write-ahead-log (WAL)` 来保证数据的持久性。相反，大型分布式存储系统一般使用多副本来提升性能和可用性，例如，如果某个副本的数据损坏或者无法访问，那么系统可以基于其他完好的副本重新构建损坏的副本。对于这些系统来说，`WAL` 就不是那么重要。另外，分布式系统一般也有自己的一致性协议日志（如 `Paxos` 协议），这种情况下 `WAL` 就可以不需要了。

因此，`RocksDB` 需要能够针对不同的场景灵活配置 `WAL`，`RocksDB` 提供了三种选项：
1. 同步刷盘写 `WAL`
2. 先将 `WAL` 写入到缓冲区，然后定期由后台低优先级线程刷盘
3. 无 `WAL`

### 数据格式兼容性
大型分布式应用往往运行在诸多节点上，并且最好不发生服务中断。因此，软件更新往往是逐台（或者小批量同时）发布，出现问题时再回滚。因此，`RocksDB` 需要能够保证存储在磁盘上的数据能够后向和前向兼容。另外，出于副本构建或者负载均衡的需要，系统会在各节点之间复制数据，因此整个集群可能运行着多个版本格式的数据。

对于后向兼容来说，`RocksDB` 需要能够识别之前的所有数据格式，这无疑增加了实现了维护的复杂度。对于前向兼容来说，`RocksDB` 需要能识别新的数据格式，并且至少要支持一年的向前兼容，这方面的技术手段借助于 `Protocol Buffer` 或者 `Thrift`。对于配置项的兼容性来说，`RocksDB` 需要能够识别未知的配置，并尽最大可能尝试猜测配置的含义或者忽视。

## 错误处理的经验总结
`RocksDB` 的开发团队通过产线的实践总结了三条关于错误处理的经验：
1. 数据损坏越早监测到越好，从而最低程度的避免数据不可用或丢失，同时也能精确定位数据损坏的源头。`RocksDB` 通过在系统各层级计算数据的校验和并在数据传输时验证校验和来识别数据是否损坏
2. 完整性保护必须覆盖整个系统，从而避免由于静默的硬件数据损坏传递给 `RocksDB` 客户端或者其他副本。仅仅在数据未使用或者传输时检测是不够的的，因为数据损坏有可能由异常的软件，异常的 `CPU` 或者其他异常的硬件引入。不过，即使基础设施一直扫描系统中是否有异常的硬件，某些硬件异常也不一定能够被发现
3. 错误需要能够区别对待。`RocksDB` 的开发团队最开始将所有非 `EINTR` （系统调用中断）类型的文件系统错误统一处理。如果错误发生在读取操作，那么 `RocksDB` 直接将错误传递给客户端。如果错误发生在写操作，那么 `RocksDB` 认为这是一个不可恢复的错误，然后永久中断所有的写入；`RocksDB` 需要重启才能恢复写入，并且可能还需要额外的运维操作。为了减少这种粗暴的重启，`RocksDB` 的开发团队开始对错误按照严重性分门别类，并且只有在遇到确实是不可恢复的错误时才中断操作

### 静默损坏的频率
在真实的 `RocksDB` 使用场景中，多久会发生一次静默的数据损坏？出于成本的考虑，应用一般使用的存储设备并不提供端到端的数据包含，相反，应用依赖 `RocksDB` 提供的块校验和来监测数据损坏。另一方面，基于 `RocksDB` 的应用本身也会运行数据校验程序来对比副本间的数据，不过这个过程识别出的数据损坏既有可能是 `RocksDB` 引入的，也有可能是应用本身引入的。

通过比较 `MyRocks` 中主键和二级索引的使用情况，`RocksDB` 的开发团队推断出每 `100 PB` 数据在每三个月内会发生一次由 `RocksDB` 本身引起的数据损坏。其中40%的情况下，这些数据损坏已经扩散到了其他副本上。

另一方面，数据损坏也有可能发生在数据传输中，这经常是由于软件 `bug` 导致。例如，底层存储系统在处理网络异常时的一个 `bug` 会导致一段时间后，每传输 `1 PB` 数据大约有17个校验和不匹配。

### 多级保护
数据损坏需要尽早识别，以免扩大影响范围，并尽可能的减少服务中断时间和数据丢失。大多数的 `RocksDB` 应用会持有一份数据的多个副本，并定期检测副本的校验和来识别损坏的副本，一旦发现损坏的副本，应用可以丢弃该副本并替换为正确的备份。不过，这种做法的前提是系统中始终持有有效数据的副本。

如下图所示，`RocksDB` 启用了多级校验和保护，从而能尽早的发现数据损坏。

![alt](/images/rocksdb-2.png)

#### 块完整性（Block Integrity）
块校验和传承自 `LevelDB`，是为了避免文件系统层的数据损坏传递到客户端。这里的块不仅仅指 `SSTable` 块，也包括了 `WAL` 段（`fragment`），在块生成时会同时生成校验和。只要一个块被读取，`RocksDB` 都会检验它的校验和。

#### SSTable 完整性（SSTable Integrity）
每个 `SSTable` 文件也保存了一个校验和，该功能在2020年引入，是为了避免 `SSTable` 在传输时造成损坏，会在生成 `SSTable` 时生成校验和，并保存在 `SSTable` 的元数据中，`RocksDB` 会在传输 `SSTable` 时检验校验和。不过，这篇文章发表时，还没有 `WAL` 文件级别的校验和。

#### Handoff 完整性（Handoff Integrity）
在往文件系统写入数据前，会同时生成一个 `handoff` 校验和，然后将数据和校验和一起传递给下一层，由下一层进行数据校验。`RocksDB` 的开发团队期望用这种方式对 `WAL` 进行校验，因为 `WAL` 都是增量的追加写，不过可惜的是，很少有本地文件系统支持这种校验方式。不过，当 `RocksDB` 结合远程存储使用时，可以修改 `write` 接口使其接收额外的校验和，然后将其添加到存储服务内部的 `ECC` （`Error Correction Code`，用于校验数据完整性）中，最后远程存储服务在收到写请求时就可以进行校验。

#### 端到端的键值对完整性保护（Key-Value Integrity Protection）
上述的完整性校验依然存在不足，其中一个不足在于文件系统之外的数据没有完整性保护，例如 `MemTable` 和 `block cache` 中的数据。因此，在这一层的数据损坏就无法被监测并最终扩散到上层应用。而如果此时发生了 `MemTable` 的刷盘或者合并操作，则会将损坏的数据永久的持久化到磁盘上。

因此，`RocksDB` 的开发团队的解决方案是实现每个键值对级别的校验和，从而在文件系统层之外发现数据损坏。当某个键值对被复制时，其校验和也会随之复制，不过在写入到文件时这部分校验和会忽略，因为在文件级别已经有其他校验和机制来保证完整性了，从而减少冗余。

#### 基于严重性的错误处理
大部分情况下，`RocksDB` 遇到的故障都是底层存储系统返回的错误。这些错误可能来自于各种各样的问题，从比较严重的问题例如文件系统变成了只读，到短暂的问题例如磁盘空间满了或者访问远程存储时网络异常。在早些时候，如果是读操作发生的错误，`RocksDB` 就简单的将错误信息返回给客户端，而如果是写操作发生的错误，`RocksDB` 则会永久性的暂停所有写操作。

而优化后 `RocksDB` 仅在遇到无法本地恢复的错误时才中断操作，例如暂时的网络错误不应该要求重启 `RocksDB` 实例。对于暂时性的错误，`RocksDB` 会周期性的重试。

## 参考
* [RocksDB: Evolution of Development Priorities in a Key-value Store Serving Large-scale Applications](https://research.facebook.com/publications/rocksdb-evolution-of-development-priorities-in-a-key-value-store-serving-large-scale-applications/)
* [Write amplification](https://en.wikipedia.org/wiki/Write_amplification)
