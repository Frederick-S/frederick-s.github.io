<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"frederick-s.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Übung macht den Meister">
<meta property="og:url" content="https://frederick-s.github.io/index.html">
<meta property="og:site_name" content="Übung macht den Meister">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Xiaodan Mao">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://frederick-s.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Übung macht den Meister</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-J1NC2B33VK"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-J1NC2B33VK');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Übung macht den Meister" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Übung macht den Meister</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fas fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fas fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fas fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-mit-6.824">

    <a href="/mit-6.824/" rel="section"><i class="fas fa-landmark fa-fw"></i>MIT 6.824</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fas fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-rss">

    <a href="/atom.xml" rel="section"><i class="fas fa-rss fa-fw"></i>RSS</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/Frederick-S" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://frederick-s.github.io/2022/05/15/mit-6.824-lab2-implementation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaodan Mao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Übung macht den Meister">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/15/mit-6.824-lab2-implementation/" class="post-title-link" itemprop="url">MIT 6.824 - Lab 2 (3): 实现</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-15 00:00:00" itemprop="dateCreated datePublished" datetime="2022-05-15T00:00:00+08:00">2022-05-15</time>
            </span>

          
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>7.2k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>12 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p><a target="_blank" rel="noopener" href="https://blog.josejg.com/debugging-pretty/">Debugging by Pretty Printing</a> 中介绍了如何高效的打印日志，这有助于在实验时进行问题排查。</p>
<p>首先在 <code>Go</code> 侧需要封装一个日志打印函数 <code>PrettyDebug</code>（<code>raft</code> 目录下已经有了 <code>Debug</code> 变量，所以这里重命名为 <code>PrettyDebug</code>），在 <code>raft</code> 目录下新建一个 <code>Go</code> 文件，复制以下内容：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> raft</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;log&quot;</span></span><br><span class="line">	<span class="string">&quot;os&quot;</span></span><br><span class="line">	<span class="string">&quot;strconv&quot;</span></span><br><span class="line">	<span class="string">&quot;time&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Retrieve the verbosity level from an environment variable</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getVerbosity</span><span class="params">()</span> <span class="title">int</span></span> &#123;</span><br><span class="line">	v := os.Getenv(<span class="string">&quot;VERBOSE&quot;</span>)</span><br><span class="line">	level := <span class="number">0</span></span><br><span class="line">	<span class="keyword">if</span> v != <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">		<span class="keyword">var</span> err error</span><br><span class="line">		level, err = strconv.Atoi(v)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			log.Fatalf(<span class="string">&quot;Invalid verbosity %v&quot;</span>, v)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> level</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> logTopic <span class="keyword">string</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	dClient    logTopic = <span class="string">&quot;CLNT&quot;</span></span><br><span class="line">	dCommit    logTopic = <span class="string">&quot;CMIT&quot;</span></span><br><span class="line">	dDrop      logTopic = <span class="string">&quot;DROP&quot;</span></span><br><span class="line">	dError     logTopic = <span class="string">&quot;ERRO&quot;</span></span><br><span class="line">	dInfo      logTopic = <span class="string">&quot;INFO&quot;</span></span><br><span class="line">	dLeader    logTopic = <span class="string">&quot;LEAD&quot;</span></span><br><span class="line">	dCandidate logTopic = <span class="string">&quot;CAND&quot;</span></span><br><span class="line">	dLog       logTopic = <span class="string">&quot;LOG1&quot;</span></span><br><span class="line">	dLog2      logTopic = <span class="string">&quot;LOG2&quot;</span></span><br><span class="line">	dPersist   logTopic = <span class="string">&quot;PERS&quot;</span></span><br><span class="line">	dSnap      logTopic = <span class="string">&quot;SNAP&quot;</span></span><br><span class="line">	dTerm      logTopic = <span class="string">&quot;TERM&quot;</span></span><br><span class="line">	dTest      logTopic = <span class="string">&quot;TEST&quot;</span></span><br><span class="line">	dTimer     logTopic = <span class="string">&quot;TIMR&quot;</span></span><br><span class="line">	dTrace     logTopic = <span class="string">&quot;TRCE&quot;</span></span><br><span class="line">	dVote      logTopic = <span class="string">&quot;VOTE&quot;</span></span><br><span class="line">	dWarn      logTopic = <span class="string">&quot;WARN&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> debugStart time.Time</span><br><span class="line"><span class="keyword">var</span> debugVerbosity <span class="keyword">int</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">init</span><span class="params">()</span></span> &#123;</span><br><span class="line">	debugVerbosity = getVerbosity()</span><br><span class="line">	debugStart = time.Now()</span><br><span class="line"></span><br><span class="line">	log.SetFlags(log.Flags() &amp;^ (log.Ldate | log.Ltime))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">PrettyDebug</span><span class="params">(topic logTopic, format <span class="keyword">string</span>, a ...<span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> debugVerbosity &gt;= <span class="number">1</span> &#123;</span><br><span class="line">		t := time.Since(debugStart).Microseconds()</span><br><span class="line">		t /= <span class="number">100</span></span><br><span class="line">		prefix := fmt.Sprintf(<span class="string">&quot;%06d %v &quot;</span>, t, <span class="keyword">string</span>(topic))</span><br><span class="line">		format = prefix + format</span><br><span class="line">		log.Printf(format, a...)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>PrettyDebug</code> 会通过环境变量 <code>VERBOSE</code> 来决定是否打印日志，该方法接受三个参数，第一个是日志主题用于对日志分组，后两个参数则是传递给 <code>log.Printf</code> 进行格式化打印，使用方法如下：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PrettyDebug(dTimer, <span class="string">&quot;S%d, apply log, log index=%v, log term=%v, log command=%v&quot;</span>, rf.me, entry.Index, entry.Term, entry.Command)</span><br></pre></td></tr></table></figure>

<p>日志信息中的 <code>S%d</code> 是关键，它表示当前节点的编号，如 <code>S0</code>，<code>S1</code>，按照这个模式打印日志，在后续日志处理时能将日志按照节点分组。</p>
<p>然后，就可以通过 <code>VERBOSE=1 go test -run TestFigure82C</code> 来进行测试（这里的 <code>TestFigure82C</code> 可以换成其他的测试用例）：</p>
<p><img src="/images/raft-lab-1.png" alt="alt"></p>
<p>不过所有日志都混到了一起，不好区分，作者因此提供了一个 <code>Python</code> 脚本 <a target="_blank" rel="noopener" href="https://gist.github.com/JJGO/e64c0e8aedb5d464b5f79d3b12197338">dslogs</a> 来美化日志。这个脚本用到了 <code>typer</code> 和 <code>rich</code> 两个库，可以通过 <code>pip</code> 全局安装。接着再执行测试 <code>VERBOSE=1 go test -run TestFigure82C | pipenv run python dslogs.py</code>（这里使用了 <code>pipenv</code> 来安装依赖和运行脚本，不使用 <code>pipenv</code> 的可以按照作者的方式执行），美化后的日志根据主题着色后有了更强的区分度：</p>
<p><img src="/images/raft-lab-2.png" alt="alt"></p>
<p>更进一步，还可以将日志按照节点分组展示 <code>VERBOSE=1 go test -run TestFigure82C | pipenv run python dslogs.py -c 3</code>：</p>
<p><img src="/images/raft-lab-3.png" alt="alt"></p>
<p>在上图中，每一列表示一个节点的日志，而且自上而下随时间排序。</p>
<h3 id="批量测试"><a href="#批量测试" class="headerlink" title="批量测试"></a>批量测试</h3><p>做实验时有时候测试用例成功了，有时候失败了，每次手动测试不方便抓取日志，<a target="_blank" rel="noopener" href="https://blog.josejg.com/debugging-pretty/">Debugging by Pretty Printing</a> 的作者提供了另一个脚本 <a target="_blank" rel="noopener" href="https://gist.github.com/JJGO/0d73540ef7cc2f066cb535156b7cbdab">dstest</a> 来进行批量测试，并且当测试失败时自动保存日志到文件中，从而可以使用上面提到的脚本 <code>dslogs</code> 来处理日志，<code>dstest</code> 这个脚本也依赖 <code>typer</code> 和 <code>rich</code> 这两个库。</p>
<p>然后通过 <code>pipenv run python dstest.py 2A -n 10 -v 1</code> 进行批量测试，这里 <code>2A</code> 可以换成其他的测试用例，<code>-n 10</code> 表示测试多少次，默认是10，<code>-v 1</code> 表示设置环境变量 <code>VERBOSE</code>，这样就能告诉 <code>Go</code> 打印日志：</p>
<p><img src="/images/raft-lab-4.png" alt="alt"></p>
<p><img src="/images/raft-lab-5.png" alt="alt"></p>
<p>脚本貌似有个小问题，当设置 <code>-v x</code> 参数时，会多一个名为 <code>x</code> 的测试任务，不过并不影响使用。</p>
<p>如果某次测试执行失败，则会保存相应的日志：</p>
<p><img src="/images/raft-lab-6.png" alt="alt"></p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="2A"><a href="#2A" class="headerlink" title="2A"></a>2A</h3><p>第一个实验是选主，关键有两点：随机化的 <code>election timeout</code> 和什么时候重置 <code>election timeout</code>。</p>
<p>当候选节点发出 <code>RequestVote</code> 请求后，应该在哪里判断是否获得了足够的选票？一种是在遍历完所有从节点发出 <code>RequestVote</code> 请求后，不过由于 <code>RPC</code> 的异步性，需要某种异步通知机制来通知当前的 <code>goroutine</code>。可以使用 <code>sync.WaitGroup</code>，事先计算好需要多少张选票才能成为主节点，发送 <code>RPC</code> 请求前调用 <code>WaitGroup.Add(1)</code>，每当获得一张选票后就调用 <code>WaitGroup.Done()</code>，当获得了足够的选票后当前 <code>goroutine</code> 就能被唤醒，不过由于当前节点不一定能成为主节点，所以存在无法被唤醒的可能。虽然可以把 <code>WaitGroup</code> 设置成所有 <code>RPC</code> 都响应后再唤醒，不过整个响应时间就受限于最慢的 <code>RPC</code> 请求，等待时间可能会超过一个 <code>election timeout</code> 周期。使用这种方式的一个很大的问题就是无法及时响应其他候选节点成为主节点的情况，因为当前候选节点还阻塞在 <code>WaitGroup.Wait()</code>。</p>
<p>所以可以将是否获得了足够的选票的判断放在每个 <code>RequestVote</code> 的响应中。先初始化需要的选票数量，每次获得选票后使用原子方法 <code>atomic.AddInt32</code> 对票数减1，当返回票数小于等于0时，说明当前候选节点成为了主节点。</p>
<h3 id="2B"><a href="#2B" class="headerlink" title="2B"></a>2B</h3><p>第二个实验需要实现日志复制。日志是 <code>Raft</code> 的核心部分，首先定义 <code>LogEntry</code>，包含三个字段，索引、任期、指令：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> LogEntry <span class="keyword">struct</span> &#123;</span><br><span class="line">	Index   <span class="keyword">int</span></span><br><span class="line">	Term    <span class="keyword">int</span></span><br><span class="line">	Command <span class="keyword">interface</span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>之所以这里需要 <code>Index</code> 是因为需要对日志压缩，所以不能使用 <code>rf.log</code> 的数组下标作为日志项的索引。</p>
<p>复制日志时，可以选择在调用 <code>Start</code> 方法时就发送 <code>AppendEntries</code> 请求，并且在响应中判断从节点的日志是否匹配来更新 <code>prevLogIndex</code>，然后继续发送 <code>AppendEntries</code> 请求。不过，这会造成两个问题。</p>
<p>第一个问题是冗余的 <code>RPC</code> 请求，假设客户端连续调用了10次 <code>Start</code>，那么根据当前的 <code>prevLogIndex</code> 计算，主节点所发送的 <code>AppendEntries</code> 请求中分别包含1条日志，2条日志，…，10条日志。然而这10次 <code>AppendEntries</code> 请求完全可以由第10条请求替代，而如果 <code>prevLogIndex</code> 不匹配，主从节点间来回协调的过程又会带来更多的 <code>RPC</code> 交互，最终有可能导致测试用例 <code>TestCount2B</code> 的失败。</p>
<p>第二个问题是测试用例会模拟出特别不稳定的网络，如果在 <code>AppendEntries</code> 的响应中接着递归异步调用 <code>AppendEntries</code>，由于 <code>goroutine</code> 都在等待网络可能会造成同时存在的 <code>goroutine</code> 数量过多，导致测试失败。</p>
<p>所以，可以选择不在 <code>Start</code> 中发送带日志的 <code>AppendEntries</code> 请求，而是在常规心跳中根据 <code>nextIndex</code> 计算是否要发送日志。</p>
<h3 id="2C"><a href="#2C" class="headerlink" title="2C"></a>2C</h3><p>第三个实验是持久化，虽然从代码编写角度来说是所有实验中最简单和直白的，但是测试用例并不会比其他实验简单。特别是 <code>TestFigure8Unreliable2C</code>，容易在指定时间内无法提交某条日志，一方面是可以批量发送日志而不是逐条发送，另一方面是及时识别过期的 <code>RPC</code> 请求并丢弃，例如如果响应中的任期小于当前任期则可以直接忽略该响应，因为从节点收到请求时会更新任期（如果从节点的任期比主节点的小），并将更新后的任期放到响应中，所以在当前任期下主节点收到的响应中的任期必然等于当前任期，如果收到了小于当前任期的响应，必然是过期的响应。</p>
<h3 id="2D"><a href="#2D" class="headerlink" title="2D"></a>2D</h3><p>第四个实验遇到两个死锁问题。第一个死锁发生在应用已提交的日志，日志的应用会由一个单独的 <code>goroutine</code> 执行，它会遍历所有需要应用的日志，然后发送到 <code>applyCh</code>，并且在整个期间持有锁：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">applyLog</span><span class="params">(applyCh <span class="keyword">chan</span> ApplyMsg)</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> rf.killed() == <span class="literal">false</span> &#123;</span><br><span class="line">		rf.mu.Lock()</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> rf.lastApplied &lt; rf.commitIndex &#123;</span><br><span class="line">			<span class="keyword">for</span> i := rf.lastApplied + <span class="number">1</span>; i &lt;= rf.commitIndex; i++ &#123;</span><br><span class="line">				logEntry := rf.log[i]</span><br><span class="line">				applyMsg := ApplyMsg&#123;</span><br><span class="line">					CommandValid: <span class="literal">true</span>,</span><br><span class="line">					Command:      logEntry.Command,</span><br><span class="line">					CommandIndex: logEntry.Index,</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				applyCh &lt;- applyMsg</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			rf.lastApplied = rf.commitIndex</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		rf.mu.Unlock()</span><br><span class="line"></span><br><span class="line">		time.Sleep(time.Millisecond * <span class="number">10</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这种处理方式在之前的实验中没有问题，不过在 <code>2D</code> 中，客户端从 <code>applyCh</code> 中取出数据后，有一定概率会调用 <code>Snapshot</code> 方法，而实现 <code>Snapshot</code> 方法时会继续获取锁，从而造成死锁：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (m.CommandIndex+<span class="number">1</span>)%SnapShotInterval == <span class="number">0</span> &#123;</span><br><span class="line">	w := <span class="built_in">new</span>(bytes.Buffer)</span><br><span class="line">	e := labgob.NewEncoder(w)</span><br><span class="line">	e.Encode(m.CommandIndex)</span><br><span class="line">	<span class="keyword">var</span> xlog []<span class="keyword">interface</span>&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> j := <span class="number">0</span>; j &lt;= m.CommandIndex; j++ &#123;</span><br><span class="line">		xlog = <span class="built_in">append</span>(xlog, cfg.logs[i][j])</span><br><span class="line">	&#125;</span><br><span class="line">	e.Encode(xlog)</span><br><span class="line">	rf.Snapshot(m.CommandIndex, w.Bytes())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个问题也在 <a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt">Raft Locking Advice</a> 中提到，不建议在等待某个事件时持有锁。</p>
<p>第二个死锁发生在 <code>InstallSnapshot</code>，从节点收到快照后也会通过 <code>applyCh</code> 将快照发送给客户端，这里将 <code>applyCh</code> 作为 <code>Raft</code> 的一个字段使用，不过由于忘记赋值造成 <code>InstallSnapshot</code> 往一个空 <code>channel</code> 中发数据，造成始终阻塞，并导致死锁。</p>
<h2 id="其他工具"><a href="#其他工具" class="headerlink" title="其他工具"></a>其他工具</h2><h3 id="go-deadlock"><a href="#go-deadlock" class="headerlink" title="go-deadlock"></a>go-deadlock</h3><p>如果怀疑有死锁，可以使用 <a target="_blank" rel="noopener" href="https://github.com/sasha-s/go-deadlock">go-deadlock</a> 检测，只需要将 <code>Raft</code> 中的 <code>sync.Mutex</code> 替换成 <code>deadlock.Mutex</code> 即可，如果某个 <code>goroutine</code> 在较长的一段时间后依然无法获取锁，那么就有可能发生了死锁，<code>go-deadlock</code> 会输出持有锁的 <code>goroutine</code> 和希望获取锁的 <code>goroutine</code>，而且也会输出持有锁的 <code>goroutine</code> 阻塞在哪个代码上：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">POTENTIAL DEADLOCK:</span><br><span class="line">Previous place where the lock was grabbed</span><br><span class="line">goroutine <span class="number">240</span> lock <span class="number">0xc820160440</span></span><br><span class="line">inmem.<span class="keyword">go</span>:<span class="number">799</span> bttest.(*table).gc &#123; t.mu.RLock() &#125; &lt;&lt;&lt;&lt;&lt;</span><br><span class="line">inmem_test.<span class="keyword">go</span>:<span class="number">125</span> bttest.TestConcurrentMutationsReadModifyAndGC.func5 &#123; tbl.gc() &#125;</span><br><span class="line"></span><br><span class="line">Have been trying to lock it again <span class="keyword">for</span> more than <span class="number">40</span>ms</span><br><span class="line">goroutine <span class="number">68</span> lock <span class="number">0xc820160440</span></span><br><span class="line">inmem.<span class="keyword">go</span>:<span class="number">785</span> bttest.(*table).mutableRow &#123; t.mu.Lock() &#125; &lt;&lt;&lt;&lt;&lt;</span><br><span class="line">inmem.<span class="keyword">go</span>:<span class="number">428</span> bttest.(*server).MutateRow &#123; r := tbl.mutableRow(<span class="keyword">string</span>(req.RowKey)) &#125;</span><br><span class="line">inmem_test.<span class="keyword">go</span>:<span class="number">111</span> bttest.TestConcurrentMutationsReadModifyAndGC.func3 &#123; s.MutateRow(ctx, req) &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Here is what goroutine <span class="number">240</span> doing now</span><br><span class="line">goroutine <span class="number">240</span> [<span class="keyword">select</span>]:</span><br><span class="line">github.com/sasha-s/<span class="keyword">go</span>-deadlock.lock(<span class="number">0xc82028ca10</span>, <span class="number">0x5189e0</span>, <span class="number">0xc82013a9b0</span>)</span><br><span class="line">        /Users/sasha/<span class="keyword">go</span>/src/github.com/sasha-s/<span class="keyword">go</span>-deadlock/deadlock.<span class="keyword">go</span>:<span class="number">163</span> +<span class="number">0x1640</span></span><br><span class="line">github.com/sasha-s/<span class="keyword">go</span>-deadlock.(*Mutex).Lock(<span class="number">0xc82013a9b0</span>)</span><br><span class="line">        /Users/sasha/<span class="keyword">go</span>/src/github.com/sasha-s/<span class="keyword">go</span>-deadlock/deadlock.<span class="keyword">go</span>:<span class="number">54</span> +<span class="number">0x86</span></span><br><span class="line">google.golang.org/cloud/bigtable/bttest.(*table).gc(<span class="number">0xc820160440</span>)</span><br><span class="line">        /Users/sasha/<span class="keyword">go</span>/src/google.golang.org/cloud/bigtable/bttest/inmem.<span class="keyword">go</span>:<span class="number">814</span> +<span class="number">0x28d</span></span><br><span class="line">google.golang.org/cloud/bigtable/bttest.TestConcurrentMutationsReadModifyAndGC.func5(<span class="number">0xc82015c760</span>, <span class="number">0xc820160440</span>)      /Users/sasha/<span class="keyword">go</span>/src/google.golang.org/cloud/bigtable/bttest/inmem_test.<span class="keyword">go</span>:<span class="number">125</span> +<span class="number">0x48</span></span><br><span class="line">created by google.golang.org/cloud/bigtable/bttest.TestConcurrentMutationsReadModifyAndGC</span><br><span class="line">        /Users/sasha/<span class="keyword">go</span>/src/google.golang.org/cloud/bigtable/bttest/inmem_test.<span class="keyword">go</span>:<span class="number">126</span> +<span class="number">0xb6f</span></span><br></pre></td></tr></table></figure>

<h3 id="pprof"><a href="#pprof" class="headerlink" title="pprof"></a>pprof</h3><p><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/labs/lab-raft.html">6.824 Lab 2: Raft</a> 的每个实验都给出了参考的执行时间，如果发现某个实验的执行时间相差太大，可以使用 <code>pprof</code> 分析。这里以 <code>CPU</code> 耗时分析为例，首先在测试时增加 <code>-cpuprofile cpu.prof</code> 参数，其中 <code>cpu.prof</code> 是输出文件名：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">go</span> test -run TestInitialElection2A -cpuprofile cpu.prof</span><br></pre></td></tr></table></figure>

<p>然后安装 <a target="_blank" rel="noopener" href="https://github.com/google/pprof">pprof</a> 并执行 <code>pprof -top cpu.prof</code> 分析：</p>
<p><img src="/images/raft-lab-7.png" alt="alt"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/labs/lab-raft.html">6.824 Lab 2: Raft</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.josejg.com/debugging-pretty/">Debugging by Pretty Printing</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/google/pprof">pprof</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/sasha-s/go-deadlock">go-deadlock</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://frederick-s.github.io/2022/05/07/mit-6.824-lab2-raft-locking-advice/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaodan Mao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Übung macht den Meister">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/07/mit-6.824-lab2-raft-locking-advice/" class="post-title-link" itemprop="url">MIT 6.824 - Lab 2 (2): Raft Locking Advice</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-07 00:00:00" itemprop="dateCreated datePublished" datetime="2022-05-07T00:00:00+08:00">2022-05-07</time>
            </span>

          
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>1.6k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>3 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt">Raft Locking Advice</a> 提供了些关于如何在 <code>Lab 2</code> 中使用锁的建议。</p>
<h2 id="规则1"><a href="#规则1" class="headerlink" title="规则1"></a>规则1</h2><p>只要有多个 <code>goroutine</code> 访问同一份数据，并且至少有一个 <code>goroutine</code> 会修改数据，那么就需要对数据加锁保护。建议在测试时开启 <code>Go</code> 的竞争检测（添加 <code>-race</code> 标记）来识别这类问题。</p>
<h2 id="规则2"><a href="#规则2" class="headerlink" title="规则2"></a>规则2</h2><p>如果有多个数据需要作为一个整体被修改，为了避免其他的 <code>goroutine</code> 看到部分数据更新而造成不正确的行为，此时也需要加锁。例如：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rf.mu.Lock()</span><br><span class="line">rf.currentTerm += <span class="number">1</span></span><br><span class="line">rf.state = Candidate</span><br><span class="line">rf.mu.Unlock()</span><br></pre></td></tr></table></figure>

<p>上面的代码需要同时更新 <code>rf.currentTerm</code> 和 <code>rf.state</code>，如果不加锁其他 <code>goroutine</code> 有可能看到更新后的任期，但是节点状态还未更新。同时，其他任何地方用到 <code>rf.currentTerm</code> 或者 <code>rf.state</code> 的地方也必须先持有锁，一是保证可见性，二是避免在多处同时修改 <code>rf.currentTerm</code> 或者 <code>rf.state</code>。</p>
<h2 id="规则3"><a href="#规则3" class="headerlink" title="规则3"></a>规则3</h2><p>如果需要对某个数据做一系列读操作（或者读写混合），那么为了避免其他 <code>goroutine</code> 在中途修改数据，就需要对这一系列操作加锁。例如：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rf.mu.Lock()</span><br><span class="line"><span class="keyword">if</span> args.Term &gt; rf.currentTerm &#123;</span><br><span class="line">    rf.currentTerm = args.Term</span><br><span class="line">&#125;</span><br><span class="line">rf.mu.Unlock()</span><br></pre></td></tr></table></figure>

<p>上面的代码是典型的 <code>如果满足某个条件，那么就执行 xxx</code> 场景。如果不加锁，可能其他的 <code>goroutine</code> 将 <code>rf.currentTerm</code> 更新后，当前 <code>goroutine</code> 会将 <code>rf.currentTerm</code> 重置为 <code>args.Term</code>，在 <code>Raft</code> 中有可能造成任期倒退。</p>
<p>在真实的 <code>Raft</code> 代码中加锁的粒度可能会更大，例如可能在整个 <code>RPC handler</code> 处理期间都持有锁。</p>
<h2 id="规则4"><a href="#规则4" class="headerlink" title="规则4"></a>规则4</h2><p>不建议在等待某个事件时持有锁，例如从 <code>channel</code> 中读取数据，向 <code>channel</code> 发送数据，计时器等待，调用 <code>time.Sleep()</code>，或者发送一个 <code>RPC</code> 请求并等待响应结果。因为有可能造成死锁，文中举了两个节点互发 <code>RPC</code> 请求并希望获取对方持有的锁的例子，这是个典型的死锁场景。</p>
<p>又或者某个 <code>goroutine</code> 先持有锁，但是使用 <code>time.Sleep</code> 来等待某个条件发生，其他的 <code>goroutine</code> 由于无法获取锁从而使得等待的条件永远无法成立，这个时候应该用 <code>sync.Cond</code>：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mu.Lock()</span><br><span class="line"></span><br><span class="line">while (!someCondition) &#123;</span><br><span class="line">    time.Sleep(time.Millisecond * <span class="number">1000</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mu.Unlock()</span><br></pre></td></tr></table></figure>

<h2 id="规则5"><a href="#规则5" class="headerlink" title="规则5"></a>规则5</h2><p>当释放锁然后重新获取锁之后，某些释放锁之前成立的条件可能此时已经不成立。例如下面的候选节点获取选票的实现是不正确的：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rf.mu.Lock()</span><br><span class="line">rf.currentTerm += <span class="number">1</span></span><br><span class="line">rf.state = Candidate</span><br><span class="line"><span class="keyword">for</span> &lt;each peer&gt; &#123;</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        rf.mu.Lock()</span><br><span class="line">        args.Term = rf.currentTerm</span><br><span class="line">        rf.mu.Unlock()</span><br><span class="line">        Call(<span class="string">&quot;Raft.RequestVote&quot;</span>, &amp;args, ...)</span><br><span class="line">        <span class="comment">// handle the reply...</span></span><br><span class="line">    &#125; ()</span><br><span class="line">&#125;</span><br><span class="line">rf.mu.Unlock()</span><br></pre></td></tr></table></figure>

<p>在每个 <code>goroutine</code> 中，重新获取锁后拿到的任期可能已经不是当初的任期。这里需要将 <code>goroutine</code> 中的 <code>rf.currentTerm</code> 提取到循环之外作为一个变量，然后在 <code>goroutine</code> 中访问这个变量。另外，在 <code>Call</code> 执行完成后，也需要再次获取锁并检查 <code>rf.currentTerm</code> 或其他变量是否还满足条件，例如需要检查下当前的任期是否还是最初的任期，如果不是那说明又开启了一轮选主或者已经有其他节点成为了主节点。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt">Raft Locking Advice</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://frederick-s.github.io/2022/05/06/mit-6.824-lab2-students-guide-to-raft/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaodan Mao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Übung macht den Meister">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/06/mit-6.824-lab2-students-guide-to-raft/" class="post-title-link" itemprop="url">MIT 6.824 - Lab 2 (1): Students' Guide to Raft</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-06 00:00:00" itemprop="dateCreated datePublished" datetime="2022-05-06T00:00:00+08:00">2022-05-06</time>
            </span>

          
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>6.3k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>10 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://thesquareplanet.com/blog/students-guide-to-raft/">Students’ Guide to Raft</a> 是 <a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/">MIT 6.824: Distributed Systems</a> 之前的助教写给学生看的实验生存指南。</p>
<h2 id="实现-Raft"><a href="#实现-Raft" class="headerlink" title="实现 Raft"></a>实现 Raft</h2><p>论文中的 <code>Figure 2</code> 和 <code>Figure 13</code> 描述了实现 <code>Raft</code> 的主要接口，以及各节点需要维护的状态和响应接口时的行为逻辑，所以在做 <code>Lab 2</code> 之前需要先充分理解这部分的内容。文中的建议是<strong>必须</strong>严格落实图中的每一句话，而不是仅仅实现了功能，否则就有可能遇到奇怪的问题或不正确的系统行为。</p>
<p>文中举了三个例子。第一，每次收到 <code>AppendEntries</code> 或 <code>RequestVote</code> 请求就重置 <code>election timeout</code>，因为收到这两个请求说明存在主节点或者正在选主。不过，图中还有描述：</p>
<blockquote>
<p>If election timeout elapses without receiving AppendEntries RPC from current leader or granting vote to candidate: convert to candidate.</p>
</blockquote>
<p>重置 <code>election timeout</code> 有两个关键条件：<code>AppendEntries</code> 要求发送请求的主节点是当前任期的主节点；<code>RequestVote</code> 要求成功投出选票。</p>
<p>先看第一个条件，假设当前任期的主节点异常，此时还有一个之前任期的主节点存活在不断发送 <code>AppendEntries</code> 请求，假设此时某个从节点马上要进入选主状态，但由于收到了 <code>AppendEntries</code> 请求又没有校验任期就重置了 <code>election timeout</code>，造成无主状态时间延长。这个场景下如果过期的主节点不实现下面的逻辑杀伤力倍增：</p>
<blockquote>
<p>If RPC request or response contains term T &gt; currentTerm: set currentTerm = T, convert to follower</p>
</blockquote>
<p>如果不实现上面的逻辑，那么过期的主节点会一直是主节点，然后一直重置其他本来可以成为候选节点的从节点。</p>
<p>再来看第二个条件，假设当前任期的主节点异常，各从节点准备进入选主状态，此时一个不可能成为主节点的从节点先变成了候选节点（该节点的日志相对于其他任何一个从节点来说都不够新，即最后一条日志的任期不够大，或者任期相同但是日志条数不够多），<code>RequestVote</code> 永远不会成功，如果各从节点一收到 <code>RequestVote</code> 就重置 <code>election timeout</code>，就会造成始终没有主节点的情况。</p>
<p>第二个例子，从节点收到心跳 <code>AppendEntries</code> 后就直接返回 <code>success = true</code> 和重置 <code>election timeout</code>。重置 <code>election timeout</code> 的危害上面已经说过，充当心跳的 <code>AppendEntries</code> 是不带日志的请求，文中提到学生在实现时容易将心跳 <code>AppendEntries</code> 特殊对待，而不进行 <code>prevLogIndex</code> 和 <code>prevLogTerm</code> 的校验。从节点返回 <code>true</code> 时主节点就会认为从节点的日志和自己匹配，从而错误的认为某些日志已经复制到了从节点上，从而可以提交日志。不过看到这里可能会有疑问，为什么主节点会在心跳响应中提交日志？通过心跳更新 <code>nextIndex</code> 和 <code>matchIndex</code> 是合理的，如果过半数节点已复制的日志是之前任期的，论文中有描述这是不允许提交的；如果过半数节点已复制的日志是当前任期的，那么可能是之前带日志的 <code>AppendEntries</code> 请求中从节点实际已完成了日志的复制，但是主节点没有收到响应，所以在最新的心跳中提交日志。</p>
<p>引申开来，假设系统中此时 <code>x = 1</code>，有个客户端发送了 <code>write x = 2</code> 的请求，主节点成功将日志复制到了过半数的节点上，但是还没有响应客户端就异常了。系统重新选主后，此时有个客户端发送请求 <code>read x</code>，<code>x</code> 应该返回多少？应该是2，因为在 <code>Raft</code> 层面，一条日志是否已提交只取决于是否在当前任期内复制到了过半数的节点上，而不是取决于客户端是否收到响应，而只要日志提交了就可以被应用到状态机。类似的场景可以想象一下平时填了某个表单，提交时提示系统异常，但是刷新页面后表单信息已更新。</p>
<p>第三个例子，学生在实现 <code>AppendEntries</code> 时，可能将 <code>prevLogIndex</code> 之后的日志都替换为请求中的日志。同样的，图中也有描述：</p>
<blockquote>
<p>If an existing entry conflicts with a new one (same index but different terms), delete the existing entry and all that follow it.</p>
</blockquote>
<p>因为当前的 <code>AppendEntries</code> 可能是个过期的请求，假设请求中的日志只是当前从节点已提交的日志的某一段子集，那么如果从节点将 <code>prevLogIndex</code> 之后的日志都替换为请求中的日志就会丢失已提交的日志。</p>
<h2 id="调试-Raft"><a href="#调试-Raft" class="headerlink" title="调试 Raft"></a>调试 Raft</h2><p>调试是实验中最花时间的一个部分，文中列举了四个常见的问题：活锁，不正确或不完整的 <code>RPC</code> 实现，没有遵循论文中的 <code>Rules for Servers</code> 以及任期混淆。</p>
<h3 id="活锁"><a href="#活锁" class="headerlink" title="活锁"></a>活锁</h3><p>出现活锁时，等同于各节点都在做无用功。选主是最容易出现活锁的场景，例如始终无法选出一个主节点，或者某个候选节点获得了足够的选票后，马上有另外一个节点发起选主，使得刚成为主节点的节点重新退回到从节点（某个节点成为主节点后，还没来得及发送心跳，刚投了票的某个节点 <code>election timeout</code> 到期，发起新的选主，由于它的任期加了1比当前的主节点的任期大，主节点收到 <code>RequestVote</code> 后发现自己的任期小，从而转为从节点）。</p>
<p>常见的原因有以下几种：</p>
<p>第一，没有严格按照 <code>Figure 2</code> 的要求重置 <code>election timeout</code>。重置 <code>election timeout</code> 仅限于几种情况：</p>
<ol>
<li>收到当前任期的主节点的 <code>AppendEntries</code> 请求（如果 <code>AppendEntries</code> 中的任期比自身的任期小，则忽略该请求）。</li>
<li>开启一轮新的选主，这个属于 <code>election timeout</code> 到期，选主的同时需要重置一个新的随机值。</li>
<li>在某轮选举中，将票投给了某个候选节点。</li>
</ol>
<p>第二，没有按照 <code>Fiture 2</code> 的要求开启选主。特别的，如果当前节点是候选节点，但是 <code>election timeout</code> 到期了，那么需要开启新的一轮选主。</p>
<p>第三，没有遵循 <code>Rules for Servers</code> 的第二条：</p>
<blockquote>
<p>If RPC request or response contains term T &gt; currentTerm: set currentTerm = T, convert to follower (§5.1)</p>
</blockquote>
<p>例如，当前节点已投票，按照 <code>Raft</code> 的要求，一个从节点在一个任期内只能投票一次，此时又收到了一个 <code>RequestVote</code> 请求，请求中的任期大于自身的任期，那么就需要再次投票并更新自身的任期，避免新的候选节点拿不到足够的选票。</p>
<h3 id="不正确的-RPC-实现"><a href="#不正确的-RPC-实现" class="headerlink" title="不正确的 RPC 实现"></a>不正确的 <code>RPC</code> 实现</h3><p>这里总结了学生们过往在实现 <code>RPC</code> 接口时容易犯错的点：</p>
<ul>
<li>如果某一步要求 <code>reply false</code>，那么这说明需要立即回复 <code>false</code> 并返回，不要再执行后续的步骤。</li>
<li>如果收到了一条 <code>AppendEntries</code> 请求，但是请求中的 <code>prevLogIndex</code> 比当前日志的最后一条的索引还要大，也要返回 <code>false</code>，这也属于日志不匹配。</li>
<li>如前面描述，即使 <code>AppendEntries</code> 没有包含任何日志，接收方也需要校验 <code>prevLogIndex</code> 和 <code>prevLogTerm</code>。</li>
<li><code>AppendEntries</code> 中的第五步的 <code>min</code> 是有必要的，而且是和新日志中的最后一条日志的索引相比较，而不是当前日志的最后一条索引。<ul>
<li>首先来看 <code>leaderCommit &lt; index of last new entry</code> 的情况，假设 <code>S1</code> 为主节点，日志为 <code>[1, 2, 3, 4, 5]</code>，<code>leaderCommit</code> 为3（索引按从1开始算），<code>S2</code> 为从节点，日志为 <code>[1, 2]</code>，那么 <code>S1</code> 需要将 <code>[3, 4, 5]</code> 发给 <code>S2</code>，<code>S2</code> 收到后 <code>index of last new entry</code> 是5，但是 <code>commitIndex</code> 只能更新到 <code>3</code>，也就是 <code>leaderCommit</code>，因为 <code>[4, 5]</code> 还没有提交。</li>
<li>然后来看 <code>leaderCommit &gt; index of last new entry</code> 的情况，乍看之下好像不可能，因为对于主节点来说，在添加一条新日志的那个时刻，<code>leaderCommit</code> 肯定小于新的日志的索引，复制日志到从节点时这个关系也不会变。这里只能想到的是，<code>leaderCommit</code> 被并发修改了，因为主节点不会删除日志，所以 <code>index of last new entry</code> 不会变，当主节点和从节点的日志不匹配时，主从节点需要来回的就 <code>prevLogIndex</code> 和 <code>prevLogTerm</code> 达成一致（或者单纯就是因为执行的慢），在这个期间，可能过半数的节点已经达成了完成日志复制，主节点就可以提交这条日志，并更新 <code>leaderCommit</code>，所以在原先的 <code>AppendEntries</code> 中，如果 <code>leaderCommit</code> 引用的是全局变量，而 <code>entries[]</code> 是固定的，就会造成 <code>leaderCommit</code> 的值比 <code>entries[]</code> 的最后一条日志的索引还要大，这个时候从节点的 <code>commitIndex</code> 就需要取 <code>index of last new entry</code>。文中举了个例子说明为什么这时候不能取 <code>leaderCommit</code>，假设 <code>S1</code> 为主节点，日志为 <code>[1, 2, 3, 6]</code>，<code>leaderCommit</code> 为3，<code>S2</code> 为从节点，日志为 <code>[1, 2, 3, 4, 5]</code>，那么 <code>S1</code> 需要将 <code>[6]</code> 发给 <code>S2</code>，注意这里 <code>prevLogIndex</code> 和 <code>prevLogTerm</code> 校验通过，而论文中只提到校验不通过时才删除从节点的日志，所以这里就不会删除 <code>S2</code> 的 <code>[4, 5]</code>，又假设此时 <code>S1</code> 通过其他节点的交互将日志成功提交到了 <code>[1, 2, 3, 6, 7]</code>，<code>leaderCommit</code> 变为了第5个位置，<code>S2</code> 收到的 <code>AppendEntries</code> 中 <code>leaderCommit = 5</code>，<code>entries = [6]</code>，如果按照最新的 <code>leaderCommit</code> 更新就会造成把 <code>S2</code> 日志中的5归为已提交。</li>
</ul>
</li>
<li>严格按照论文5.4节的描述来比较两个节点的日志哪个较新，不要只比较日志的长度。</li>
</ul>
<h3 id="未遵循-Rules-for-Servers"><a href="#未遵循-Rules-for-Servers" class="headerlink" title="未遵循 Rules for Servers"></a>未遵循 Rules for Servers</h3><ul>
<li>如果 <code>commitIndex &gt; lastApplied</code>，说明可以应用新的日志到状态机中。应用到状态机的顺序必须严格按照日志的顺序，可以由一个单独的线程顺序执行，也可以由多个线程并行执行，但各线程间必须确保不互相干扰，避免执行覆盖，对于同一个变量的不同写请求，多线程间必须按照日志的顺序写入。</li>
<li>确保周期性的检查 <code>commitIndex &gt; lastApplied</code> 或者在 <code>commitIndex</code> 更新后检查（在 <code>matchIndex</code> 更新后）。这里并没有太明白这个问题，作者举了个例子，假设现在主节点发出了 <code>AppendEntries</code> 请求并同时检查 <code>commitIndex</code>，如果这条日志被过半数的节点复制了，那么需要等到主节点再添加一条新日志后才能应用上一条日志到状态机。不过根据这个例子也没有看出两者的关联，因为如果采用单线程顺序应用日志到状态机的方式，如果 <code>commitIndex &gt; lastApplied</code> 不满足，线程可以先睡眠然后再尝试，除此以外没有依赖其他条件。</li>
<li>假设主节点发出 <code>AppendEntries</code> 请求然后被拒绝了，如果不是因为日志不一致被拒绝，那么这个主节点就必须马上转为从节点，且不能更新 <code>nextIndex</code>，因为这说明当前主节点的任期小于从节点。如果错误的更新了 <code>nextIndex</code>，而这个主节点在转为从节点后又当选了主节点，就会造成 <code>nextIndex</code> 和从节点永远不会匹配的情况（相当于错位了一格）。</li>
<li>如果主节点发现某条过去任期的日志被大多数的节点复制了，但是自身的 <code>commitIndex</code> 却小于这条日志的索引，此时主节点也不能够更新 <code>commitIndex</code>，这个就是论文中 <code>Figure 8</code> 描述的问题，提交一条之前任期的日志存在被新一轮的主节点覆盖的风险。所以提交日之前需要判断 <code>log[N].term == currentTerm</code>。</li>
</ul>
<p>有一个常见的困惑是 <code>nextIndex</code> 和 <code>matchIndex</code> 的区别。你可能会观察到 <code>matchIndex = nextIndex - 1</code>，然后用 <code>nextIndex</code> 来替代 <code>matchIndex</code>，然而这是有风险的。举一个很明显的反例，假设某个节点被选为主节点，此时收到一个客户端请求并添加了一条日志，那么它发给从节点的 <code>nextIndex</code> 就是最后一条日志的下一条，而 <code>matchIndex</code> 显然不是 <code>nextIndex - 1</code>（新添加的日志的索引）。</p>
<h3 id="任期混淆"><a href="#任期混淆" class="headerlink" title="任期混淆"></a>任期混淆</h3><p>任期混淆指的是节点收到了来自之前任期的 <code>RPC</code> 请求。<code>Figure 2</code> 清晰的描述了收到过期任期的请求应该怎么做，但是并没有说明收到过期任期的响应应该怎么做。一个简单的做法就是比较响应中的任期和当前的任期，如果相等则执行相应的逻辑，如果当前任期小则转为从节点（只有主节点、候选节点才能发请求，才有收到响应的可能），如果当前任期大则不处理。</p>
<p>还有一个相关但不同的问题，假设主节点收到 <code>AppendEntries</code> 响应后设置 <code>matchIndex = nextIndex - 1</code> 或 <code>matchIndex = len(log)</code>，这都是不安全的，因为 <code>nextIndex</code> 和 <code>len(log)</code> 在这期间都有可能改变，正确的做法应该是使用原始请求中不变的参数，即 <code>matchIndex = prevLogIndex + len(entries[])</code>。</p>
<h3 id="关于性能"><a href="#关于性能" class="headerlink" title="关于性能"></a>关于性能</h3><p>实验中会要求实现两个关于性能方面的优化，一个是日志压缩，另一个是快速确认正确的 <code>prevLogIndex</code>。</p>
<p>关于日志压缩需要注意几点：</p>
<ul>
<li>当执行快照时，需要确保快照中的状态和指定日志中的索引是严格对应的，即快照不能落后于日志，也不能超前。</li>
<li>论文中并没有描述当节点异常然后恢复后应该如何处理快照。论文中要求创建完快照后，需要删除快照所对应的日志，而系统可能在删除日志前异常，那么系统恢复后会根据快照和日志进行重放。文中说系统可能会重放已经在快照中的日志，这里有点疑问，因为快照中包含了 <code>lastIncludedIndex</code> 和 <code>lastIncludedTerm</code>，和当前日志进行对比就能知道哪些日志已经在快照中了，从而可以跳过。</li>
</ul>
<p>论文中并没有详细描述如何快速确认正确的 <code>prevLogIndex</code>，这部分的逻辑可以参考 <a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/notes/l-raft2.txt">6.824 2022 Lecture 7: Raft (2)</a> 或者文中的建议：</p>
<ul>
<li>如果从节点在 <code>prevLogIndex</code> 处没有日志，说明从节点的日志较短，则返回 <code>conflictIndex = len(log)</code> 以及 <code>conflictTerm = None</code>。</li>
<li>如果从节点在 <code>prevLogIndex</code> 处有日志，但是日志对应的任期不匹配，那么返回 <code>conflictTerm = log[prevLogIndex].Term</code>，然后找到属于 <code>conflictTerm</code> 的第一条日志的索引。</li>
<li>主节点收到 <code>conflictIndex/conflictTerm</code> 后，首先在日志中搜索 <code>conflictTerm</code>，如果找到有日志属于 <code>conflictTerm</code>，那么 <code>nextIndex</code> 需要更新为主节点中属于 <code>conflictTerm</code> 的最后一条日志的索引加1。</li>
<li>如果主节点没有找到属于 <code>conflictTerm</code> 的日志，那么更新 <code>nextIndex</code> 为 <code>conflictIndex</code>。</li>
</ul>
<p>其中一个不完善的实现是只考虑 <code>conflictIndex</code> 而忽略了 <code>conflictTerm</code>，这样会简化实现，不过主从节点交互的次数会变多，因为如果考虑了 <code>conflictTerm</code>，那么一次性能跳过的日志会更多。</p>
<p>最后一部分的 <code>Applications on top of Raft</code> 属于 <code>Lab 3</code> 的内容，即基于 <code>Raft</code> 构建应用，在 <code>Lab 2</code> 中暂不描述。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://thesquareplanet.com/blog/students-guide-to-raft/">Students’ Guide to Raft</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://frederick-s.github.io/2022/05/03/mit-6.824-raft/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaodan Mao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Übung macht den Meister">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/03/mit-6.824-raft/" class="post-title-link" itemprop="url">MIT 6.824 - In Search of an Understandable Consensus Algorithm (Extended Version)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-03 00:00:00" itemprop="dateCreated datePublished" datetime="2022-05-03T00:00:00+08:00">2022-05-03</time>
            </span>

          
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>28k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>46 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>共识算法使得一批机器作为一个整体对外提供服务，同时当部分机器异常时系统仍能保证可用性。因此，共识算法在构建可靠的大型软件系统中扮演着至关重要的角色。而在所有的共识算法中，<code>Paxos</code> 占据了主导的地位：大部分共识算法的实现都是基于 <code>Paxos</code> 或者受其影响，并且它也成为了教授学生们共识算法的第一选择。</p>
<p>然而，尽管人们为了让 <code>Paxos</code> 易于理解做了大量的尝试，<code>Paxos</code> 依然非常难以理解。另外，如果要将 <code>Paxos</code> 应用到实际的系统中需要涉及复杂的修改。因此，系统开发人员和学生都受困于 <code>Paxos</code>。</p>
<p>在受困于 <code>Paxos</code> 之后，<code>Raft</code> 的作者开始尝试设计一种新的共识算法，从而能够为系统构建和教学提供一个更好的基础。和常见的共识算法的设计目标不同，这个新的共识算法的首要设计目标是可理解性：能否为实际的系统设计一个远比 <code>Paxos</code> 易于理解的共识算法？另外，对于系统构建者来说这个算法需要能易于实现。该算法仅仅是正确的还不够，重要的是能显而易见的被人们理解为什么是正确的。</p>
<p>这个新的共识算法就是 <code>Raft</code>。在设计 <code>Raft</code> 时为了提高可理解性作者借助了某些特定的手段，包括解耦（<code>Raft</code> 分离了选主，日志复制和安全性的处理）和减少了状态机的状态（相比于 <code>Paxos</code>，<code>Raft</code> 减少了非确定性的状态以及各服务器间不一致的情况）。根据两所大学共43名学生的调查反馈，<code>Raft</code> 明显比 <code>Paxos</code> 易于理解：在学习了两种共识算法后，有33名学生在回答关于 <code>Raft</code> 的问题时比回答关于 <code>Paxos</code> 的问题表现的更好。</p>
<p><code>Raft</code> 和现今已有的共识算法有很多相之处（特别是 <code>Oki</code> 和 <code>Liskov</code> 的 <code>Viewstamped Replication</code>），不过也有几个方面的创新：</p>
<ul>
<li>强主节点（<code>Strong leader</code>）：相比于其他共识算法，<code>Raft</code> 使用了更严格的主节点要求。例如，日志只会从主节点发往其他服务器。这简化了日志复制的管理，同时也使得 <code>Raft</code> 更易于理解。</li>
<li>选主（<code>Leader election</code>）：<code>Raft</code> 使用随机计时器来进行选主（后面会提到主节点和各从节点间会维持一个心跳，如果在一段时间内从节点没有收到心跳，那么就可以认为主节点异常，从而重新发起选主，对于每个从节点来说，这个等待的时间不是固定的，而是随机的）。因为心跳本身在共识算法中是一个必不可少的技术，使用随机计时器仅仅在这之上增加了一些额外的机制，却能简单快速的解决选主冲突问题（例如有两个节点瓜分了存活着的节点的全部选票，却没有任何一个节点获得了超过半数的选票，需要重新选主）。</li>
<li>节点变更（<code>Membership changes</code>）：当集群中的节点需要变更时，<code>Raft</code> 使用 <code>joint consensus</code> 机制来保证在变更时新旧两套集群下过半数的节点同时属于两套集群。这就保证了整个集群在节点变更时依然正常对外提供服务。</li>
</ul>
<p><code>Raft</code> 的作者认为不管是出于教学还是实现目的，<code>Raft</code> 都优于 <code>Paxos</code> 和其他共识算法。它相比于其他共识算法更简单和易于理解；也完全覆盖了实现一个实际系统的需求；它也有了一些开源的实现并且已经被一些公司所使用；它的安全性也已被正式定义和证明；其性能也不输给其他共识算法。</p>
<h2 id="复制状态机"><a href="#复制状态机" class="headerlink" title="复制状态机"></a>复制状态机</h2><p>谈论共识算法时一般离不开复制状态机（<code>replicated state machines</code>）。在这个模型下，集群中的每台机器上的状态机能产生有着相同状态的副本，并且在某些机器异常时整个系统依然能对外提供服务。复制状态机被用于解决分布式系统中的一系列容错问题。例如，对于 <code>GFS</code>、<code>HDFS</code> 和 <code>RAMCloud</code> 这样的单主节点的大型系统来说，一般会用一个独立的复制状态机来管理选主，以及存储某些配置信息，并且主节点发生异常时这些信息也不会丢失。复制状态机的应用包括 <code>Chubby</code> 和 <code>ZooKeeper</code>。</p>
<p><img src="/images/raft-1.png" alt="alt"></p>
<p>复制状态机一般通过复制日志来实现。在上图中，每台机器保存的日志中包含了一系列命令，这些命令会被状态机按顺序执行。每份日志中以相同的顺序保存着相同的命令，所以每台状态机能以相同的顺序执行这些命令。因为状态机是确定性的，所以最终所有状态机的状态和输出的结果都是相同的。</p>
<p>共识算法的任务就是要保证这些日志数据的一致性。服务器上的共识模块收到客户端的命令后会将其添加到本地日志中。然后它会和其他服务器上的共识模块通信来保证即使在某些服务器异常的情况下，各服务器也会以相同的顺序记录下所有的请求日志。当客户端命令被正确复制后，每台服务器上的状态机会以日志中的顺序执行这些命令，然后将结果返回给客户端。从整体上来说，所有的服务器对外组成了一个独立，高可用的状态机。</p>
<p>针对实际系统的共识算法来说一般有以下几个特性：</p>
<ul>
<li>保证在所有非拜占庭情况下的正确性（永远不会返回一个错误的结果给客户端），这里的场景包括网络延迟，网络分区，网络包丢失、重复和重排序等等。</li>
<li>只要系统中过半数的服务器依然存活并且相互间以及和客户端间可以通信，整个系统对外来说依然是可用的。因此，一个由五台服务器组成的集群可以容忍任意两台服务器的异常。如果某台服务器停止响应则会被认为是异常，它可能之后会自动恢复并加载持久化存储上的状态然后重新加入到集群中。</li>
<li>不依赖时间来保证日志的一致性：错误的时钟和极大的消息延迟在最坏的情况下会造成可用性问题。</li>
<li>在一般情况下，当集群中过半数的服务器在一轮 <code>RPC</code> 请求中成功响应时，这次的客户端请求就被视为完成，剩下少数响应缓慢的服务器不会影响整个系统的性能。</li>
</ul>
<h2 id="Paxos-的问题"><a href="#Paxos-的问题" class="headerlink" title="Paxos 的问题"></a>Paxos 的问题</h2><p><code>Leslie Lamport</code> 的 <code>Paxos</code> 协议几乎成为了共识算法的代名词：它是在课堂上被教授的最多的协议，以及大部分的共识算法的实现都以此为出发点。<code>Paxos</code> 首先定义了一个协议能够对单一决策达成共识，例如复制某一条日志。这个被称之为 <code>single-decree Paxos</code>。然后，<code>Paxos</code> 能组合多个单一决策以达成对一系列决策的共识（<code>multi-Paxos</code>），例如一整个日志文件。<code>Paxos</code> 保证了安全性和存活性，同时支持集群中的节点变更。它的正确性已经被证明而且在常规使用中已足够高效。</p>
<p>不幸的是，<code>Paxos</code> 有两个重大的缺点。第一个缺点是 <code>Paxos</code> 非常难以理解。它的完整的解释是众所周知的晦涩难懂，只有少数人拼尽全力后才能正确理解。因此，人们尝试用通俗易懂的方式来解释 <code>Paxos</code>。不过这些尝试主要针对的是 <code>single-decree Paxos</code>，虽然也足够具有挑战性。根据 <code>NSDI 2012</code> 与会者的一项非正式调查显示，即使在经验老到的研究者中，也只有少数人能掌握 <code>Paxos</code>。<code>Raft</code> 的作者自身也受困于 <code>Paxos</code>，直到它们读了某些简化的 <code>Paxos</code> 的解释和实现了 <code>Raft</code> 之后才理解了 <code>Paxos</code> 的完整的协议，而这花了几乎一年的时间。</p>
<p><code>Raft</code> 的作者认为 <code>Paxos</code> 的晦涩难懂来源于 <code>Paxos</code> 选择 <code>single-decree</code> 作为其协议的基础。<code>single-decree Paxos</code> 难以理解：它被分为两阶段但是又缺少简单直白的解释来单独理解每个阶段。鉴于此，人们也很难理解为什么整个 <code>single-decree</code> 协议是正确的。而由 <code>single-decree Paxos</code> 组合而来的 <code>multi-Paxos</code> 则更添加了额外的复杂性。<code>Raft</code> 的作者相信多决策共识的问题能够以更直白的方式拆解。</p>
<p><code>Paxos</code> 的第二个问题是没有为构建实际的系统提供坚实的基础。其中一个原因是还没有一个被广泛认可的 <code>multi-Paxos</code> 算法。<code>Lamport</code> 的论文中主要描述的是 <code>single-decree Paxos</code>；他只是概括性的描述了 <code>multi-Paxos</code>，但是缺少很多细节。虽然人们有很多尝试来补充和优化 <code>Paxos</code>，但是它们相互之间以及和 <code>Lamport</code> 的描述都各有不同。虽然有些系统如 <code>Chubby</code> 实现了类似 <code>Paxos</code> 的算法，但是实现的算法细节并没有公开。</p>
<p>另外，<code>Paxos</code> 的架构对于实际系统的构建来说不够友好，这也是一个将 <code>single-decree</code> 分为两阶段后造成的后果。例如，没有必要独立的选择一些日志然后再将其合并为有序的日志，这只会增加复杂性。相比而言，设计一个以日志为中心的系统并且只允许按照指定的顺序来追加写日志会更简单和高效。另一方面，<code>Paxos</code> 的核心实现采用了对等点对点的方式（虽然它最终建议一种弱主节点的方式来作为一种性能优化的手段）。这种设计适合于单一决策共识的场景，不过很少有实际的系统采用这种方式。当需要对一系列决策达成共识时，首先选择一个领导者然后由领导者协调做决策会更简单和高效。</p>
<p>因此，实际的系统很少采用 <code>Paxos</code> 的方式。每一种实现都基于 <code>Paxos</code>，然后在实现时遇到了困难，接着就演变出了大不相同的架构。这既费时又容易出错，<code>Paxos</code> 的难以理解又加剧了这个问题。<code>Paxos</code> 的描述可能非常适合证明其正确性，不过实际系统的实现却大相径庭，<code>Paxos</code> 的证明也没有什么太大的帮助。来自 <code>Chubby</code> 的实现者的评论一针见血的指出了这个问题：</p>
<blockquote>
<p><code>Paxos</code> 的描述和现实世界的系统的实现间存在巨大的鸿沟…最终的系统将会构建在一个没有被证明的协议上。</p>
</blockquote>
<p>鉴于以上的问题，<code>Paxos</code> 即没有为系统构建也没有为教学提供一个坚实的基础。考虑到共识算法在构建大型软件系统中的重要性，<code>Raft</code> 的作者决定尝试能否设计成比 <code>Paxos</code> 更优秀的共识算法，<code>Raft</code> 因此应运而生。</p>
<h2 id="为可理解性设计"><a href="#为可理解性设计" class="headerlink" title="为可理解性设计"></a>为可理解性设计</h2><p>作者在设计 <code>Raft</code> 时有几个目标：必须为系统构建提供完整坚实的基础，从而能大大减少开发人员的设计工作；必须在任何场景下保证安全性以及在特定操作场景下保证可用性；大多数的操作必须高效。不过最重要也是最困难的是可理解性。它必须能让大部分的受众易于理解。另外，这个算法必须能让人形成直观的认识，系统构建者就可以在实现时进行必要的扩展。</p>
<p>在设计 <code>Raft</code> 时有很多方面需要在多种方案中做选择。在选择哪种方案时基于的是可理解性：解释每种方案难度有多大（例如，其内部状态有多复杂），读者完全理解这个方案需要付出多大的努力？</p>
<p>虽然做这样的分析有很大的主观性，作者使用了两方面的手段来解决这个问题。第一个手段是众所周知的问题分解：只要有可能，作者都会先将一个问题分解为一系列独立可解决，可解释，可相对的单独理解的子问题。例如，在 <code>Raft</code> 中选主，日志复制，安全性，集群节点变更被分解为独立的模块。</p>
<p>第二个手段是通过减少系统状态的数量来简化系统状态，这样就使得系统更具一致性并尽可能的消除非确定性。特别的，系统中的日志不允许有空洞，<code>Raft</code> 也限制了各节点间日志不一致的场景。虽然在大多数情况下会尽可能的消除非确定性，不过在某些场景下非确定性却更有助于理解。特别是随机化会带来非确定性，不过它能以相似的手段来处理所有可能的场景来降低系统状态的复杂性。<code>Raft</code> 使用随机化来简化了选主算法。</p>
<h2 id="Raft-共识算法"><a href="#Raft-共识算法" class="headerlink" title="Raft 共识算法"></a>Raft 共识算法</h2><p><code>Raft</code> 是一种管理第二节中所描述的复制日志的算法，下面描述了该算法的关键特性：</p>
<ul>
<li><code>Election Safety</code>：在任一任期内最多只有一个节点被选为主节点。</li>
<li><code>Leader Append-Only</code>：主节点永远不会覆盖或者删除某些日志项，它只会追加写新的日志项。</li>
<li><code>Log Matching</code>：如果两份日志在同一索引处的日志项对应相同的任期，那么双方在这个索引之前的日志项都相同。</li>
<li><code>Leader Completeness</code>：如果某个日志项在某个任期内被提交了，那么这条日志会出现在后续所有新任期下的主节点的日志中。</li>
<li><code>State Machine Safety</code>：如果某台服务器将某个索引位置的日志应用到了自身的状态机中，那么不会有任何一台服务器在相同索引位置应用了一条不同的日志到状态机中。</li>
</ul>
<p><code>Raft</code> 在实现共识时会先进行选主，然后完全交由主节点来管理日志的复制。主节点接收来自客户端的日志请求，然后将日志复制到其他从节点上，最后在合适的时机告诉各从节点将日志中的内容应用到自身的状态机中。使用主节点的方式简化了复制日志的管理。例如，主节点可自行决定在哪里插入新的日志而不用和其他服务器交互，其他数据流也是类似的只会从主节点流向从节点。当主节点异常或无法和其他从节点连通时，系统会选举一个新的主节点。</p>
<p>通过主节点的方式，<code>Raft</code> 将共识问题分解成了三个相对独立的子问题：</p>
<ul>
<li>选主：当主节点异常时，系统必须选举一个新的主节点。</li>
<li>日志复制：主节点必须从客户端接收日志请求，然后将其复制到其他从节点上，并强制要求其他从节点的日志以主节点的为准。</li>
<li>安全性：如果任意一台服务器已经将某条日志应用到了自身的状态机中，那么其他任何服务器都不能在这条日志对应的索引上应用不同的命令到状态机中。</li>
</ul>
<h3 id="Raft-基础"><a href="#Raft-基础" class="headerlink" title="Raft 基础"></a>Raft 基础</h3><p>一个 <code>Raft</code> 集群包含若干台服务器，5台是一个常见的配置，这允许系统最多能容忍两台服务器的异常。在任一时刻，每台服务器只会处于其中一个状态：主节点（<code>leader</code>），从节点（<code>follower</code>），或者候选节点（<code>candidate</code>）。在正常操作下，系统中只有一个主节点，剩下的都是从节点。从节点是被动的：它们不会主动发起任何请求，只是简单的响应来自主节点和候选节点的请求。主节点会处理所有来自客户端的请求（如果客户端将请求发给了一个从节点，从节点会将其转发给主节点）。候选节点这个状态会在选举新的主节点时用到。下图展示了各节点状态间的转换：</p>
<p><img src="/images/raft-2.png" alt="alt"></p>
<p>如下图所示，<code>Raft</code> 将时间切分为任意长度的任期（<code>term</code>）。任期会以连续的整数来标记。每个任期以选举作为开始，一个或多个候选节点会尝试成为主节点。如果某个候选节点赢得了选举，那么在这个任期剩下的时间里它将作为主节点。在某些情况下，多个候选节点可能会分票，造成没有一个候选节点赢得半数的选票。在这种情况下，当前任期会以没有主节点的状态结束，接着系统会马上对新的任期发起新的一轮选举。<code>Raft</code> 保证在任一任期内至多只有一个主节点。</p>
<p><img src="/images/raft-3.png" alt="alt"></p>
<p>不同的服务器可能会观测到不同次数的任期转变，在某些情况下一台服务器可能观测不到某次选举的发生或者感知不到整个任期。任期在 <code>Raft</code> 中扮演了逻辑时钟的角色，它能允许服务器侦测过期的信息例如过期的主节点。每台服务器都会保存一个当前任期（<code>current term</code>）的数字，这个数字会随时间递增。在服务器间通信时双方会附加上当前任期，如果某台服务器的当前任期小于另外一台服务器，那么这个服务器会将当前任期更新为另一台服务器的值。如果某个候选节点或者主节点发现自己的当前任期过期了，那么它会马上转为从节点状态。如果某台服务器收到的请求中的任期过期了，那么它会拒绝这个请求。</p>
<p><code>Raft</code> 服务器间通过 <code>RPC</code> 进行通信，基础的共识算法只需要两种 <code>RPC</code>。<code>RequestVote</code> 用于候选节点在选主期间获取选票，<code>AppendEntries</code> 用于主节点向各从节点复制日志以及充当心跳的作用。如果某个 <code>RPC</code> 请求在一段时间内没有响应，那么服务器会重新发起请求，同时服务器也会并行发送 <code>RPC</code> 请求来达到最佳性能。</p>
<h3 id="选主"><a href="#选主" class="headerlink" title="选主"></a>选主</h3><p><code>Raft</code> 通过心跳机制来触发选主。当服务器启动时，它们的初始状态是从节点。只要服务器能收到来自候选节点或者主节点的有效请求，就会一直出于从节点状态。主节点会定期的向所有从节点发送心跳（不带任何日志的 <code>AppendEntries</code> 请求）来维持自己主节点的地位。如果在某段时间内某台从节点没有收到心跳，那么它就会认为此时没有存活的主节点，则会发起选主来选择一个新的主节点，这个等待时间就叫做 <code>election timeout</code>。</p>
<p>开始新的一轮选主时，从节点会先将当前任期递增然后转换为候选节点。接着，它会给自己投一票然后并行发起 <code>RequestVote</code> 请求给其他从节点来获取选票。一个候选节点会保持当前的状态直到下面三种情况之一发生：</p>
<ol>
<li>当前候选节点赢得了选举</li>
<li>有另外一个候选节点赢得了选举</li>
<li>一段时间之后没有一个候选节点赢得选举</li>
</ol>
<p>当候选节点获得了集群中针对某个任期的过半数节点的选票时就赢得了选举。在某个任期内，每台服务器只会最多给一个候选节点投票，以先来后到为准。过半数的选票保证了在某个任期内最多只会有一个候选节点被选举为主节点（<code>Election Safety Property</code>）。当某个候选节点赢得选举时，它就成为了主节点。然后它就开始向其他服务器发送心跳来维持主节点的状态并阻止其他节点继续发起选主。</p>
<p>候选节点在等待选票时有可能收到其他自认为是主节点的 <code>AppendEntries</code> 的请求。如果请求中的任期号不小于当前候选节点记录的任期号，则该候选节点会将此主节点作为主节点并转为从节点状态。如果请求中的任期号小于当前候选节点记录的任期号，则该候选节点会拒绝此请求并继续处于候选节点状态。</p>
<p>第三种情况是没有一个候选节点赢得了选举：当很多从节点在同一时间转变为候选节点时，会分散选票，最终造成没有一个候选节点赢得过半数的选票。当发生这种情况时，候选节点会将此次选主作超时处理，然后再次将当前任期自增，重新发起新的任期的选主，并向其他从节点继续发起一轮 <code>RequestVote</code> 请求。不过，如果缺少额外机制，分票可能会一直持续下去。</p>
<p><code>Raft</code> 通过随机的 <code>election timeout</code> 来确保分票极少会发生并且在发生时能快速解决。为了避免分票，首先 <code>election timeout</code> 的值会在一个固定区间内随机选择（例如150-300ms）。这就分散了各从节点的选主启动时机，使得在大多数的情况下只有一个从节点会进入选主状态；在其他从节点进入选主状态之前，这个节点就已经赢得了选举并向其他从节点发送了心跳，这就大大扼杀了分票的可能性。同样的机制也被用来解决当分票确实发生的场景，每个候选节点在启动新的选主时会重新设置一个随机的 <code>election timeout</code>，然后在这段期间内等待其他从节点的选票，或者新的主节点的心跳，假设第一轮选主发生了分票，那么由于随机 <code>election timeout</code> 的存在，不同的候选节点进入第二轮选主的时机也不会相同，这就降低了第二轮选主继续发生分票的可能性。</p>
<p>选主是一个很好的例子展示了可理解性这个设计目标如何来指导在不同设计方案中做出选择。在最初的方案中作者打算使用一个排序系统：每一个候选节点被分配了一个唯一的权重，用来在多个候选节点中选择最终的主节点。如果某个候选节点发现其他候选节点的权重比自己高，那么这个候选节点就会退回到从节点状态，这就使得有着更高权重的候选节点能更容易的赢得下一轮的选举。不过这种实现可能会造成难以察觉的可用性问题（在不采用随机 <code>election timeout</code> 的情况下，当某个高权重的候选节点异常时，由于低权重的候选节点已经退回到了从节点状态，它需要再等待一个 <code>election timeout</code> 周期才能再次转变为候选节点，而在正常的情况下本身各节点间的信息交换速度较快，其他候选节点可能都已经退回到了从节点状态，此时高权重的候选节点异常就会造成系统没有候选节点，而距离各从节点进入选主状态又还有较长时间，从而造成系统在这段期间的不可用）。虽然作者对该算法进行了多次调整，但是每次调整后都会出现新的边界问题。最终作者认为随机化的 <code>election timeout</code> 更胜一筹和易于理解。</p>
<h3 id="日志复制"><a href="#日志复制" class="headerlink" title="日志复制"></a>日志复制</h3><p>当某个候选节点被选为主节点后，它就开始处理客户端请求。每个客户端请求中包含了复制状态机需要执行的命令。主节点将这个命令以日志的形式追加到自己的日志文件中，然后并行的给所有从节点发送 <code>AppendEntries</code> 请求来复制日志。当日志在各从节点上被安全的复制后，主节点就将日志对应的命令应用到自身的状态机中，并将结果返回给客户端。如果某个从节点异常或者运行缓慢，或者网络包丢失，主节点会一直重发 <code>AppendEntries</code> 请求（即使主节点已经将结果返回给了客户端），直到所有从节点保存了所有的日志。</p>
<p>日志内容的组织如下图所示，每一条日志包含了状态机需要执行的命令以及主节点收到该请求时对应的任期。日志中的任期信息用来检测日志间的不一致性以及保证前面所提到的 <code>Raft</code> 的几个关键特性。每条日志同时有一个索引信息来标记这条日志在日志文件中的位置。</p>
<p><img src="/images/raft-4.png" alt="alt"></p>
<p>在上图中，方块中形如 <code>x &lt;- 3</code> 表示客户端的命令，命令上方的数字表示任期。</p>
<p>主节点会决定什么时候能安全的将某条日志对应的命令应用到状态机中，该条日志就被称为已提交（<code>commited</code>）。<code>Raft</code> 保证已提交的日志是持久化了的，并且最终会被所有可用的状态机执行。当主节点将某条日志复制到集群中过半数的机器上时（例如上图中索引位置为7的日志），这条日志就会被标记为已提交。同时，该条日志之前的日志也都会被提交，包括从其他主节点复制而来的日志。主节点维护了已提交日志中的最大的索引值，并将其附加到后续的 <code>AppendEntries</code> 请求中（包括心跳），所以最终所有的机器都能知道当前的最远日志提交位置。当某个从节点发现某条日志已经提交了，它就会将该条日志对应的命令应用到自身的状态机中（以日志中出现的顺序执行命令）。</p>
<p><code>Raft</code> 设计的日志机制能保证在不同机器间高层次下的一致性。这不仅简化了系统的行为和使得系统更有可预测性，同时也是保证安全性的重要组成部分。<code>Raft</code> 保证了下面两个性质（同时也组成了 <code>Log Matching Property</code>）：</p>
<ul>
<li>如果两个日志文件中相同索引位置的日志保存着相同的任期，则它们保存着相同的客户端命令</li>
<li>如果两个日志文件中相同索引位置的日志保存着相同的任期，则在这个索引位置之前的日志都相同</li>
</ul>
<p>第一个性质保证是因为主节点在某个任期内最多只会在某个索引位置创建一条日志，而日志的位置创建后就不会改变。第二个性质保证则是通过 <code>AppendEntries</code> 请求中的一致性检查来实现。当主节点发送 <code>AppendEntries</code> 请求时，主节点会附带上本次需要复制的日志的前一条日志的索引和对应任期。如果从节点没有在自己的日志中找到这个指定索引和任期的日志，那么它会拒绝新日志的写入请求。日志的一致性检查类似于数学归纳法：初始情况日志为空，所以满足 <code>Log Matching Property</code>，假设从索引位置1到 <code>i</code> 的日志都满足 <code>Log Matching Property</code>，那么当主节点在成功提交了新的日志即索引位置 <code>i + 1</code> 的日志后，则在 <code>[i + 1, i + 1]</code> 这个范围满足了 <code>Log Matching Property</code>，从而得出从索引位置1到 <code>i + 1</code> 的日志都满足了 <code>Log Matching Property</code>。因此，只要 <code>AppendEntries</code> 的请求成功返回，那么主节点就知道从节点的日志和自己的相同。</p>
<p>在正常的操作下，主节点的日志和从节点的日志始终能保持一致，所以 <code>AppendEntries</code> 的一致性检查从来不会失败。不过，当主节点异常时则会造成日志不一致（主节点还没有来得及复制所有的日志）。同样的，从节点的异常也会造成日志不一致（从节点没有收到所有的日志）。下图展示了从节点的日志可能和新的主节点不同的情况。一个从节点有可能缺少某些主节点拥有的日志，或者拥有一些主节点没有的日志，或者两者都有。这种日志的缺少或多余的情况可能会横跨几个任期。</p>
<p><img src="/images/raft-5.png" alt="alt"></p>
<p>在上图中，方块中的数字表示任期。当前已提交的日志的索引位置是1-9，一共有四台机器在1-9位置的日志相同，符合过半数的原则。<code>a</code> 和 <code>b</code> 相比于主节点来说缺少日志，<code>c</code> 和 <code>d</code> 相比于主节点来说有多余的日志，<code>e</code> 和 <code>f</code> 两种情况都有。对于 <code>f</code> 来说，它在任期2中被选为主节点，然后开始接受客户端请求并写入本地日志，但是还没有成功复制到其他从节点上就异常了，恢复后又被选举为任期3的主节点，又重复了类似的操作。</p>
<p>在 <code>Raft</code> 中，主节点通过强制从节点复制自己的日志来保证日志的一致性。也就是对于从节点来说，和主节点日志不一致的地方会被主节点的日志覆盖。</p>
<p>为了让从节点的日志和主节点保持一致，主节点必须知道到哪个索引位置为止主从节点间的日志是一致的，然后删除从节点在这个索引位置之后的日志，并替换为主节点中的日志。主节点为每个从节点维护了一个 <code>nextIndex</code> 变量，用来表示下一条由主节点发送给从节点的日志索引位置。当某个节点成为主节点时，它先将 <code>nextIndex</code> 初始化为自身日志文件中最后一条日志的索引位置加1。如果从节点的日志和主节点不一致，那么在下一次的 <code>AppendEntries</code> 请求中会返回失败。当主节点收到失败响应后，会将 <code>nextIndex</code> 减1并重新发送 <code>AppendEntries</code> 请求（前面提到，当主节点发送 <code>AppendEntries</code> 请求时，主节点会附带上本次需要复制的日志的前一条日志的索引和对应任期，从节点会根据这两个值来决定是否接受写入，当 <code>nextIndex</code> 减1时，在新的 <code>AppendEntries</code> 请求中这两个值也需要相应的往前移），最终 <code>nextIndex</code> 会等于某个值使得主从节点在 <code>nextIndex</code> 之前的日志都相同。此时 <code>AppendEntries</code> 会返回成功，从节点会将 <code>nextIndex</code> 及其之后的日志都替换为主节点的日志，至此，主从节点的日志就恢复了一致，并一直保持到当前任期结束。</p>
<blockquote>
<p>如果需要的话，可以对上述的交互协议进行优化来减少 <code>AppendEntries</code> 的次数，尤其是从节点的日志落后主节点太多的时候。例如，当从节点需要拒绝 <code>AppendEntries</code> 的请求时，可以在响应结果中带上不一致的日志的任期，以及该任期下的第一条日志的索引位置。根据这个信息，主节点就可以大幅减少 <code>nextIndex</code> 的值从而直接跳过该任期下不一致的日志，而不是一次 <code>RPC</code> 请求识别一条日志。不过作者怀疑这个优化在实际中是否有必要，因为异常并不会频繁发生所以不太可能会有那么多的不一致。</p>
</blockquote>
<p>针对上面的优化手段作者并没有描述过多的细节，在 <a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/notes/l-raft2.txt">6.824 2022 Lecture 7: Raft (2)</a> 中 <code>Robert Morris</code> 提出了自己的猜想。在下图中，<code>S1</code> 是主节点，<code>S2</code> 到 <code>S7</code> 是从节点，其中 <code>S2</code> 到 <code>S4</code> 节点的日志和主节点不一致，日志索引1到3已提交，现在主节点要在索引位置4追加一条新日志，分别来看 <code>S2</code> 到 <code>S4</code> 如何处理。</p>
<p><img src="/images/raft-6.png" alt="alt"></p>
<p><code>Robert Morris</code> 认为当从节点拒绝 <code>AppendEntries</code> 请求时，需要返回三个信息给主节点：</p>
<ul>
<li><code>XTerm</code>：冲突的日志的任期（如果有的话）</li>
<li><code>XIndex</code>：冲突的日志的任期下的第一条日志的索引位置（如果有的话）</li>
<li><code>XLen</code>：整个日志的长度</li>
</ul>
<p>记 <code>prevLogIndex</code> 表示主节点发送给从节点的上一条日志的索引位置，<code>prevLogTerm</code> 表示上一条日志所属的任期。则在主节点的第一轮 <code>AppendEntries</code> 中 <code>prevLogIndex = 3</code>，<code>prevLogTerm = 6</code>，对于 <code>S2</code> 到 <code>S4</code> 初始化的 <code>nextIndex</code> 为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;S2&quot;: 4,</span><br><span class="line">    &quot;S3&quot;: 4,</span><br><span class="line">    &quot;S4&quot;: 4</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对于 <code>S2</code>，首先检查自己日志中索引位置为 <code>prevLogIndex = 3</code> 的日志，发现其任期为5，和 <code>prevLogTerm = 6</code> 不匹配，从而拒绝请求，并返回 <code>XTerm = 5</code>，<code>XIndex = 2</code>，<code>XLen = 3</code>。主节点收到结果后，发现 <code>S2</code> 中 <code>prevLogIndex</code> 指向的任期和自己不同，这里比主节点小所以主节点往前遍历日志，发现没有任期5的日志，说明 <code>S2</code> 中整个任期5的日志都可以跳过，因此主节点将 <code>S2</code> 的 <code>nextIndex</code> 修改为 <code>XIndex</code>，即 <code>nextIndex = 2</code>。在第二轮的 <code>AppendEntries</code> 请求中，<code>prevLogIndex = 1</code>，<code>prevLogTerm = 4</code>，同时主节点也会将索引位置2到3的日志连同最新的日志一起随请求发送。<code>S2</code> 再次收到请求后，发现这次 <code>prevLogIndex/prevLogTerm</code> 标记的日志和自己匹配，因此返回成功，并将主节点的日志覆盖到索引位置2到4中。</p>
<p>对于 <code>S3</code>，首先检查自己日志中索引位置为 <code>prevLogIndex = 3</code> 的日志，发现其任期为4，和 <code>prevLogTerm = 6</code> 不匹配，从而拒绝请求，并返回 <code>XTerm = 4</code>，<code>XIndex = 1</code>，<code>XLen = 3</code>。主节点收到结果后，发现 <code>S3</code> 中 <code>prevLogIndex</code> 指向的任期和自己不同，这里比主节点小所以主节点往前遍历日志，发现了任期4的日志，说明 <code>S3</code> 中任期4的日志比主节点多，因此主节点将 <code>S3</code> 的 <code>nextIndex</code> 修改为2，即主节点中任期4的最后一个日志的索引位置加1（这里 <code>Robert Morris</code> 的原话是 <code>leader&#39;s last entry for XTerm</code>，不过为了能统一处理下一次请求中的 <code>prevLogIndex</code> 和 <code>prevLogTerm</code>，这里加了1）。在第二轮的 <code>AppendEntries</code> 请求中，<code>prevLogIndex = 1</code>，<code>prevLogTerm = 4</code>，同时主节点也会将索引位置2到3的日志连同最新的日志一起随请求发送。<code>S3</code> 再次收到请求后，发现这次 <code>prevLogIndex/prevLogTerm</code> 标记的日志和自己匹配，因此返回成功，并将主节点的日志覆盖到索引位置2到4中。</p>
<p>对于 <code>S4</code>，首先检查自己日志中索引位置为 <code>prevLogIndex = 3</code> 的日志，发现不存在，所以拒绝请求，并返回 <code>XTerm = null</code>，<code>XIndex = null</code>，<code>XLen = 1</code>。主节点收到结果后，发现 <code>XTerm</code> 和 <code>XIndex</code> 都为空，说明 <code>S4</code> 的日志比自己短，因此主节点将 <code>S4</code> 的 <code>nextIndex</code> 修改为2，即 <code>S4</code> 的日志长度加1（这里 <code>Robert Morris</code> 的原话是 <code>XLen</code>，不过为了能统一处理下一次请求中的 <code>prevLogIndex</code> 和 <code>prevLogTerm</code>，这里加了1）。在第二轮的 <code>AppendEntries</code> 请求中，<code>prevLogIndex = 1</code>，<code>prevLogTerm = 4</code>，同时主节点也会将索引位置2到3的日志连同最新的日志一起随请求发送。<code>S4</code> 再次收到请求后，发现这次 <code>prevLogIndex/prevLogTerm</code> 标记的日志和自己匹配，因此返回成功，并将主节点的日志覆盖到索引位置2到4中，如果不匹配，则又会流转到上面两种情况中。</p>
<p>在这种机制下，主节点不需要采用其他特殊的操作就能保持从节点日志的一致性。通过正常的 <code>AppendEntries</code> 请求，结合其一致性检查的功能，从节点的日志就可以自行恢复一致。而对于主节点来说，它永远不会覆盖或者删除自己的日志（<code>Leader Append-Only Property</code>）。</p>
<p>这种日志复制机制展现了第二节中描述的共识算法所需要的特性：只要集群中过半数的机器存活，<code>Raft</code> 就能接收，复制和应用新的日志；在正常情况下，只需要一轮过半数机器的 <code>RPC</code> 请求响应成功就能复制一条新日志；一台运行缓慢的机器并不会影响整体的性能。</p>
<h3 id="安全性"><a href="#安全性" class="headerlink" title="安全性"></a>安全性</h3><p>前面几节描述了 <code>Raft</code> 如何选主以及复制日志。不过，这些机制还不足以保证每个节点能以相同的顺序执行相同的命令。例如，某个从节点可能在某段期间内异常了，在这段期间内某个主节点已经提交了部分日志，此时假设主节点异常，之前异常的节点恢复并被选为主节点，根据前面所描述的 <code>AppendEntries</code> 的工作流程，它就有可能将其他从节点中之前提交的日志覆盖掉。因此，不同的节点可能会执行不同的命令序列。</p>
<p>本节通过描述了什么样的节点才允许被选为主节点来补充完善 <code>Raft</code> 算法。这个选主的限制保证了如果某个节点被选为了主节点，那么它就一定包含之前所有任期内由其他主节点提交的日志（<code>Leader Completeness Property</code>）。根据这个选主限制，我们可以使日志提交的规则更加清晰。</p>
<h4 id="选主限制"><a href="#选主限制" class="headerlink" title="选主限制"></a>选主限制</h4><p>在所有基于主节点的共识算法中，最终主节点必须保存所有已提交的日志。在某些共识算法中，例如 <code>Viewstamped Replication</code>，如果某个节点没有包含所有已提交的日志也能被选为主节点。这些共识算法有额外的机制来识别出缺失的日志，并在选主期间或之后将缺失的日志发送给主节点。然而，这就增加了额外的机制和复杂性。<code>Raft</code> 采用了一种更简便的方案来保证每一个被新选举的主节点一定包含了之前所有主节点提交的日志，而无需额外传输缺失的日志给主节点。这就表明日志始终是单向流动，即从主节点流向从节点，并且主节点永远不会覆盖已经存在的日志。</p>
<p><code>Raft</code> 在选主阶段会避免某个没有完整提交日志的候选节点成为主节点。一个候选节点必须和集群中过半数的节点通信来获取选票，这说明每一条提交的日志都至少存在于其中一台节点上（假设有 <code>S1</code> 到 <code>S5</code> 五个节点，之前 <code>S1</code> 是主节点并将某条日志成功提交到 <code>S1</code>、<code>S2</code>、<code>S3</code>，此时 <code>S1</code> 异常假设 <code>S4</code> 成为新的主节点，并获得了 <code>S3</code>、<code>S4</code>、<code>S5</code> 的选票，由于过半数原则，新的主节点的选票必然和之前的主节点的选票存在重合，也就说明必然至少有一台机器上保存了之前主节点提交的日志）。如果候选节点的日志至少和过半数的节点的日志一样新（一样新的定义见后文描述），那么它将会拥有所有已提交的日志。<code>RequestVote</code> 接口实现了这个限制：发送请求时会带上候选节点的日志信息，如果其他节点的日志比这个候选节点还要新，则会拒绝这个候选节点的选票。</p>
<p><code>Raft</code> 通过比较两个日志文件中的最后一条日志的索引位置和任期来决定哪个日志较新。如果两个日志的任期不同，则更高任期的日志较新；如果两个日志的任期相同，则索引位置大的日志较新。</p>
<h4 id="提交之前任期的日志"><a href="#提交之前任期的日志" class="headerlink" title="提交之前任期的日志"></a>提交之前任期的日志</h4><p>对于主节点来说，如果当前任期下的某条日志被过半数的节点成功复制，那么这条日志就可以被提交。如果日志变为已提交前主节点异常了（已复制的副本未过半数或者还没有来得及执行提交），后续的主节点会尝试继续复制该日志。然而，主节点并不能马上下结论说某条之前任期产生的日志在过半数的节点上保存后就算被安全提交了。</p>
<p><img src="/images/raft-7.png" alt="alt"></p>
<p>上图描述了某条日志被过半数的节点复制后，有可能会被某个新的主节点的日志所覆盖的情况。在 <code>a</code> 中，<code>S1</code> 是主节点，并且部分复制了索引位置2的日志到其他从节点上；在 <code>b</code> 中，<code>S1</code> 发生异常，<code>S5</code> 被选为主节点，在索引位置2的地方追加了一条任期3的日志；在 <code>c</code> 中，<code>S5</code> 发生异常，<code>S1</code> 恢复并再次被选为了主节点，此时任期是4，<code>S1</code> 会继续通过 <code>AppendEntries</code> 的一致性检查将索引位置2的日志继续复制给过半数的节点，但是还未提交，同时又在索引位置3的地方追加了一条任期4的日志；在 <code>d</code> 中，<code>S1</code> 发生异常，<code>S5</code> 成为主节点，则 <code>S5</code> 会将自己的索引位置2的日志复制给其他的从节点；而在 <code>e</code> 中的另一种情况下，如果 <code>S1</code> 在异常前将当前任期下的索引位置3的日志复制到了过半数的节点上，那么索引位置3的日志就可以被提交，因为 <code>S5</code> 节点不可能会成为主节点（它的日志相比于其他过半数的节点来说不够新）也就不会覆盖，而在此时，索引位置3之前的日志也可以被认为是已提交的。</p>
<p>为了避免上面 <code>d</code> 所描述的情况，<code>Raft</code> 永远不会通过计算某条之前任期的日志副本的数量来判断这条日志是否能提交（也就是 <code>c</code> 中的情况，此时 <code>S1</code> 是主节点，任期是4，通过和其他从节点的通信将任期2的日志复制给了过半数的从节点，但是 <code>S1</code> 不会提交任期2的日志，因为一旦提交就有可能出现 <code>d</code> 中的情况）。只有当前主节点当前任期下生成的日志才会根据已复制的副本的数量来判断是否能提交；如果当前任期下的某条日志以这种方式提交了，那么根据 <code>Log Matching Property</code> 这条日志之前的日志也会间接的被提交。虽然在某些场合下，主节点可以知道某条之前任期的日志是被提交的（例如，如果这条日志在每个节点上都有保存），但是 <code>Raft</code> 出于简洁性的考虑采用了更保守的策略。回到上面的例子，在 <code>c</code> 中即使 <code>S1</code> 将索引位置2的日志提交了才发生异常，也依然有可能发生 <code>d</code> 的情况，因为 <code>d</code> 中的 <code>S5</code> 并不知道其他过半数的从节点提交了索引位置2任期2的日志（除非有额外的机制，所以这里说 <code>Raft</code> 采用了保守的策略没有引入其他机制），也就是说提交一个之前任期的日志并不能保证它不会被之后的主节点覆盖，所以这部分之前任期的日志就不能当做已提交或者做了提交也没有用。一直等到 <code>e</code> 中的情况，在当前任期4下，只要 <code>S1</code> 将索引位置3的日志复制到了过半数的节点上，即使此时 <code>S1</code> 异常了，这个日志在未来也能被其他主节点间接提交，因为下一轮的候选节点要么有索引位置3的日志，要么没有索引位置3的日志，而根据日志新旧原则，没有索引位置3的日志的候选节点不可能成为主节点，所以只有那些复制了索引位置3的日志的候选节点才有可能成为新的主节点，因为主节点不会增删日志，所以索引位置2、3的日志必然存在于新的主节点中，一旦新的主节点提交了一个更高任期的日志，索引位置2、3的日志也就被间接提交了。这就保证了在这种方式下索引位置3及其之前的日志不会被覆盖。</p>
<p>因为主节点将之前任期的日志复制到其他从节点时依然保留原始任期信息，所以 <code>Raft</code> 选择在提交日志时增加额外的机制（上述描述的日志提交规则，同时引入了一定的复杂性）来保证安全性。在其他共识算法中，如果一个新的主节点需要复制之前任期的日志，那么就必须以当前任期的名义。<code>Raft</code> 的做法可以很方便的识别每条日志属于哪个任期，因为每条日志的任期不会随时间而改变。另外，相比于其他共识算法，在 <code>Raft</code> 中新的主节点会发送更少的来自之前任期的日志（其他共识算法在提交日志前需要发送冗余的日志来重新编号）。</p>
<h4 id="安全性证明"><a href="#安全性证明" class="headerlink" title="安全性证明"></a>安全性证明</h4><p>以完整的 <code>Raft</code> 算法为基础，现在可以更精确的验证 <code>Leader Completeness Property</code> 的正确性。这里使用反证法来证明，假设 <code>Leader Completeness Property</code> 不正确，继而推导出一个矛盾，从而证明假设不成立。假设在任期 <code>T</code> 内某个主节点 <code>leader_T</code> 提交了一条日志，然后这条日志不会出现在新的任期的主节点中。不妨令满足假设的最小的任期为 <code>U</code>（<code>U &gt; T</code>），对应任期 <code>U</code> 的主节点为 <code>leader_U</code>，则 <code>leader_U</code> 没有 <code>leader_T</code> 提交的日志。</p>
<p><img src="/images/raft-8.png" alt="alt"></p>
<ol>
<li><code>leader_T</code> 提交的日志必然在选主时就不存在于 <code>leader_U</code> 中，因为主节点不会删除和覆盖日志。</li>
<li><code>leader_T</code> 将日志复制给了集群中过半数的节点，而 <code>leader_U</code> 从集群中过半数的节点获得了选票，所以两者必然有重合，因此必然存在一个节点即复制了 <code>leader_T</code> 提交的日志，又投票选举了 <code>leader_U</code> 作为新的主节点，在上图中这个节点就是 <code>S3</code>。这个节点是推导出矛盾的关键。</li>
<li><code>S3</code> 必然是先收到 <code>leader_T</code> 的复制日志请求然后才投票给 <code>leader_U</code>，否则的话如果 <code>S3</code> 先进入新的任期那么它会拒绝来自 <code>leader_T</code> 的 <code>AppendEntries</code> 的请求，因为 <code>S3</code> 的当前任期更大。</li>
<li><code>S3</code> 在给 <code>leader_U</code> 投票前会完成日志的复制，因为我们这里假定的 <code>U</code> 是最小的一个不包含 <code>leader_T</code> 提交的日志的任期，所以根据这个假定在 <code>[T + 1, U - 1]</code> 之间的主节点都是必然包含 <code>leader_T</code> 提交的日志的，而主节点不会删除或覆盖日志，并且只在不一致的时候删除从节点的日志，所以在任期 <code>U</code> 内，<code>S3</code> 依然是保留 <code>leader_T</code> 提交的日志的。</li>
<li><code>S3</code> 投票给了 <code>leader_U</code>，所以根据选主规则 <code>leader_U</code> 的日志至少是和 <code>S3</code> 一样新。这就导致了两个矛盾。</li>
<li>首先，如果 <code>S3</code> 和 <code>leader_U</code> 的最后一条日志的任期相同，那么 <code>leader_U</code> 的日志索引就必然大于等于 <code>S3</code>，则 <code>leader_U</code> 必然就包含 <code>S3</code> 的每一条日志。假设双方最后一条日志的任期是 <code>X</code>，则 <code>T &lt;= X &lt;= U - 1</code>，在这个范围内的主节点都是有 <code>leader_T</code> 所提交的日志的，根据 <code>AppendEntries</code> 请求的日志一致性校验，只要 <code>leader_U</code> 在这期间收到了主节点的请求，那么主节点就会补齐 <code>leader_U</code> 的日志（如果不一致的话），所以 <code>leader_U</code> 必然就包含 <code>S3</code> 的每一条日志。因此这就产生了一个矛盾，因为根据开头的假设 <code>leader_U</code> 是不应该有 <code>leader_T</code> 提交的日志的。</li>
<li>所以，要想让 <code>leader_U</code> 的日志比 <code>S3</code> 新，那就只能是 <code>leader_U</code> 的最后一条日志的任期大于 <code>S3</code> 的最后一条日志的任期。而这个任期也必然比 <code>T</code> 大，因为 <code>S3</code> 的最后一条日志的任期至少是 <code>T</code>。记这个任期的主节点为 <code>leader_P</code>，根据 <code>Log Matching Property</code>，<code>leader_U</code> 中到最后一条日志前的日志应该和 <code>leader_P</code> 相同，又因为任期 <code>U</code> 之前的主节点都包含 <code>leader_T</code> 所提交的日志，所以 <code>leader_U</code> 也应该包含 <code>leader_T</code> 所提交的日志，所以又产生一个矛盾。</li>
<li>至此完成了所有矛盾的证明。因此，所有任期比 <code>T</code> 大的主节点都必然包含 <code>leader_T</code> 在任期 <code>T</code> 内所提交的日志。</li>
<li>而 <code>Log Matching Property</code> 也保证了后续的主节点也包含了间接提交的日志。</li>
</ol>
<blockquote>
<p>根据 <code>Leader Completeness Property</code>，我们可以证明 <code>State Machine Safety Property</code>，即如果某台服务器将某个索引位置的日志应用到了自身的状态机中，那么不会有任何一台服务器在相同索引位置应用了一条不同的日志到状态机中。如果某台服务器要将某条日志应用到状态机中，那么这台服务器在这条日志之前的日志都是和主节点相同的，而且是已提交的。现在来考虑在某个最小的任期内，某一台服务器要应用某一条日志，根据 <code>Log Completeness Property</code> 的保证，后续更高任期的主节点都会存储这条日志，所以后续节点在后续任期中应用相同索引位置的日志时也必然是应用了相同的日志内容。因此 <code>State Machine Safety Property</code> 是正确的。</p>
<p>最后，<code>Raft</code> 要求各节点以日志的索引顺序来应用日志。结合 <code>State Machine Safety Property</code>，可以得出所有的节点会以相同的顺序应用相同的日志到自身的状态机中。</p>
</blockquote>
<h3 id="主节点和候选节点异常"><a href="#主节点和候选节点异常" class="headerlink" title="主节点和候选节点异常"></a>主节点和候选节点异常</h3><p>截止目前我们讨论的都是主节点异常。从节点和候选节点异常相比于主节点异常来说更容易处理，而且能以相同的方式处理。当从节点或候选节点异常时，则其他节点发送的 <code>RequestVote</code> 和 <code>AppendEntries</code> 请求都会失败。<code>Raft</code> 通过无限重试来处理这个问题；如果异常的机器重启了，那么这些 <code>RPC</code> 请求就会成功。如果某台机器完成了某个请求但在响应前异常了，那么它会在重启后收到相同的请求。<code>Raft</code> 的 <code>RPC</code> 请求都是幂等的，所以这种情况不会造成影响。例如，某个从节点收到了 <code>AppendEntries</code> 请求然后发现请求中的日志已经在本地日志中了，那么这个节点就会忽略这个请求。</p>
<h3 id="时间和可用性"><a href="#时间和可用性" class="headerlink" title="时间和可用性"></a>时间和可用性</h3><p><code>Raft</code> 的其中一个要求是安全性的保证不依赖于时间：系统不能因为某些事件发生的快了些或慢了些而产生不正确的结果。然而，可用性（系统可以及时的响应客户端）不可避免的要依赖时间。例如，如果服务器间的信息交换需要的时间大于服务器异常的间隔时间（例如信息交换需要5秒，而每隔2秒服务器就异常了），则候选节点没有足够的时间来赢得选举；而缺少主节点，系统也无法继续运行。</p>
<p>选主是 <code>Raft</code> 中对时间要求最关键的方面。只要遵循如下的时间要求那么 <code>Raft</code> 就能维持选主的正常运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">broadcastTime &lt;&lt; electionTimeout &lt;&lt; MTBF</span><br></pre></td></tr></table></figure>

<p>其中 <code>broadcastTime</code> 是服务器并行的给集群中的其他服务器发送 <code>RPC</code> 请求并收到响应的平均时间；<code>electionTimeout</code> 是前面描述的选主等待时间，如果在这个时间段内某个节点没有收到来自主节点的心跳，那么它就会启动选主流程；<code>MTBF</code> 则是单台服务器各次异常间的平均间隔时间。<code>broadcastTime</code> 应该比 <code>electionTimeout</code> 小一个量级，这样主节点才能来得及给从节点发送心跳来维持主节点的地位；再结合随机化的 <code>electionTimeout</code>，这种不固定的时间也避免了选主的分票。<code>electionTimeout</code> 应该比 <code>MTBF</code> 小几个量级从而使得系统能平稳运行。当主节点异常时，系统大概会在 <code>electionTimeout</code> 的时间段内不可用，如果 <code>MTBF</code> 足够大就可以避免这个现象频繁发生。</p>
<p><code>broadcastTime</code> 和 <code>MTBF</code> 由底层系统决定，而 <code>electionTimeout</code> 则是需要由设计者决定。<code>Raft</code> 的 <code>RPC</code> 请求一般会要求接受者将数据持久化到可靠存储上，根据存储技术的不同 <code>broadcastTime</code> 的范围会在0.5毫秒到20毫秒之间。因此，<code>electionTimeout</code> 的设定范围一般是10毫秒到500毫秒之间。服务器的 <code>MTBF</code> 时间一般是几个月或更久，已经足够满足 <code>Raft</code> 对时间的要求。</p>
<h2 id="集群节点变更"><a href="#集群节点变更" class="headerlink" title="集群节点变更"></a>集群节点变更</h2><p>目前为止我们所讨论的都是基于集群配置（参与共识算法的服务器）不变的基础上。而实际上，集群的配置不是一成不变的，有时候就需要修改集群的配置，例如替换掉异常的服务器或者调整复制的级别。虽然操作时可以先下线集群中的全部机器，然后更新配置文件，最后再重启集群，不过在操作期间整个系统都是不可用的。另外，如果其中涉及了人工操作，那么就会有人为错误的风险。为了避免这些问题，<code>Raft</code> 的作者决定将集群配置设计为自动化并整合到 <code>Raft</code> 共识算法中。</p>
<p>为了使集群配置变更是安全的，需要保证在变更期间不会发生在某个任期内有两个主节点的情况。不幸的是，不管怎么样让服务器直接从旧的配置替换为新的配置都是不安全的。因为不可能原子性的将所有服务器一次性的完成配置替换，所以如下图所示，在转换期间整个集群有可能分裂成两个独立的群体。</p>
<p><img src="/images/raft-9.png" alt="alt"></p>
<p>在上图中，集群由原来3台机器扩展为5台，由于每台服务器实际替换配置文件的时机不同，如红色箭头所示，存在某一时刻集群中可能会有两个主节点，假设此时发生选主，由于在 <code>Server 1</code> 看来集群中的节点数量还是3个，所以它只要获取到 <code>Server 2</code> 的选票就可以声明自己为主节点；而在 <code>Server 5</code> 看来，此时集群中有5个节点，所以它在获取了 <code>Server 3</code>、<code>Server 4</code> 的选票后就成为主节点，此时集群中就存在了两个主节点。</p>
<p>为了保证安全性，机器配置变更必须使用两阶段提交。有很多种方式来实现两阶段提交，例如，某些系统在第一阶段会先禁用掉旧的配置从而使系统不再处理客户端请求；然后在第二阶段启用新的配置。在 <code>Raft</code> 中，集群会先切换为一个被称之为 <code>joint consensus</code> 的过渡配置；一旦这个过渡配置被提交，集群就会切换为新配置。<code>joint consensus</code> 包含了新老两套配置：</p>
<ul>
<li>日志会复制到两套配置下的所有节点中。</li>
<li>新老配置下的节点都可以被选为主节点。</li>
<li>选主和日志提交需要同时获得新老配置下大多数节点的选票。</li>
</ul>
<p><code>joint consensus</code> 使得每台服务器能在不同的时间点切换配置而不会破坏 <code>Raft</code> 的安全性。另外，<code>joint consensus</code> 也保障了集群在切换期间能正常响应客户端请求。</p>
<p><img src="/images/raft-10.png" alt="alt"></p>
<p>集群配置通过日志文件中特殊的日志项来存储和通信，上图描述了配置变更的过程。当主节点收到需要将配置从 <code>C_old</code> 变为 <code>C_new</code> 的请求时，它首先将 <code>joint consensus</code> 的配置 <code>C_old_new</code> 写入到本地日志中，然后将其复制到过半数的从节点中。一旦某个节点将 <code>C_old_new</code> 写入到自己的日志中后，它后续的决定都会基于 <code>C_old_new</code> 的规则（<code>Raft</code> 中的节点始终使用最新的配置来做决策，不管这条日志是否已提交）。所以主节点会根据 <code>C_old_new</code> 需要的规则来决定 <code>C_old_new</code> 状态下的日志是否已提交。如果此时主节点异常，新的主节点可能来自 <code>C_old</code>，也可能来自 <code>C_old_new</code>，这取决于这个候选节点是否收到来自 <code>C_old_new</code> 的复制请求。不过不管哪种情况，在这期间 <code>C_new</code> 下的节点都不可能单方面做决策。</p>
<p>一旦 <code>C_old_new</code> 提交成功，说明集群中过半数的机器都有了 <code>C_old_new</code> 配置，此时不管是 <code>C_old</code> 还是 <code>C_new</code> 的机器都不可能单方面做决策，另外 <code>Leader Completeness Property</code> 也保证了后续没有 <code>C_old_new</code> 日志的节点不可能被选为主节点。所以此时主节点可以开始将 <code>C_new</code> 写入到日志中，然后再复制给其他从节点。同样的，各节点一旦收到 <code>C_new</code> 的配置就会以 <code>C_new</code> 的配置为准做决策。当 <code>C_new</code> 被提交后，旧配置就无关紧要了，那些不在新配置下的机器就可以被下线。在整个变更期间，没有任何一个时刻 <code>C_old</code> 和 <code>C_new</code> 的节点可以同时做决策，这就保证了安全性。</p>
<p>在配置变更时还有其他一些问题需要指出。第一个问题是新加入的节点一开始可能没有保存任何日志。如果它们以这个状态加入集群，那么它们可能需要很长一段时间来复制日志，可能会造成在这期间主节点无法提交新的日志。为了避免造成可用性问题，<code>Raft</code> 在配置变更前引入了额外的一个阶段，新加入的节点不会参与投票（主节点会将日志发给这些节点，但是在基于过半数节点原则做决策时会忽略这些节点）。当这些新加入的节点的日志追赶上其他节点后，<code>Raft</code> 就开始上面描述的配置变更流程。</p>
<p>第二个问题是在配置变更后当前的主节点可能不再属于新集群（例如归到其他集群或下线）。在这种情况下，一旦 <code>C_new</code> 的配置被提交，那么它就会退回到从节点状态。这说明存在某段时间（提交 <code>C_new</code> 的期间），当前主节点在管理集群时不会把自己计算在内；它在复制日志时依然会进行过半数的节点确认，但这个过半数的节点不包括自己。主节点状态的转变发生在 <code>C_new</code> 提交后，因为只有这个时间点 <code>C_new</code> 才能独立做决策（选出的主节点一定包含 <code>C_new</code> 的配置）。而在这个时间点之前，可能只有 <code>C_old</code> 的节点能被选为主节点。</p>
<p>第三个问题是被删除的节点可能会干扰集群（不在 <code>C_new</code> 中的节点）。因为这些节点不再收到心跳，所以过了 <code>election timeout</code> 后它们会发起选主流程。它们会向其他节点发送新任期的 <code>RequestVote</code> 请求，当前的主节点收到请求后因为新任期比自己的任期大（假设这些被剔除的节点在剔除时所属的任期和主节点相同；不过即使任期比主节点小也没关系，因为每次选主都会增加任期，这些被删除的节点不断的选主然后失败，任期会逐渐增加，最终超过主节点），主节点会认为自己的任期过期了，所以会转为从节点状态。最终系统会选择出一个新的主节点，不过这些被删除的节点又会再次超时然后再次发起选主，从而造成系统可用性问题。</p>
<p>为了避免这个问题，各节点在确认主节点存在的情况下会丢弃 <code>RequestVote</code> 请求。如果某个节点在最小 <code>election timeout</code> 内收到了 <code>RequestVote</code> 请求（前面提到，<code>election timeout</code> 的值是某个区间内的随机值，某个节点在收到主节点的心跳后，在还没有达到最小 <code>election timeout</code> 的时候就又收到 <code>RequestVote</code> 请求，例如 <code>election timeout</code> 的区间为 <code>[150, 300]</code>，这里最小 <code>election timeout</code> 指的就是100），则该节点不会更新自己的任期或者投票。这样做并不会影响正常的选主，因为正常的选择至少会等待一个最小 <code>election timeout</code> 时间。不过这样做却能避免那些被删除的节点对集群的干扰，只要主节点能给其他从节点发送心跳，那么它就不会受到被删除的节点所发送的更大的任期的影响。</p>
<h2 id="日志压缩"><a href="#日志压缩" class="headerlink" title="日志压缩"></a>日志压缩</h2><p>在和客户端的日常通信中，<code>Raft</code> 节点的日志会逐渐增长，但是在实际的系统中，日志不可能无限增长。当日志越来越多，它占据的空间也越来越大，需要根据日志来重放的时间也越来越多。如果没有一个机制来丢弃过期的日志，那么这最终会导致可用率问题。</p>
<p>快照是压缩日志的最简便的方法。执行快照时，整个系统的状态被写入到保存在可靠存储上的快照里，那么一直到快照执行时的日志都可以被丢弃。快照技术也在 <code>Chubby</code> 和 <code>ZooKeeper</code> 中使用，在本节剩余的内容中将会介绍 <code>Raft</code> 中的快照。</p>
<p>类似 <code>log cleaning</code> 和 <code>log-structured merge trees</code> 这样的增量手段也能处理压缩日志。它们每次只操作一部分数据，所以就将压缩带来的负载影响随着时间摊平。它们首先会选择一片已经积累了大量被删除或被覆盖的对象的数据区域，然后以更紧凑的方式重写这片区域内存活的对象，最后释放这片区域。这种手段相对于快照来说需要引入额外的机制同时复杂性也更高，而快照通过始终操作整个数据集来简化了问题。<code>log cleaning</code> 技术需要对 <code>Raft</code> 进行修改，状态机可以使用和快照相同的接口来实现 <code>log-structured merge trees</code>。</p>
<p><img src="/images/raft-11.png" alt="alt"></p>
<p>上图展示了 <code>Raft</code> 快照的概念。每台服务器会独立的执行快照，并且只包含已提交的日志。执行快照时大部分的工作是状态机将当前的状态写入到快照中。<code>Raft</code> 同时添加了一小部分元数据到快照中：快照对应的最后一个日志的索引（<code>last included index</code>，状态机已应用的最后一条日志），以及最后一个日志对应的任期（<code>last included term</code>）。这两个元数据信息用于执行快照后的第一次 <code>AppendEntries</code> 请求的一致性检查，因为需要比对前一个日志的索引和任期。同时为了启用集群配置变更，快照中也保存了当前最新的集群配置。一旦服务器完成了快照的写入，那么它就可以删除到快照为止的所有日志，以及以前的快照。</p>
<p>虽然各节点能独立的创建快照，主节点有时必须将快照发给那些落后的从节点。这个发生在主节点已经丢弃了需要发送给从节点的日志的情况下。幸运的是，这种情况在正常情况下不大可能发生：一个时刻和主节点保持同步的从节点已经包含了需要的日志。然而，某个运行异常缓慢的从节点或者新加入集群的从节点可能会缺少很多日志。所以主节点通过直接发送快照的方式来同步从节点的状态。</p>
<p>主节点通过一个新的 <code>RPC</code> 请求 <code>InstallSnapshot</code> 来将快照发送给那些落后太多的从节点。当从节点收到这个请求后，它必须决定如何处理目前已有的日志。一般来说，快照会包含当前从节点还没有的日志。在这种情况下，因为快照捕捉的是整个状态机的状态，说明快照对应的日志范围大于当前从节点的日志范围，所以从节点直接丢弃自己的日志即可。当然从节点的日志可能会包含某些未提交的日志和快照冲突，不过因为是未提交所以也没有问题。不过，如果从节点收到的快照只覆盖了自己日志的前一部分（可能是因为重传或者错传），那么和快照重合的日志就可以被删除，不过在这之后的日志还依然有效而且也必须保留。</p>
<p>快照违背了 <code>Raft</code> 的强主节点规则（<code>strong leader</code>），因为从节点可以在不知晓主节点的情况下创建快照。不过，作者认为这个做法是合理的。虽然主节点避免了做决策时发生冲突，不过因为快照保存的是已经应用到状态机的日志，本身就没有冲突，所以也就不需要主节点的存在。数据流依然只从主节点流向从节点，只不过有了快照后从节点可以重新组织自己的数据。</p>
<p>作者曾经考虑过另一种基于主节点的快照实现方案：快照只能由主节点创建，然后由主节点将快照发给每一个从节点。不过，这种方案有两个缺点。第一，由主节点发送快照会浪费带宽并且减慢了快照的完成速度。每个从节点本身就有了执行快照所需要的全部信息，从自身状态创建快照相比于从网络接收快照来说成本更低。第二，主节点的实现会更加复杂。例如，主节点需要并行的发送快照和新的日志给从节点，这样才不会阻塞新的客户端请求。</p>
<p>还有两个问题会影响快照的执行速度。第一，服务器必须决定在什么时候进行快照。如果执行快照太频繁，那么就会浪费磁盘 <code>IO</code> 和资源。如果执行快照频率太低，那么它就有耗尽存储空间的风险，以及延长了节点异常重启后重放日志的时间。一个简单的策略是当日志的大小达到一个固定的大小后就执行快照。如果这个大小远大于快照的大小，那么快照所带来的的磁盘 <code>IO</code> 影响就足够小。</p>
<p>第二个性能问题是一次写快照可能会花费较长的时间，所以需要它不能延误正常操作。解决方法是使用写时复制技术（<code>copy-on-write</code>），这样节点也能同时接受新的更新而不会影响快照。例如，由函数式数据结构组成的状态机天然的支持写时复制。或者操作系统的写时复制支持（例如 <code>Linux</code> 的 <code>fork</code>）可以用来在内存中创建一份整个状态机的快照（<code>Raft</code> 的实现采用了这种方式）。</p>
<p><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/papers/raft2-faq.txt">Raft (2) FAQ</a> 中提到写时复制是如何来实现的：</p>
<blockquote>
<p>当某个节点需要写快照时，它会调用 <code>Linux</code> 的 <code>fork</code> 方法，从而创建了一个子进程，子进程会复制父进程的内存空间，从而复制了状态机的状态。当然，如果单纯的复制内存显然是不现实的，<code>Linux</code> 不会整个复制内存，在这里就采用了写时复制技术，操作系统会将涉及的内存页标记为 <code>copy-on-write</code>，对于父进程和子进程来说都是只读的，当父进程或子进程想要写入某个内存页时会触发缺页中断（<code>page fault</code>），这个时候操作系统才会实际复制这个内存页。</p>
</blockquote>
<h2 id="客户端交互"><a href="#客户端交互" class="headerlink" title="客户端交互"></a>客户端交互</h2><p>本节描述了客户端如何与 <code>Raft</code> 交互，包括客户端如何找到集群中的主节点以及 <code>Raft</code> 如何支持线性化语义。这几个问题同时适用于其他基于共识的系统，<code>Raft</code> 的解决方案也和其他系统类似。</p>
<p><code>Raft</code> 的客户端会将所有请求发送给主节点。当客户端首次启动时，它会随机与一台服务器建立连接。如果客户端选中的服务器不是主节点，那么这台服务器就会拒绝客户端的请求并告知客户端它所知道的最近一段时间的主节点（<code>AppendEntries</code> 请求中包含了主节点的网络地址）。如果主节点异常了，那么客户端的请求会超时，然后它会继续随机挑选一台服务器重复上述流程。</p>
<p><code>Raft</code> 的实现目标是线性化语义（每个操作看起来都是立即执行，在请求和响应之间只执行了一次）。不过，通过对 <code>Raft</code> 的描述可以知道一个客户端的请求有可能被执行多次：例如，主节点在日志提交完成后但是在响应客户端前发生异常，那么客户端就会选择一个新的主节点发起重试，造成请求被执行两次。这个解决方案是让客户端在每次请求时生成一个唯一的编号。这样，状态机就可以跟踪每个请求的最新编号和对应的响应。如果某个节点收到的请求编号已经被处理过了，那么它就立即返回响应的结果而不会重新执行。</p>
<p>只读请求可以不记录日志而直接处理。然而，如果没有其他机制的保证，这有可能会返回过期的数据，因为和客户端通信的主节点已经有可能被其他的主节点所取代。在线性化语义下，<code>Raft</code> 不能够返回过期数据。<code>Raft</code> 需要额外的两个措施在不借助日志的情况下避免返回过期的数据。第一，主节点必须知道最新提交的日志的信息。虽然 <code>Leader Completeness Property</code> 保证了主节点有所有已提交的日志，但是在主节点任期开始的时候，它可能并不知道哪些日志是提交的。为了找到已经提交的日志，主节点需要在任期内提交一条日志。<code>Raft</code> 会要求主节点在任期开始时提交一条 <code>no-op</code> 的日志。<a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/papers/raft2-faq.txt">Raft (2) FAQ</a> 中解释了为什么这么做：</p>
<blockquote>
<p>提交 <code>no-op</code> 日志本质和提交普通的日志没有什么不同，也需要过半数的节点响应，一旦提交完成，那就说明 <code>no-op</code> 之前的日志也必然是被提交的，所以主节点就可以根据这个 <code>no-op</code> 日志为界来知道哪些日志被提交了。</p>
</blockquote>
<p>第二，主节点在处理只读请求前必须检查自己还是否是有效的主节点（如果有新的主节点，那么当前主节点持有的信息有可能过期了）。主节点会先和集群中过半数的节点交换心跳来确保自己还是主节点，然后再响应只读请求。另外，主节点也可以在心跳机制的基础上引入租约来确保自己是主节点，不过这就依赖时间来保证正确性（假设时间误差是有界的）。同样的，在 <a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/papers/raft2-faq.txt">Raft (2) FAQ</a> 中 <code>Robert Morris</code> 也提出了自己的设想：</p>
<blockquote>
<p>例如主节点在发给其他从节点的心跳中要求在接下来的100毫秒内其他节点不允许成为主节点，那么在接下来的100毫秒内主节点就可以直接处理只读请求而不用担心会有新的主节点。</p>
</blockquote>
<h2 id="RPC"><a href="#RPC" class="headerlink" title="RPC"></a>RPC</h2><p>这部分的内容来自于论文中的 <code>Figure 2</code> 和 <code>Figure 13</code>，主要描述了 <code>Raft</code> 涉及的 <code>RPC</code> 接口的实现要求以及其他一些系统约束。</p>
<h3 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h3><p>本节描述了各个节点内部需要维护的状态。</p>
<p>所有节点都需要持久化的状态（先持久化再响应 <code>RPC</code> 请求）：</p>
<ul>
<li><code>currentTerm</code>：当前节点所处在的最新任期（第一次启动时初始化为0，单调递增），如果这个不持久化，那么节点重启后由于不知道当前的准确任期从而无法发起选主流程，日志中虽然记录了任期但不一定是最新的，例如某个节点的日志可以一直为空但当前任期却一直在增长；同时也可以避免将选票投给任期更低的节点，以及识别出过期的主节点。</li>
<li><code>votedFor</code>：在当前任期内投票的候选节点 <code>id</code>，可以为空，因为 <code>Raft</code> 规定了在某个任期内每个节点最多只能投票一次，为了避免节点投票后发生异常然后重启并继续投票，所以需要持久化。</li>
<li><code>log[]</code>：日志是 <code>Raft</code> 的核心，是状态机重放的保证，必然要持久化。每条日志包含了发给状态机的命令和主节点收到请求时的任期，日志的索引位置从1开始。</li>
</ul>
<p>所有节点无需持久化的状态：</p>
<ul>
<li><code>commitIndex</code>：已知目前最远已提交的日志索引（初始化为0，单调递增）。对于主节点来说，只要在重启后提交了一条新的日志（或者自己主动提交一条 <code>no-op</code> 日志），那么这条日志之前的日志都会被间接提交，当前日志的索引就是最新的 <code>commitIndex</code>；对于从节点来说，后面 <code>AppendEntries</code> 中会提到它会发送主节点的最远提交索引，所以没有必要持久化。</li>
<li><code>lastApplied</code>：已知目前最远已应用到状态机的日志索引（初始化为0，单调递增）。对于不同的服务来说，它的状态机不一定是持久化的，例如内存数据库，这样的服务在重启后需要重放所有的日志来恢复状态，所以 <code>lastApplied</code> 从0开始，一直重放到 <code>commitIndex</code>。不过对于持久化的状态机来说，虽然从0开始重放不影响，但是效率太低，持久化 <code>lastApplied</code> 还是有必要的。</li>
</ul>
<p>主节点无需持久化的状态（选主后重新初始化）：</p>
<ul>
<li><code>nextIndex[]</code>：每个从节点插入下一条日志的索引位置（初始化为主节点最后一条日志的索引位置加1）。这个在前面提到过，根据主节点初始化的值，<code>nextIndex</code> 的有效值可以在多次 <code>RPC</code> 请求之间计算出来，所以没有必要持久化，而且持久化了有可能存在过期的风险。</li>
<li><code>matchIndex[]</code>：每个从节点已复制的日志的最大索引位置（初始化为0，单调递增）。这个值可以结合 <code>AppendEntries</code> 请求计算出来，只要 <code>AppendEntries</code> 成功了，那么 <code>matchIndex</code> 就等于主节点当前日志的索引位置。同样的，持久化了也有可能存在过期的风险。</li>
</ul>
<p>另外，某个主节点重启后不一定还能当选主节点，持久化 <code>nextIndex</code> 和 <code>matchIndex</code> 也意义不大。</p>
<h3 id="AppendEntries-RPC"><a href="#AppendEntries-RPC" class="headerlink" title="AppendEntries RPC"></a>AppendEntries RPC</h3><p><code>AppendEntries</code> 由主节点发送给从节点，用于复制日志和作为心跳探测。</p>
<p>参数：</p>
<ul>
<li><code>term</code>：主节点的任期</li>
<li><code>leaderId</code>：主节点 <code>id</code>，当客户端连接上一个从节点时，从节点可以将主节点的 <code>id</code> 发给客户端，客户端就能和主节点建立连接</li>
<li><code>prevLogIndex</code>：当前要写入的日志的前一个日志的索引</li>
<li><code>prevLogTerm</code>：当前要写入的日志的前一个日志的任期</li>
<li><code>entries</code>：需要从节点复制的日志（对于心跳来说这个字段为空），从效率考虑可能会包含多条日志，因为从节点的日志可能落后于主节点或者和主节点的不一致，这时就需要补齐从节点的日志</li>
<li><code>leaderCommit</code>：主节点已提交的日志索引</li>
</ul>
<p>返回：</p>
<ul>
<li><code>term</code>：当前从节点的任期，主节点收到以后如果发现自己的任期小于从节点的任期，就说明自己是个过期的主节点，从而会转为从节点状态，并更新当前的任期</li>
<li><code>success</code>：如果 <code>prevLogIndex</code> 和 <code>prevLogTerm</code> 和从节点当前的日志状态匹配，则返回 <code>true</code>，否则返回 <code>false</code>；主节点收到 <code>false</code> 就会更新 <code>nextIndex</code> 并继续发送新的 <code>prevLogIndex</code> 和 <code>prevLogTerm</code> 给从节点</li>
</ul>
<p>接受者实现：</p>
<ol>
<li>如果主节点的任期小于从节点的任期，则返回 <code>false</code></li>
<li>如果 <code>prevLogIndex</code> 和 <code>prevLogTerm</code> 和从节点当前的日志状态不匹配，则返回 <code>false</code></li>
<li>如果新添加的日志和当前某个位置的日志冲突（相同的日志索引，不同的任期），则删除该位置及其之后的日志</li>
<li>追加所有不在从节点中的日志</li>
<li>如果 <code>leaderCommit</code> 大于自身的 <code>commitIndex</code>，则更新 <code>commitIndex</code> 为 <code>min(leaderCommit, 最后一条新的日志的索引)</code></li>
</ol>
<h3 id="RequestVote-RPC"><a href="#RequestVote-RPC" class="headerlink" title="RequestVote RPC"></a>RequestVote RPC</h3><p><code>RequestVote</code> 在选主时由候选节点发起，用于向其他节点获取选票。</p>
<p>参数：</p>
<ul>
<li><code>term</code>：候选节点的任期</li>
<li><code>candidateId</code>：候选节点的 <code>id</code>，在前面提到每个节点需要持久化 <code>votedFor</code>，从而避免二次投票</li>
<li><code>lastLogIndex</code>：候选节点的最后一条日志的索引</li>
<li><code>lastLogTerm</code>：候选节点的最后一条日志的任期</li>
</ul>
<p>返回：</p>
<ul>
<li><code>term</code>：其他节点的任期，如果候选节点发现自己的任期比其他节点的小，那么根据规则它就不能成为主节点，从而退回到从节点状态，并更新当前的任期</li>
<li><code>voteGranted</code>：<code>true</code> 表示候选节点获得了一张选票，<code>false</code> 表示没有获得选票</li>
</ul>
<p>接受者实现：</p>
<ol>
<li>如果候选节点的 <code>term</code> 小于自身的任期，则返回 <code>voteGranted</code> 为 <code>false</code></li>
<li>如果 <code>votedFor</code> 为空或者等于 <code>candidateId</code>，并且候选节点的日志相比自身的日志一样新或者较新，则返回 <code>voteGranted</code> 为 <code>true</code></li>
</ol>
<h3 id="InstallSnapshot-RPC"><a href="#InstallSnapshot-RPC" class="headerlink" title="InstallSnapshot RPC"></a>InstallSnapshot RPC</h3><p><code>InstallSnapshot</code> 由主节点发起，用于向其他从节点发送快照块（<code>chunk</code>）。主节点始终会按顺序发送 <code>chunk</code>。</p>
<p>参数：</p>
<ul>
<li><code>term</code>：主节点的任期</li>
<li><code>leaderId</code>：主节点 <code>id</code>，当客户端连接上一个从节点时，从节点可以将主节点的 <code>id</code> 发给客户端，客户端就能和主节点建立连接</li>
<li><code>lastIncludedIndex</code>：快照所对应的的最后一条日志的索引，快照执行成功后，这个索引以及之前的日志就都可以被删除</li>
<li><code>lastIncludedTerm</code>：快照所对应的的最后一条日志的任期</li>
<li><code>offset</code>：当前 <code>chunk</code> 在整个快照数据中的偏移位置</li>
<li><code>data[]</code>：当前 <code>chunk</code> 的原始数据，起始于 <code>offset</code></li>
<li><code>done</code>：如果当前 <code>chunk</code> 是最后一个则为 <code>true</code>，否则为 <code>false</code></li>
</ul>
<p>返回：</p>
<ul>
<li><code>term</code>：当前从节点的任期，主节点收到以后如果发现自己的任期小于从节点的任期，就说明自己是个过期的主节点，从而会转为从节点状态，并更新当前的任期</li>
</ul>
<p>接受者实现：</p>
<ol>
<li>如果主节点的任期小于从节点的任期，则直接返回</li>
<li>如果当前 <code>chunk</code> 是第一个（<code>offset</code> 为0），则新建一个快照文件</li>
<li>以 <code>offset</code> 为起点，将 <code>data[]</code> 写入文件</li>
<li>如果 <code>done</code> 是 <code>false</code>，则继续等待下一个 <code>chunk</code></li>
<li>当所有 <code>chunk</code> 接受完毕后，保存快照文件，丢弃所有旧的快照文件</li>
<li>如果 <code>lastIncludedIndex/lastIncludedTerm</code> 对应的日志在自身中存在，则保留 <code>lastIncludedIndex</code> 之后的日志并返回</li>
<li>删除全部的日志（符合第6条情况的日志除外）</li>
<li>将快照的内容应用到状态机中，并加载快照中的集群配置</li>
</ol>
<h3 id="各节点需要遵循的规则"><a href="#各节点需要遵循的规则" class="headerlink" title="各节点需要遵循的规则"></a>各节点需要遵循的规则</h3><p>所有节点：</p>
<ul>
<li>如果 <code>commitIndex</code> 大于 <code>lastApplied</code>，则自增 <code>lastApplied</code>，并应用 <code>log[lastApplied]</code> 到状态机</li>
<li>如果某个 <code>RPC</code> 的请求或响应中的 <code>term T</code> 大于 <code>currentTerm</code>，则设置 <code>currentTerm = T</code>，并转为从节点状态</li>
</ul>
<p>从节点：</p>
<ul>
<li>响应来自候选节点和主节点的 <code>RPC</code> 请求</li>
<li>如果在 <code>election timeout</code> 期间内没有收到来自当前主节点的 <code>AppendEntries</code> 的请求以及候选节点的 <code>RequestVote</code> 的请求，则转为候选节点</li>
</ul>
<p>候选节点：</p>
<ul>
<li>由从节点转换为候选节点后，开始选主：<ul>
<li>将 <code>currentTerm</code> 自增</li>
<li>给自己投票，设置 <code>votedFor</code> 为自己的 <code>id</code></li>
<li>重置 <code>election timeout</code></li>
<li>并行给其他节点发送 <code>RequestVote</code> 请求</li>
</ul>
</li>
<li>如果候选节点收到了过半数的选票，则转换为主节点</li>
<li>如果候选节点收到了某个主节点的 <code>AppendEntries</code> 的请求（任期校验有效），则转换为从节点</li>
<li>如果在一个 <code>election timeout</code> 期间内没有完成选主，则回到第一步重新开始选主</li>
</ul>
<p>主节点：</p>
<ul>
<li>一旦成为主节点，就开始向其他节点发送空的 <code>AppendEntries</code> 请求（心跳），并在空闲时周而复始的发送，避免其他节点进入选主过程</li>
<li>如果收到来自客户端的请求，则追加一条新的日志，并在将日志应用到状态机后返回结果给客户端</li>
<li>如果最后一条日志的索引大于等于某个从节点的 <code>nextIndex</code>，说明从节点的日志和主节点的日志不一致，则主节点会将 <code>[nextIndex, 当前主节点的最后一条日志的索引]</code> 范围内的日志通过 <code>AppendEntries</code> 请求发给从节点：<ul>
<li>如果执行成功，则更新该从节点对应的 <code>nextIndex</code> 和 <code>matchIndex</code></li>
<li>如果执行失败，说明当前 <code>nextIndex</code> 还不是最优解，将 <code>nextIndex</code> 减1后继续重试</li>
</ul>
</li>
<li>如果存在某个数字 <code>N</code> 满足 <code>N &gt; commitIndex</code>，且过半数的从节点的 <code>matchIndex &gt;= N</code>，以及 <code>log[N].term</code> 等于 <code>currentTerm</code>，则将 <code>commitIndex</code> 设置为 <code>N</code>。这说明 <code>N</code> 是实质上已提交的日志索引，但是主节点还不知道</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf">In Search of an Understandable Consensus Algorithm (Extended Version)</a></li>
<li><a target="_blank" rel="noopener" href="http://thesecretlivesofdata.com/raft/">Raft: Understandable Distributed Consensus</a></li>
<li><a target="_blank" rel="noopener" href="https://thesquareplanet.com/blog/students-guide-to-raft/">Students’ Guide to Raft</a></li>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/notes/l-raft2.txt">6.824 2022 Lecture 7: Raft (2)</a></li>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/papers/raft2-faq.txt">Raft (2) FAQ</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/46223417/how-does-raft-handle-committing-entries-from-previous-one">How does raft handle committing entries from previous one?</a></li>
<li><a target="_blank" rel="noopener" href="https://thesquareplanet.com/blog/raft-qa/">Raft Q&amp;A</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://frederick-s.github.io/2022/04/24/mit-6.824-vm-ft/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaodan Mao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Übung macht den Meister">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/24/mit-6.824-vm-ft/" class="post-title-link" itemprop="url">MIT 6.824 - The Design of a Practical System for Fault-Tolerant Virtual Machines</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-24 00:00:00" itemprop="dateCreated datePublished" datetime="2022-04-24T00:00:00+08:00">2022-04-24</time>
            </span>

          
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>12k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>20 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>和一般描述的应用级别的主从备份不同，本文描述的是虚拟机的主从备份。主从备份作为一种常见的容错实现手段，当主节点异常时，从节点能取代主节点从而保证系统依然可用。作为从节点，它的状态必须尽可能的与主节点随时保持一致，这样当主节点异常时从节点能马上取代主节点，而客户端也不会感知到异常，同时也没有数据丢失。其中一种同步主从节点状态的方式是持续将主节点的所有修改发送给从节点，这里的修改包括 <code>CPU</code>、内存以及 <code>IO</code> 设备。然而，采用这种同步方式需要大量的网络带宽，尤其是发送内存的修改。</p>
<p>另一种只需要耗费少量带宽的方式是状态机（<code>state machine</code>）同步。该方法将主从同步抽象为确定性状态机（<code>deterministic state machine</code>）同步问题，在确定性状态机模型下，对于两个初始状态一样的状态机来说，按照相同的顺序执行相同的一系列输入指令后，最后的状态也一定是相同的。然而，对于大部分的服务来说，存在某些非确定性的操作，例如生成一个随机数，这时候就需要额外的协调使得主从间依然是同步的，即从节点也要生成一模一样的随机数。不过，处理这种情况所需要维护的额外信息相比于主节点状态的修改（主要是内存的修改）来说不值一提。</p>
<p>对于物理机来说，随着主频的增加，同步主从间的确定性操作也愈发困难。然而对于运行在 <code>hypervisor</code> 上的虚拟机来说却非常适合实现状态机同步。一个虚拟机本身就可以看做一个明确的状态机，它的所有操作就是被虚拟化的机器的操作（包括所有的设备）。和物理机一样，虚拟机也存在一些非确定性的操作（例如读取当前时间或者发送一个中断），所以也需要发送额外的信息给从节点来保证主从同步。因为 <code>hypervisor</code> 掌管着虚拟机的执行，包括发送所有的输入给被虚拟化的机器，所以它能捕获到执行非确定性操作的所有需要的信息，从而能正确的在从节点上执行重放操作。</p>
<p>因此，基于状态机同步的主从同步方式可以在不需要修改硬件的情况下在廉价的硬件上实现，使得容错技术适用于最新的微处理器。另外，对带宽较低的要求使得长距离的虚拟机主从同步成为了可能。例如，可以在跨校园间不同的物理机上做主从同步，相比于同大厦内的主从同步更为可靠。</p>
<p>目前在 <code>VMware vSphere 4.0</code> 平台上已经实现了这种容错技术，该平台能高效完整的虚拟化 <code>x86</code> 架构的机器。因为 <code>VMware vSphere</code> 实现了一个完全的 <code>x86</code> 虚拟机，所以可以自动的对任何 <code>x86</code> 的操作系统和应用提供容错支持。通过确定性重放（<code>deterministic replay</code>），系统可以记录下主节点的执行并且确保能在从节点执行相同的操作。<code>VMware vSphere Fault Tolerance (FT)</code> 在此基础之上增加了额外的功能和协议来支持构建一个可完全容错的系统。除了对硬件的容错外，当主节点异常时，系统能自动的在本地集群中启动一台可用的从节点来接管主节点。在该篇论文发表的时候，确定性重放技术和 <code>VMware FT</code> 仅支持单核的虚拟机。受限于严重的性能问题，多核虚拟机的重放支持仍在进行中，因为在多核场景下，几乎每一个对共享内存的访问都是一个非确定性的操作。</p>
<p><code>Bressoud</code> 和 <code>Schneider</code> 针对惠普的 <code>PA-RISC</code> 平台的虚拟机容错做了个原型实现。<code>VMware</code> 的实现与其类似，不过出于性能的考虑做了些根本的修改以及调研了一些其他实现方案。另外，为了能构建一个高效、可用的容错系统来支持用户的企业级应用，<code>VMware</code> 还设计和实现了许多其他组件以及解决一些实际的问题。和大多数实际的系统要解决的问题一样，这里的容错针对的是 <code>fail-stop</code> 的异常，即在造成外部可见的不正确的行为前可被监测到的异常，例如磁盘空间不足、网络无法连通等等，而诸如应用程序的 <code>bug</code> 或者人为失误等则不属于 <code>fail-stop</code> 异常，系统也无法进行容错。</p>
<h2 id="基础设计"><a href="#基础设计" class="headerlink" title="基础设计"></a>基础设计</h2><p><img src="/images/vm-ft-1.png" alt="alt"></p>
<p>上图展示了支持容错的虚拟机的基本配置。对于每一台需要支持容错的虚拟机（<code>primary VM</code>），系统会在其他物理机上同时运行一台备份虚拟机（<code>backup VM</code>），备份虚拟机和主虚拟机会保持同步，并执行和主虚拟机相同的指令，不过会存在一定的延迟。这两台虚拟机被称为处于 <code>virtual lockstep</code> 状态。同时，虚拟机连接着相同的共享存储，输入和输出都可以被主从虚拟机访问。不过，只有主虚拟机才会暴露在网络中，所以所有的网络输入都只会发送给主虚拟机。同样的，其他所有的输入（例如键盘和鼠标输入）也都只会发送给主虚拟机。</p>
<p>主虚拟机收到的所有输入都会通过 <code>logging channel</code> 发送给从虚拟机。对系统来说，主要的输入负载就是网络和磁盘。为了保证从虚拟机能和主虚拟机执行相同的非确定性操作，还需要发送一些额外的信息给从虚拟机。从结果上来说，从虚拟机会始终执行和主虚拟机相同的操作。不过，所有从虚拟机的输出都会被 <code>hypervisor</code> 丢弃，只有主虚拟机的输出才会返回给客户端。后面会提到，主从虚拟机间的通信会遵循一个特定的协议，包括从虚拟机对消息的确认，来保证当主虚拟机异常时不会发生数据丢失。</p>
<p>为了监测主虚拟机或者从虚拟机是否发生异常，系统会通过和主从虚拟机间的心跳以及 <code>logging channel</code> 的流量来判断。另外，系统必须保证在任一时间只有一台主虚拟机或者从虚拟机作为对外执行的入口，即使主虚拟机和从虚拟机间失联发生脑裂的场景。</p>
<h3 id="确定性重放的实现"><a href="#确定性重放的实现" class="headerlink" title="确定性重放的实现"></a>确定性重放的实现</h3><p>在前面提到过，主从虚拟机的同步可以抽象为确定性状态机同步问题。如果两个确定性状态机以相同的初始状态启动，并且按照相同的顺序执行相同的输入，那么这两个状态机会经历相同的状态流转并输出相同的结果。一台虚拟机会有一系列的输入，包括网络包，磁盘读取，以及键盘和鼠标输入。而非确定性的事件（例如虚拟中断（<code>virtual interrupts</code>））和非确定性的操作（例如读取当前处理器的时钟周期数）也会影响虚拟机的内部状态。这就给重放执行一台运行着任意操作系统和任意服务的虚拟机带来了3个挑战：</p>
<ol>
<li>需要正确的捕捉到主虚拟机的所有输入和非确定性的操作</li>
<li>需要正确的将输入和非确定性操作在从虚拟机上重放</li>
<li>不能影响系统性能</li>
</ol>
<p>另外，<code>x86</code> 处理器中很多复杂的操作往往伴有副作用，因此也是非确定性的操作，如何捕捉到这些非确定性的操作并正确的在从虚拟机上重放也是一个挑战。</p>
<p><code>VMware vSphere</code> 平台为 <code>x86</code> 虚拟机提供了上述的重放功能。确定性重放技术会将主虚拟机的所有输入和所有可能的非确定性操作写入到日志中。从虚拟机就可以读取日志并执行和主虚拟机一样的操作。对于非确定性的操作来说，系统会写入一些额外的信息来保证重放时生成相同的虚拟机状态和输出。对于非确定性的事件例如计时器或者 <code>IO</code> 完成中断，在事件发生时所执行的指令也会记录在日志中。在重放时，事件会和指令一同出现在指令流中。借助联合 <code>AMD</code> 和 <code>Intel</code> 开发的 <code>hardware performance counter</code>（一组特殊寄存器用来记录硬件相关事件发生的次数）和其他技术，<code>VMware</code> 确定性重放技术能高效的记录和重放非确定性的事件。</p>
<p><code>Bressound</code> 和 <code>Schneider</code> 在其实现中提到将虚拟机的执行以 <code>epoch</code> 为单位进行切分，其中所有的非确定性操作例如中断都放在 <code>epoch</code> 的最后。这个想法是出于批量处理的考虑，因为单独将每一个中断和中断发生时对应的指令重放执行代价较大。不过 <code>VMware</code> 的实现足够高效使得不需要借助 <code>epoch</code> 来实现确定性重放，每一个中断都能被准确的记录并伴随着发生中断时的指令一起执行。</p>
<h3 id="FT-协议"><a href="#FT-协议" class="headerlink" title="FT 协议"></a>FT 协议</h3><p><code>VMware FT</code> 使用确定性重放技术将主虚拟机的执行流记录到日志中，不过主虚拟机并不是将日志写入到磁盘上，而是通过 <code>logging channel</code> 将日志发送给从虚拟机。从虚拟机能实时的读取日志并将其重放，从而执行和主虚拟机一样的操作。然而，双方在 <code>logging channel</code> 的通信必须遵循 <code>FT</code> 协议来保证容错。其中一条基本的要求是：</p>
<blockquote>
<p><code>Output Requirement</code>：当主虚拟机异常，从虚拟机接管执行时，从虚拟机的输出必须和先前主虚拟机已经发送给客户端的输出一致。</p>
</blockquote>
<p>当异常发生时（主虚拟机异常，从虚拟机接管执行），从虚拟机的执行可能会和在没有异常发生时主虚拟机的执行不同，因为在执行时会有很多非确定性的操作。然而，只要从虚拟机的输出满足 <code>Output Requirement</code>，则在主从切换时就不会有外部可见的状态或者数据丢失，而客户端也不会感知到中断或者服务的不一致。</p>
<p>通过主虚拟机的延迟输出，保证从虚拟机确认收到了所有日志后，主虚拟机才将输出返回给客户端来实现 <code>Output Requirement</code>。一个先决的条件是主虚拟机在执行输出操作前，从虚拟机必须已经收到所有的日志。这些日志能保证从虚拟机执行到主虚拟机最新的执行点。然而，假设当主虚拟机刚开始执行输出操作时发生了异常，此时发生了主从切换，从虚拟机必须先将未处理完的日志进行重放，然后才能 <code>go live</code>（不再执行重放，接管成为主虚拟机）。如果在这之前从虚拟机 <code>go live</code>，可能会有一些非确定性的事件（例如计时器中断）在从虚拟机执行输出操作前改变了执行的路径。</p>
<p>针对上述的要求，最简单的实现 <code>Output Requirement</code> 的方式是为每一条输出操作创建一条特殊的日志，从而可以通过以下规则来保证 <code>Output Requirement</code>：</p>
<blockquote>
<p><code>Output Rule</code>：在从虚拟机确认收到输出操作对应的日志前，主虚拟机不能执行输出操作。</p>
</blockquote>
<p>如果从虚拟机收到了所有的日志，包括输出操作对应的日志，那么从虚拟机就能重放出和主虚拟机在执行输出操作时一模一样的状态，当主虚拟机异常时，从虚拟机就能恢复到主虚拟机执行输出操作前一致的状态。相反的，如果从虚拟机在没有收到日志前就接管了主虚拟机的操作，那么它的状态就和主虚拟机不一致，从而导致最终的输出也不一致。</p>
<p>注意 <code>Output Rule</code> 并没有要求在从虚拟机确认收到输出日志前停止主虚拟机的执行。这里只是延迟了主虚拟机的输出，它依然可以执行其他指令。因为操作系统以异步中断的方式来通知非阻塞网络和磁盘输出的完成，主虚拟机可以在这期间轻易的执行其他指令，而不用阻塞等待。相反的，在其他的一些实现中，在从虚拟机确认收到输出日志前，主虚拟机必须完全停止等待。</p>
<p><img src="/images/vm-ft-2.png" alt="alt"></p>
<p>上图展示了 <code>FT</code> 协议的要求。主虚拟机到从虚拟机的箭头表示日志的发送，从虚拟机到主虚拟机的箭头表示日志的确认。所有异步事件，输入和输出的操作都必须发送给从虚拟机并得到确认。只有当从虚拟机确认了某条输出操作的日志后，主虚拟机才能执行输出操作。所以只要遵循了 <code>Output Rule</code>，从虚拟机在接管执行时就能保持和主虚拟机一致的状态。</p>
<p>不过在异常发生时，<code>VMware FT</code> 并不能保证所有的输出只发送一次。在缺少两阶段提交的帮助下，从虚拟机不能知晓主虚拟机在发送某条输出之前还是之后发生了异常。不过，网络协议（包括常见的 <code>TCP</code> 协议）在设计时就已经考虑了包的丢失和重复包的情况，所以这里无需特殊处理。另外，在主虚拟机异常时发送给主虚拟机的输入也有可能丢失，因此从虚拟机也会丢失这部分的输入。不过，即使在主虚拟机没有异常的情况下，网络包本身就有可能丢失，所以这里同样也不需要特殊处理，不管是网络协议、操作系统还是应用程序，在设计和编写时本身已经考虑到了包丢失的情况。</p>
<h3 id="监测和响应异常"><a href="#监测和响应异常" class="headerlink" title="监测和响应异常"></a>监测和响应异常</h3><p>之前提到过，当主虚拟机或者从虚拟机发生异常时，双方都必须能快速响应。当从虚拟机异常时，主虚拟机会进入 <code>go live</code> 模式，即不再记录执行日志，以常规的方式执行。当主虚拟机异常时，从虚拟机也会进入 <code>go live</code> 模式，不过相比于主虚拟机略微复杂些。因为从虚拟机在执行上本身就落后于主虚拟机，在主虚拟机异常时，从虚拟机已经收到和确认了一部分执行日志，但是还没有执行重放，此时从虚拟机的状态和主虚拟机还是不一致的。所以，从虚拟机必须先将暂存的日志进行重放，当所有重放都执行完成后，从虚拟机就会进行 <code>go live</code> 模式，正式接管主虚拟机（此时缺少一个从虚拟机）。因为此时这台从虚拟机不再是从虚拟机，被虚拟化的操作系统所执行的输出操作都会发送给客户端。在这个转换期间，可能也需要某些设备执行一些特定的操作来保证后续输出的正确性。特别是对于网络输出来说，<code>VMware FT</code> 会自动的将新的主虚拟机的 <code>MAC</code> 地址在网络中广播，使得物理交换机知道最新的主虚拟机的地址。另外，后文会提到新的主虚拟机可能会重新发起一些磁盘 <code>IO</code> 操作。</p>
<p>有很多种方式来监测主虚拟机和从虚拟机的异常。<code>VMware FT</code> 通过 <code>UDP</code> 心跳来监测启用了容错的虚拟机是否发生了异常。另外，<code>VMware FT</code> 还会监控 <code>logging channel</code> 中的流量，包括主虚拟机发送给从虚拟机的日志，以及从虚拟机的消息确认回执。因为操作系统本身存在时钟中断，所以理论上来说 <code>logging channel</code> 中的流量应该是连续不断的。因此，如果监测到 <code>logging channel</code> 中没有流量了，那么就可以推断出某台虚拟机发生了异常。如果没有心跳或者 <code>logging channel</code> 中没有流量超过一段指定的时间（近似几秒钟），那么系统就会声明这台虚拟机发生了异常。</p>
<p>然而，这种异常监测机制可能会引发脑裂问题。如果从虚拟机不再接收到来自主虚拟机的心跳，那么有可能说明主虚拟机发生了异常，但也有可能只是双方间的网络断开。如果此时从虚拟机进入 <code>go live</code> 模式，由于此时主虚拟机依然存活，就有可能给客户端造成数据损坏或其他问题。因此，当监测到异常时必须保证只有一台主虚拟机或者从虚拟机进入 <code>go live</code> 模式。为了解决脑裂问题，<code>VMware FT</code> 借助了虚拟机所连接的共享存储。当主虚拟机或者从虚拟机希望进入 <code>go live</code> 模式时，它会向共享存储发起一个原子性的 <code>test-and-set</code> 操作。如果操作成功，那么当前虚拟机可以进入 <code>go live</code> 模式，如果操作失败，说明已经有其他虚拟机先进入了 <code>go live</code> 模式，所以当前虚拟机就将自己挂起。如果虚拟机访问共享存储失败，那么它会一直等待直到访问成功。如果共享存储由于网络问题造成无法访问，那么虚拟机本身也做不了什么因为它的虚拟磁盘就挂载在共享存储上。所以使用共享存储来解决脑裂问题不会带来其他可用性问题。</p>
<p>最后一个设计的点是如果虚拟机发生了异常，使得某台虚拟机进入了 <code>go live</code> 模式，那么 <code>VMware FT</code> 会自动在其他物理机上启动一台新的备份虚拟机。</p>
<h2 id="FT-的实际实现"><a href="#FT-的实际实现" class="headerlink" title="FT 的实际实现"></a>FT 的实际实现</h2><p>上节主要描述了 <code>FT</code> 的基础设计和协议。然而，为了构建一个可用，健壮和自动化的系统，还需要设计和实现很多其他的组件。</p>
<h3 id="启动和重启-FT-虚拟机"><a href="#启动和重启-FT-虚拟机" class="headerlink" title="启动和重启 FT 虚拟机"></a>启动和重启 FT 虚拟机</h3><p>其中一个至关重要的组件是如何启动一台有着和主虚拟机一模一样状态的备份虚拟机。这个同时也会在当某台虚拟机异常需要重新启动一台备份虚拟机时用到。因此，这个组件必须能够在主虚拟机处于任意状态的时候复制一台一模一样的备份虚拟机（而不仅仅是初始状态）。另外，这个启动操作还不能影响到主虚拟机的执行，因为这有可能影响到当前所有连接着的客户端。</p>
<p>对于 <code>VMware FT</code> 来说，它借用了 <code>VMware vSphere</code> 平台已有的 <code>VMotion</code> 的功能。<code>VMware VMotion</code> 能以极小的代价将一台运行中的虚拟机迁移到另一台机器上——虚拟机的暂停时间一般在一秒内。<code>VMware FT</code> 对 <code>VMotion</code> 做了些改动使得在不销毁当前虚拟机的情况下，在远程服务器上复制一台和当前虚拟机一模一样的虚拟机。即修改版的 <code>FT VMotion</code> 做的是虚拟机的复制而不是迁移。<code>FT VMotion</code> 也会同时建立一条 <code>logging channel</code>，源虚拟机作为主虚拟机就会将执行日志写入到 <code>logging channel</code>，而复制后的虚拟机作为从虚拟机就会读取日志开始重放执行。和常规的 <code>VMotion</code> 一样，<code>FT VMotion</code> 也能将对主虚拟机的暂停控制在一秒以内。因此，对某台运行中的虚拟机开启 <code>FT</code> 功能非常简单，且没有破坏性。</p>
<p>另一个启动一台备份虚拟机要考虑的点是选择在哪台物理机上启动。支持容错的虚拟机运行在某个可访问共享存储的集群中，所以本质上虚拟机可以运行在任意一台机器上。这个灵活性使得 <code>VMware vSphere</code> 可以轻易的为一台或多台异常的虚拟机启动新的备份虚拟机。<code>VMware vSphere</code> 实现了一套集群服务来管理集群中的资源。当虚拟机发生异常主虚拟机需要一台新的虚拟机来维持容错时，主虚拟机会通知集群服务需要一台新的备份虚拟机。此时集群服务会根据资源利用率和其他条件来决定在哪台机器上重启新的备份虚拟机，然后会由 <code>FT VMotion</code> 创建一台新的备份虚拟机。一般来说，<code>VMware FT</code> 可以在发生异常的几分钟内恢复某台虚拟机的冗余功能，而不会造成任何中断。</p>
<h3 id="管理-Logging-Channel"><a href="#管理-Logging-Channel" class="headerlink" title="管理 Logging Channel"></a>管理 Logging Channel</h3><p>在管理 <code>logging channel</code> 的流量上有很多有趣的实现细节。在 <code>VMware</code> 的实现里，<code>hypervisor</code> 为主虚拟机和从虚拟机维护了一大块日志缓冲区。在主虚拟机执行时，它将执行日志发送到日志缓冲区中，类似的，从虚拟机从日志缓冲区中消费日志。每当主虚拟机的日志缓冲区中有数据，系统就会将其发送到 <code>logging channel</code> 中，相应的在另一边则会将其放入到从虚拟机的日志缓冲区中。当从虚拟机从 <code>logging channel</code> 中读取到日志并将其放入日志缓冲区后，它就会向主虚拟机发送一个已读消息的回执。<code>VMare FT</code> 就可以根据这个已读回执决定哪些输出操作可以执行了。下图展示了这个过程：</p>
<p><img src="/images/vm-ft-3.png" alt="alt"></p>
<p>当从虚拟机从日志缓冲区读取不到任何日志时（日志缓冲区为空），它就会暂停执行直到下一条日志到达。因为从虚拟机不和外界交互，这个暂停对客户端没有任何影响。类似的，如果主虚拟机往日志缓冲区中写日志发现日志缓冲区满时，它必须停止执行直到日志缓冲区被消费。这个暂停保证了主虚拟机以一个可控的速度生产执行日志。不过，这个暂停会影响客户端的响应，直到主虚拟机可以继续写日志并恢复执行。所以，在实现时必须考虑如何尽量避免主虚拟机的日志缓冲区写满。</p>
<p>其中一个原因造成主虚拟机的日志缓冲区写满是因为从虚拟机执行的太慢从而造成消费日志太慢。一般来说，从虚拟机必须以和主虚拟机记录执行日志一样的速度来执行重放。幸运的是，在 <code>VMware FT</code> 的实现下，记录执行日志和重放所需要的时间基本是相同的。不过，如果从虚拟机所在的机器存在和其他虚拟机资源竞争（资源超卖），不管 <code>hypervisor</code> 的虚拟机调度多么高效，从虚拟机都有可能得不到足够的 <code>CPU</code> 和内存资源来保证和主虚拟机一样的速度执行重放。</p>
<p>除了主虚拟机日志缓冲区满造成的不可控暂停外，还有一个原因也要求主从虚拟机间的状态不能差太远。当主虚拟机异常时，从虚拟机必须尽快的将所有的执行日志进行重放，达到和主虚拟机一样的状态，然后接管主虚拟机向客户端提供服务。结束重放的时间基本上等于异常发生时从虚拟机落后主虚拟机的时间，所以从虚拟机进入 <code>go live</code> 模式需要的时间就基本上等于检测出异常的时间加上当前从虚拟机落后的时间。因此，从虚拟机不能落后主虚拟机太多（大于一秒），否则这会大大增加故障切换的时间。</p>
<p>因此，<code>VMware FT</code> 有另一套机制来保证从虚拟机不会落后主虚拟机太多。在主从虚拟机间的通信协议里，还会发送额外的信息来计算两者间的执行时间差。一般来说这个时间差在100毫秒以内。如果从虚拟机开始明显落后主虚拟机（例如大于1秒），那么 <code>VMware FT</code> 会通知调度器降低主虚拟机的 <code>CPU</code> 资源配额（初始减少几个百分点）来延缓主虚拟机的执行。<code>VMware FT</code> 会根据从虚拟机返回的落后时间来不断调整主虚拟机的 <code>CPU</code> 资源配额，如果从虚拟机一直落后，那么 <code>VMware FT</code> 会逐渐减少主虚拟机的 <code>CPU</code> 资源配额。相反的，如果从虚拟机开始赶上了主虚拟机的执行速度，那么 <code>VMware FT</code> 会逐渐增加主虚拟机的 <code>CPU</code> 资源配额，直到两者的执行时间差到达一个合理的值。</p>
<p>不过在实际场景中减慢主虚拟机执行的速度非常少见，一般只会发生在系统承受极大负载的情况下。</p>
<h3 id="FT-虚拟机上的操作"><a href="#FT-虚拟机上的操作" class="headerlink" title="FT 虚拟机上的操作"></a>FT 虚拟机上的操作</h3><p>另一个实际中要考虑的问题是处理针对主虚拟机的一系列控制操作。例如，如果主虚拟机主动关机了，从虚拟机也需要同步关机，而不是进入 <code>go live</code> 模式。另外，所有对主虚拟机的资源修改（例如增加 <code>CPU</code> 资源配额）都必须应用到从虚拟机上。针对这些操作，系统会将其转化为特殊的执行日志发送到 <code>logging channel</code>，从虚拟机收到后也会将其正确的重放。</p>
<p>一般来说，大部分对虚拟机的操作都只应该在主虚拟机上发起。然后 <code>VMware FT</code> 会将其转化为日志发送给从虚拟机来进行重放。唯一可以独立的在主虚拟机和从虚拟机上执行的操作是 <code>VMotion</code>。即主虚拟机和从虚拟机都可以独立的被复制到其他机器上。<code>VMware FT</code> 保证了在复制虚拟机时不会将其复制到一台已经运行了其他虚拟机的机器上，因为这无法提供有效的容错保证。</p>
<p>复制一台主虚拟机要比迁移一台普通的虚拟机复杂些，因为从虚拟机需要先和主虚拟机断开连接，然后之后在合适的时间和新的主虚拟机建立连接。从虚拟机的复制有类似的问题，不过也略微复杂些。对于普通的虚拟机迁移来说，系统会要求当前所有进行中的磁盘 <code>IO</code> 在切换前执行完成。对于主虚拟机来说，它可以等待所有进行中的磁盘 <code>IO</code> 的完成通知。而对于从虚拟机来说，它不能简单的在某个时间让所有的磁盘 <code>IO</code> 都结束，因为从虚拟机还在执行重放，也需要等待重放涉及的磁盘 <code>IO</code> 完成。而另一方面，主虚拟机在执行时有可能一直伴有磁盘 <code>IO</code>。<code>VMware FT</code> 有一个特有的方法来解决这个问题，当从虚拟机在执行切换前，会通过 <code>logging channel</code> 向主虚拟机发送请求要求暂时结束所有的磁盘 <code>IO</code>，主虚拟机收到请求后会天然的将其转化为执行日志发送到 <code>logging channel</code>，从虚拟机的磁盘 <code>IO</code> 也因此会伴随着重放而终止。</p>
<h3 id="磁盘-IO-的实现问题"><a href="#磁盘-IO-的实现问题" class="headerlink" title="磁盘 IO 的实现问题"></a>磁盘 IO 的实现问题</h3><p><code>VMware</code> 在实现时遇到了一些和磁盘 <code>IO</code> 相关的问题。首先，因为磁盘操作是非阻塞的所以可以并行执行，同一时间访问同一块磁盘区域就会导致非确定性结果。另外，磁盘 <code>IO</code> 会通过 <code>DMA</code>（<code>Direct memory access</code>） 来直接访问虚拟机的内存，因此同一时间的磁盘操作如果访问到了同一块内存页也会导致非确定性结果。<code>VMware</code> 通过监测这样的 <code>IO</code> 竞争（实际场景中极少）并强制让其在主虚拟机和从虚拟机中串行执行来解决这个问题。</p>
<p>第二，磁盘操作访问内存可能和应用程序访问内存产生竞争，因为磁盘操作可以通过 <code>DMA</code> 直接访问内存。例如，如果应用程序正在访问某个内存块，而此时磁盘也在写入这个内存块，则会发生非确定性的结果。虽然这种情况同样很少，但也仍然需要能够监测并解决。其中一种解决方案是当磁盘正在访问某个内存页时暂时对该页设置页保护。当应用程序尝试访问页保护的内存页时，会触发一个陷阱（<code>trap</code>）使得操作系统能够暂停执行直到磁盘操作完成。不过，修改 <code>MMU</code>（<code>Memory management unit</code>）的页保护机制代价较大，<code>VMware</code> 借助 <code>bounce buffer</code> 来解决这个问题。<code>bounce buffer</code> 是一块和磁盘操作正在访问的内存一样大小的临时缓冲区。磁盘的读操作被修改为磁盘先将数据复制到 <code>bounce buffer</code> 中，然后虚拟机读取 <code>bounce buffer</code> 中的数据，而且只有在磁盘 <code>IO</code> 完成后才会将 <code>bounce buffer</code> 中的数据复制到虚拟机内存中。类似的，对于磁盘写操作，数据会先写到 <code>bounce buffer</code> 中，磁盘再将数据从 <code>bounce buffer</code> 中复制到磁盘上。使用 <code>bounce buffer</code> 的情况下会降低磁盘的吞吐，不过实际中还没有发现造成可见的性能损耗。</p>
<p>第三，还有些问题发生于主虚拟机正在执行某些磁盘 <code>IO</code>（未完成），然后主虚拟机异常，从虚拟机接管执行。对于从虚拟机来说，它并不知道这些磁盘 <code>IO</code> 是否已发送给磁盘或者已成功执行。另外，由于这些磁盘 <code>IO</code> 还没有在从虚拟机上发起过，所以也不会相应的 <code>IO</code> 完成的通知，但是在操作系统的角度指令已经发出了，但是从虚拟机上的操作系统也收不到 <code>IO</code> 完成的通知，最终会终止或者重置这个过程。在这种情况下，系统会为每一个 <code>IO</code> 操作发送一个失败的通知，即使某个 <code>IO</code> 操作实际成功了而返回失败也是可以接受的。然而，由于从虚拟机上的操作系统可能不能很好的响应 <code>IO</code> 失败的通知，所以在从虚拟机 <code>go live</code> 阶段会重新发起这些 <code>IO</code> 请求。因为系统已经消除了并发 <code>IO</code> 间的竞争，所以这些 <code>IO</code> 操作可以重新发起即使它们之前已经成功执行了（操作是幂等的）。</p>
<h3 id="网络-IO-的实现问题"><a href="#网络-IO-的实现问题" class="headerlink" title="网络 IO 的实现问题"></a>网络 IO 的实现问题</h3><p><code>VMware vSphere</code> 为虚拟机的网络提供了很多的性能优化。部分优化基于 <code>hypervisor</code> 能异步更新虚拟机的网络设备的状态。例如，在虚拟机还在执行时 <code>hypervisor</code> 就可以直接更新接收缓冲区。不过，这种异步更新同样也带来了非确定性。除非能保证在异步更新的时间点在主虚拟机上执行的指令能严格一致的在从虚拟机上重放，否则从虚拟机的状态就会和主虚拟机不一致。</p>
<p><code>FT</code> 中对网络代码改动最大的一块就是禁止了异步网络更新优化。从原先的异步更新缓冲区修改为强制陷入到陷阱（<code>trap</code>）中，<code>hypervisor</code> 响应后将更新记录到日志中，然后再将其应用到虚拟机上。类似的，异步从缓冲区中拉取数据包也同样被禁止了，同样由 <code>hypervisor</code> 托管执行。</p>
<p>消除了异步的网络设备状态更新以及前面所提到的 <code>Output Rule</code> 带来的延迟输出，为网络性能优化又提出了挑战。<code>VMware</code> 通过两个方面来优化网络性能。第一，实现了集群优化来减少虚拟机的陷阱和中断。当虚拟机在接收网络包时，<code>hypervisor</code> 可以在一个陷阱、中断内处理多组数据包来减少陷阱和中断的次数。</p>
<p>第二个网络优化是降低数据包延迟发送的时间。前面提到，只有当从虚拟机确认收到所有输出操作的日志后，主虚拟机才能执行输出操作。所以减少数据包延迟发送的时间等价于减少日志发送和确认的时间。这里主要的优化点是发送和接受消息回执的过程中确保不会触发线程切换。另外，当主虚拟机将某个数据包加入发送队列时，系统会强制刷新主虚拟机的日志缓冲区到 <code>logging channel</code> 中。</p>
<h2 id="其他设计方案"><a href="#其他设计方案" class="headerlink" title="其他设计方案"></a>其他设计方案</h2><p>本节主要描述了 <code>VMware</code> 在实现 <code>VMware FT</code> 时调研和考虑的其他一些方案。</p>
<h3 id="共享磁盘和非共享磁盘"><a href="#共享磁盘和非共享磁盘" class="headerlink" title="共享磁盘和非共享磁盘"></a>共享磁盘和非共享磁盘</h3><p>在当前的实现中，主虚拟机和从虚拟机共享一个存储。因此当发生主从切换时，从虚拟机上的数据天然是和主虚拟机上的数据一致的。共享存储相对于主从虚拟机来说是一个外部系统，所以任何对共享存储的写入都被视为和外部的通信。因此，只有主虚拟机被允许写入到共享存储，而且写入必须遵循 <code>Output Rule</code> 规则。</p>
<p>另一种设计是主虚拟机和从虚拟机各自有一套独立的存储。在这个设计中，从虚拟机会将所有的输出写入到自己的存储中，所以从虚拟机的数据也能和主虚拟机保持同步。下图展示了这种设计下的配置：</p>
<p><img src="/images/vm-ft-4.png" alt="alt"></p>
<p>在非共享磁盘的场景下，每个虚拟机的存储被视为该虚拟机内部状态的一部分。因此，虚拟机的输出没有必要遵循 <code>Output Rule</code> 规则。非共享磁盘的设计在主从虚拟机无法访问共享存储时很有用。这可能时因为共享存储不可用或者过于昂贵，或者主从虚拟机间的物理距离太长。这种方案的一个主要的缺点是主从虚拟机开启容错时必须将双方的磁盘进行初始化同步。另外，双方的磁盘也有可能在异常发生后造成不同步，所以当备份虚拟机重启后，双方磁盘需要再次同步。所以，<code>FT VMotion</code> 不只要同步主从虚拟机的状态，还要同步磁盘的状态。</p>
<p>在非共享磁盘的场景下，系统就不能借助共享存储来解决脑裂问题。在这种场景下，系统可借助其他的外部组件，例如某个主从虚拟机都可以连接的第三方服务器。如果主从服务器属于某个多于两个节点的集群，那么就可以使用某个选举算法来选择谁能进入 <code>go live</code> 模式。在这种场景下，如果某台虚拟机获得了大多数节点的投票，那么它就可以进入 <code>go live</code> 模式。</p>
<h3 id="在从虚拟机上执行磁盘读操作"><a href="#在从虚拟机上执行磁盘读操作" class="headerlink" title="在从虚拟机上执行磁盘读操作"></a>在从虚拟机上执行磁盘读操作</h3><p>在当前的实现中，从虚拟机从来不会从虚拟磁盘中读取数据（不论是共享存储还是非共享存储）。因为磁盘读也是一种输入，所以会自然的将磁盘读取后的结果以日志的形式通过 <code>logging channel</code> 发送给从虚拟机。</p>
<p>另一种设计是涉及到磁盘读操作的执行由从虚拟机自行从磁盘中读取，主虚拟机不再向 <code>logging channel</code> 中发送从磁盘读取到的数据。对于磁盘密集型的系统来说，这种设计可以大幅降低 <code>logging channel</code> 的负载。不过，这种设计也存在一些问题。这可能会降低从虚拟机的执行速度，因为从虚拟机在重放时必须先等待所有依赖的磁盘 <code>IO</code> 操作完成。</p>
<p>另外，当磁盘读失败时还需要额外的处理。如果主虚拟机读取磁盘成功了，而从虚拟机读取磁盘失败了，那么从虚拟机就需要进行重试直到成功，因为从虚拟机必须拿到和主虚拟机内存中一样的数据。相反的，如果主虚拟机读取磁盘失败了，则相应主虚拟机中目标内存中的数据就必须通过 <code>logging channel</code> 发送给从虚拟机，因为此时主虚拟机中内存中的数据是不确定的。</p>
<p>最后，在共享存储的模式下如果让从虚拟机执行磁盘读也有个问题。如果主虚拟机在某个磁盘区域执行了读操作，然后马上又对相同的区域执行了写操作，那么这个写操作就必须等到从虚拟机读取完成后再执行。这个依赖也能够被监测和正确处理，不过也给系统实现增加了额外的复杂度。</p>
<p>在实际性能测试中，从虚拟机执行磁盘读操作的情况下应用程序的吞吐会降低 <code>1-4%</code>，但是也能有效降低 <code>logging channel</code> 的负载。因此，在 <code>logging channel</code> 的带宽有限的情况下，可以考虑使用从虚拟机磁盘读操作。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/papers/vm-ft.pdf">The Design of a Practical System for Fault-Tolerant Virtual Machines</a></li>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/notes/l-vm-ft.txt">6.824 2022 Lecture 4: Primary/Backup Replication</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hardware_performance_counter">Hardware performance counter</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Direct_memory_access">Direct memory access</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Memory_management_unit">Memory management unit</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://frederick-s.github.io/2022/04/19/mit-6.824-gfs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaodan Mao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Übung macht den Meister">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/19/mit-6.824-gfs/" class="post-title-link" itemprop="url">MIT 6.824 - The Google File System</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-19 00:00:00" itemprop="dateCreated datePublished" datetime="2022-04-19T00:00:00+08:00">2022-04-19</time>
            </span>

          
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>23k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>38 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在 <a target="_blank" rel="noopener" href="https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf">MapReduce: Simplified Data Processing on Large Clusters</a> 中提到，<code>MapReduce</code> 任务的输入输出构建在 <code>GFS</code> 之上，<code>GFS</code> 是 <code>Google</code> 内部开发的一个分布式文件系统，用于应对大型的数据密集型应用。在 <code>GFS</code> 之前，业界已经存在了一些分布式文件系统的实现，为什么 <code>Google</code> 还要再实现一套？因为基于 <code>Google</code> 内部应用的特点，有别于传统的分布式文件系统，除了考虑性能、可扩展性、可靠性和可用性之外，<code>GFS</code> 在设计时还考虑了以下三个方面：</p>
<ol>
<li>组件异常经常出现而不是偶尔出现。<code>GFS</code> 构建在成百上千台廉价的机器上，并同时被同等数量的客户端访问。在这个量级规模下，在任何时候某个组件都有可能发生异常以及发生异常后无法自动恢复。这里的异常不只包括硬件的异常，还包括软件的异常以及人为的错误。因此，对于异常的监控和检测，容错，以及异常的自动恢复是系统不可或缺的一个部分。</li>
<li>以传统的分布式文件系统的视角来看，<code>Google</code> 要处理的都是大文件，几个G的文件随处可见。</li>
<li>对于 <code>Google</code> 的数据应用来说，大部分对文件的写操作是追加操作而不是覆盖操作。对文件的随机写几乎可以说是不存在。文件一旦写入完成后，基本上就不会被再次修改，剩下的都是读操作，且大部分场景下是顺序读。<code>MapReduce</code> 系统就是这个应用场景的典型例子，<code>map</code> 任务持续顺序追加生成中间结果文件，<code>reduce</code> 任务不断的从中间结果文件中顺序读取。根据这个特点，追加写就成为了系统性能优化以及写操作原子性保证的主要设计方向。</li>
<li>将应用程序和文件系统的 <code>API</code> 协同设计有利于增加系统的灵活性。例如，<code>GFS</code> 提供了原子性的追加写操作，多个客户端可以并发追加写，而无需在应用程序层面进行加锁。</li>
</ol>
<h2 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h2><h3 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h3><p>本节主要描述了 <code>GFS</code> 设计的几个出发点：</p>
<ul>
<li>整个系统构建在一批廉价且经常出现异常的硬件上，所以设计上必须考虑对异常的监测和容错，以及异常发生后能够恢复到一个合适的程度。</li>
<li>需要能够高效管理几个G的大文件，系统也支持存储小文件，不过不会对其作特殊的优化。</li>
<li>系统需要支持两种文件读取方式：大量的流式读和少量的随机读。在大量的流式读场景下，每次读取大小一般在几百KB，或者1MB或更多。来自同一个客户端的连续读取一般是读取文件的某一段连续内容。而少量的随机读一般是在文件的任意位置读取几KB。对于性能敏感的应用程序来说，一般会将多个随机读根据读取的位置排序后批量读取，避免磁盘来回的随机寻址。</li>
<li>系统需要支持大量连续的文件追加写操作。对文件追加写的单次大小一般和流式读的单次大小差不多。文件一旦写入完成后就几乎不会再被修改。系统同样需要支持少量的随机写，不过和少量的随机读类似，随机写也不强调性能。</li>
<li>当有多个客户端对同一个文件进行追加写的时候，系统必须能高效的实现清晰明确的执行语义，例如是保证每个客户端至少成功追加写一次，还是至多一次或者其他。<code>GFS</code> 中的文件经常会充当某个生产者消费者场景下的缓冲队列，一边会有多个客户端不断往文件中追加写，一边也会有一个客户端同时进行流式读取（或在写入完成后读取），所以系统必须保证追加写操作的原子性。</li>
<li>整个系统的吞吐的重要性大于延迟，大部分 <code>Google</code> 的应用程序主要是大批量的文件处理，只有少量的程序会对单次的读或写有性能要求，所以系统的吞吐是第一位。</li>
</ul>
<h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p>虽然 <code>GFS</code> 并未完全实现标准的文件系统 <code>API</code>，如 <code>POSIX</code>，但仍提供了常见的文件系统接口，如创建（<code>create</code>）、删除（<code>delete</code>）、打开（<code>open</code>）、关闭（<code>close</code>）、读取（<code>read</code>）和写入（<code>write</code>）文件。类似于本地文件系统，文件在 <code>GFS</code> 内通过树状目录的形式组织，每个文件通过文件路径来唯一确定，不过这里的目录是逻辑上的概念，并不会映射到某个物理文件系统目录上。</p>
<p>除此之外，<code>GFS</code> 还支持快照（<code>snapshot</code>）和追加写（<code>record append</code>）的操作。快照能够低成本的复制一个文件或一个目录。追加写允许多个客户端并发的对同一个文件追加写入，并保证每个客户端写入的原子性。</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>一个 <code>GFS</code> 集群由一个主节点（<code>master</code>）和多个 <code>chunkserver</code> 组成，并被多个客户端（<code>client</code>）访问。不管是主节点还是 <code>chunkserver</code> 或客户端，都运行在廉价的 <code>Linux</code> 机器上。可以简单的将 <code>chunkserver</code> 和客户端运行在同一台机器上，如果系统资源允许或者能够容忍客户端代码潜在的不可靠性的话。</p>
<p>每个文件在存入 <code>GFS</code> 时会被切分为固定大小的块（<code>chunk</code>），每个 <code>chunk</code> 在创建时会被主节点分配一个全局不可变的64位 <code>chunk handle</code>。<code>chunkserver</code> 将 <code>chunk</code> 保存在本地文件系统中，每个 <code>chunk</code> 对应着本地的一个 <code>Linux</code> 文件，客户端通过指定 <code>chunk handle</code> 和文件偏移范围来读取或者写入 <code>chunk</code>。为了保证可靠性，每个 <code>chunk</code> 会复制到多台 <code>chunkserver</code> 上。<code>GFS</code> 默认为每个 <code>chunk</code> 生成3份副本，不过用户也可以为某些命名空间下的文件指定不同的副本数量。</p>
<p>主节点保存了全部的系统元数据，包括文件命名空间、访问控制信息、每个文件和对应 <code>chunk</code> 的映射、每个 <code>chunk</code> 所在的位置等。同时主节点还负责 <code>chunk</code> 的租约管理、不再使用的 <code>chunk</code> 的垃圾回收、<code>chunkserver</code> 间的 <code>chunk</code> 迁移等。主节点也会定期的向 <code>chunkserver</code> 发送心跳用于向 <code>chunkserver</code> 发送指令和收集 <code>chunkserver</code> 当前的状态。</p>
<p><code>GFS</code> 的客户端代码会集成到用户应用程序中，它负责实现文件系统 <code>API</code> 以及和主节点、 <code>chunkserver</code> 通信来读取或写入文件。<code>GFS</code> 客户端通过主节点获取文件的元数据信息，然而文件的读取和写入都只和 <code>chunkserver</code> 通信，从而减少了主节点的负担。</p>
<p>不管是 <code>GFS</code> 客户端还是 <code>chunkserver</code> 都不会缓存文件数据。客户端缓存收益不大是因为 <code>Google</code> 大部分的应用程序是对大文件的流式读取或者文件内容过大无法被缓存。不考虑缓存简化了客户端和整个系统的实现，因为引入缓存就要考虑缓存一致性问题。不过客户端会缓存文件的元数据信息，例如某个 <code>chunk</code> 的位置信息。<code>chunkserver</code> 不缓存是因为 <code>chunk</code> 是作为 <code>Linux</code> 文件保存在本地文件系统中，操作系统本身已经提供了一层文件访问缓存，没有必要再加一层。</p>
<h3 id="单主节点"><a href="#单主节点" class="headerlink" title="单主节点"></a>单主节点</h3><p>单主节点同样简化了整个系统的设计，方便主节点高效管理 <code>chunk</code>，因为所有需要的信息都保存在主节点内存中。为了避免对文件的读写使得主节点成为瓶颈，客户端读写文件时直接和 <code>chunkserver</code> 通信而不会经过主节点中转，不过在开始读写前，客户端会先和主节点通信获取需要的 <code>chunk</code> 对应的 <code>chunkserver</code> 列表，并将这个数据缓存一段时间来减少后续和主节点的通信。</p>
<p>结合下面的流程图我们来看一下一次读操作是如何执行的：</p>
<p><img src="/images/gfs-1.png" alt="alt"></p>
<ol>
<li>客户端根据固定的每个 <code>chunk</code> 的大小和想要读取的文件偏移位置，计算出 <code>chunk</code> 的索引。</li>
<li>客户端将文件名和 <code>chunk</code> 的索引发给主节点，主节点收到请求后返回对应的 <code>chunk handle</code> 和保存了该份 <code>chunk</code> 的 <code>chunkserver</code> 列表。</li>
<li>客户端收到主节点返回结果后，以文件名和 <code>chunk</code> 的索引组合作为键，将返回结果缓存。</li>
<li>客户端将 <code>chunk handle</code> 和想要读取的文件内容偏移范围发给其中一台 <code>chunkserver</code>，一般来说，客户端会选择距离最近的 <code>chunkserver</code>，由于缓存了 <code>chunkserver</code> 列表信息，后续对同一个 <code>chunk</code> 的读请求就不需要再次和主节点通信，直到缓存过期或者文件被再次打开。</li>
<li>一般来说，客户端和主节点通信时会在一次请求中要求多个 <code>chunk</code> 的信息，主节点同样也可在一次响应中返回多个 <code>chunk</code> 的信息，这也有利于减少客户端和主节点的通信次数。</li>
</ol>
<h3 id="chunk-大小"><a href="#chunk-大小" class="headerlink" title="chunk 大小"></a>chunk 大小</h3><p>每个 <code>chunk</code> 的大小是一个关键的设计选择。<code>GFS</code> 的 <code>chunk</code> 大小为 <code>64 MB</code>，远大于一般文件系统的数据块的大小。每个 <code>chunk</code> 以 <code>Linux</code> 文件的形式保存在 <code>chunkserver</code> 上，但其实际占用的大小是动态增长的，从而避免了大量的文件碎片（例如实际只需要十几K的文件却固定分配了 <code>64 MB</code> 的大小）。</p>
<p>那么，<code>GFS</code> 为什么要选择 <code>64 MB</code> 这样较大的数呢？主要是因为：</p>
<ol>
<li><code>chunk</code> 的大小越大，每个文件拆分成的 <code>chunk</code> 的数量就越少，客户端需要读取或写入的 <code>chunk</code> 数量也就越少，和主节点通信的次数也就越少。对于 <code>Google</code> 的数据应用来说，大部分都是顺序读写，客户端和主节点交互的次数是线性的时间复杂度，减少 <code>chunk</code> 的总个数有助于降低整个的时间复杂度。即使是对于少数的随机写，由于 <code>chunk</code> 大小足够大，客户端也能够将TB级别的数据对应的所有 <code>chunk</code> 的信息缓存起来。</li>
<li>一个 <code>chunk</code> 的大小越大，那么客户端对其可操作次数就越多，客户端就可以和某个 <code>chunkserver</code> 维持一段较长时间的 <code>TCP</code> 连接，减少和不同的 <code>chunkserver</code> 建立连接的次数。</li>
<li>主节点需要维护每个 <code>chunk</code> 对应的 <code>chunkserver</code> 列表，这个也是个线性的空间复杂度，较大的 <code>chunk</code> 大小能减少 <code>chunk</code> 的总个数，从而减少元数据的大小，主节点就可以将所有的元数据放在内存中。</li>
</ol>
<p>另一方面，即使将 <code>chunk</code> 的大小设置的较大，以及采用了懒分配的策略，也依然存在缺点。对于一个小文件来说，它可能只有几个甚至是一个 <code>chunk</code>，如果这个文件被访问的频率较大，它就有可能成为热点数据，保存了对应的 <code>chunk</code> 的 <code>chunkserver</code> 就要承受更大的流量。不过在 <code>Google</code> 的实际应用场景中，热点数据并没有成为一个大问题，因为大部分的系统面向的是大文件的流式读取，流量能较平均的分发到各个 <code>chunkserver</code>。</p>
<p>不过，<code>Google</code> 确实有遇到过一个热点数据问题，有个批处理系统会先将一个可执行文件写入到 <code>GFS</code> 中，这个可执行文件的大小只有一个 <code>chunk</code>，然后所有客户端会同时读取这个可执行文件并执行，这就造成了几台 <code>chunkserver</code> 同时承接了大量的请求。<code>Google</code> 通过两个方面来解决这个问题，第一针对这个文件设置一个较大的副本数量，让更多的 <code>chunkserver</code> 来分散流量，即水平扩展；第二交替启动批处理系统的客户端，避免同一时间的大量请求。另一种长期的解决方案是在这种场景下允许客户端从其他已读取完毕的客户端那读取文件。</p>
<h3 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h3><p>主节点主要保存了三类元数据：每个文件和 <code>chunk</code> 所属的命名空间、每个文件到对应所有 <code>chunk</code> 的映射、每个 <code>chunk</code> 对应的 <code>chunkserver</code> 列表。所有的元数据都保存在主节点的内存中，前两种元数据被修改时还会以日志的形式保存在本地日志文件中，并备份到其他服务器上。通过日志文件的方式使得主节点能够轻松的修改元数据，并且在主节点崩溃重启后能恢复数据，当然极端情况下主节点有可能没有保存最新的元数据到日志文件中就崩溃了，这里后文会有说明主节点会在持久化完成后才返回结果给客户端，所以客户端不会看到未持久化的元数据。主节点并不会对 <code>chunkserver</code> 列表进行持久化，而是在启动时主动拉取所有 <code>chunkserver</code> 的信息，以及每当一个新的 <code>chunkserver</code> 加入到集群中时，更新元数据，因为 <code>chunkserver</code> 是个动态更新的数据，即使持久化了也需要和当前实时的数据作比对。</p>
<h4 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h4><p>由于元数据保存在主节点内存中，所以涉及主节点的操作都是非常快的。另一方面，主节点也能够轻易的每隔一段时间遍历所有的元数据，这个主要有三个目的，第一是扫描不再需要的 <code>chunk</code> 进行垃圾回收；第二如果某个 <code>chunkserver</code> 失联了，能将其保存的 <code>chunk</code> 备份到其他 <code>chunkserver</code> 上；第三将某些流量失衡或者本地磁盘空间不够的 <code>chunkserver</code> 下的 <code>chunk</code> 转移到其他 <code>chunkserver</code> 下。</p>
<p>将元数据保存在内存中的一个缺陷是受限于主节点的内存容量，能够保存的 <code>chunk</code> 的元数据的个数是有限的，从而整个 <code>GFS</code> 的容量大小是有限的。不过在实践中，这个问题还没有成为瓶颈，每个 <code>chunk</code> 对应的元数据大小约小于64字节，由于 <code>GFS</code> 面向的主要是大文件，所以基本上每个文件也就最后一个 <code>chunk</code> 没有塞满，所以空间浪费较少。类似的，由于使用了前缀压缩，每个文件的命名空间元数据的大小也小于64字节。对于一个 <code>128 GB</code> 内存的主节点来说，假设全部用来保存 <code>chunk</code> 的元数据，则理论上保存的所有 <code>chunk</code> 的大小为 <code>128 GB / 64 B * 64 MB = 128 PB</code>，虽然实际上元数据不只 <code>chunk</code> 一种，但整体上来说也能够维护 <code>PB</code> 级别的数据。</p>
<p>如果需要 <code>GFS</code> 支撑更大的容量，相比于将元数据全部保存在内存中所带来的的简洁、可靠、性能和灵活上的收益，扩展主节点的内存所需要的成本不值一提。</p>
<h4 id="chunk-的保存位置"><a href="#chunk-的保存位置" class="headerlink" title="chunk 的保存位置"></a><code>chunk</code> 的保存位置</h4><p>主节点不会持久化每个 <code>chunk</code> 对应的 <code>chunkserver</code> 列表，它会在启动时主动拉取所有的 <code>chunkserver</code> 信息。在主节点运行后，它可以通过和 <code>chunkserver</code> 的心跳来实时更新 <code>chunkserver</code> 的状态信息。</p>
<p>在最初的设计中，<code>Google</code> 的工程师试过将 <code>chunk</code> 和 <code>chunkserver</code> 的映射关系进行持久化，但是最终还是采用了更简洁的设计，即在主节点启动时主动拉取所有的 <code>chunkserver</code> 信息并在之后定期更新内存中的数据。这就消除了频繁的数据同步，毕竟在集群环境下，<code>chunkserver</code> 加入或者离开集群，修改主机名，发生异常或重启等操作会经常发生。</p>
<p>另一方面，只有 <code>chunkserver</code> 才真正知道自己是否拥有某个 <code>chunk</code>，所以在主节点上持久化一份 <code>chunk</code> 和 <code>chunkserver</code> 的映射关系意义不大，因为这份数据很大概率是不一致的，毕竟 <code>chunkserver</code> 会发生异常然后失联或者运维重命名了一台 <code>chunkserver</code>。</p>
<h4 id="操作日志"><a href="#操作日志" class="headerlink" title="操作日志"></a>操作日志</h4><p>操作日志记录了元数据的重要历史更新，它是 <code>GFS</code> 的关键。不仅因为它是元数据的唯一持久化备份，同时它也提供了系统在并发操作下各操作的执行顺序。各个版本的文件和其对应的 <code>chunk</code> 的操作记录都被唯一的标识在日志文件中。</p>
<p>所以操作日志必须保证可靠性存储，以及在元数据持久化完成之前不能将最新的元数据更新暴露给客户端。否则就有可能丢失最新的客户端操作，即使 <code>chunkserver</code> 还存活着。因此，<code>GFS</code> 会将操作日志备份在多台机器上，并且只有在所有的副本机器都持久化完成后才会返回结果给客户端。同时主节点会对操作日志进行批量持久化以降低频繁持久化对系统整体吞吐的影响。</p>
<p>当主节点崩溃重启后会通过重放操作日志来恢复崩溃前的状态，然而如果每次都从第一条日志开始重放，主节点崩溃重启到可用需要的时间会越来越久，因此当操作日志的大小增长到一定程度的时候，主节点会为当前的元数据创建一个检查点，当主节点崩溃恢复后，可以先加载最新的检查点数据，然后再重放在这个检查点之后生成的操作日志。检查点是一个类似于 <code>B</code> 树的数据结构，可以轻易的映射到内存数据结构中，并且方便根据文件的命名空间检索。这就加快了主节点崩溃恢复的速度和提高了系统的可用性。</p>
<p>因为构建检查点需要时间，所以在创建检查点时需要确保不影响正在进行的文件修改。主节点会先创建一个新的操作日志文件，然后由一个新的线程去创建检查点，这个线程会依据新的操作日志文件创建前的日志生成检查点。对于一个百万级别数量文件的系统来说，大概需要1分钟的时间创建检查点，检查点创建完成后同样会持久化到本地磁盘和副本机器上。</p>
<p>主节点崩溃恢复只需要最新完整的检查点文件以及后续的操作日志文件，在这之前的检查点文件和操作日志文件都可以删除，不过为了提高容错性还是会保留一部分文件。如果生成检查点的时候发生异常并不会影响崩溃恢复的正确性，因为恢复的代码会校验检查点文件的完整性并跳过损坏的检查点文件。</p>
<h3 id="一致性模型"><a href="#一致性模型" class="headerlink" title="一致性模型"></a>一致性模型</h3><p><code>GFS</code> 提供了弱一致性模型，在能较好的支撑 <code>Google</code> 内部各分布式应用的同时也兼顾了简洁和高效的实现。</p>
<h4 id="GFS-的保证"><a href="#GFS-的保证" class="headerlink" title="GFS 的保证"></a><code>GFS</code> 的保证</h4><p>文件命名空间的修改是原子的（例如创建一个文件）。对命名空间的修改会由主节点加锁执行，这就保证了修改的原子性和正确性，同时主节点的操作日志记录了这些修改的全局顺序。</p>
<p>而某块文件区域修改后的状态则取决于修改的类型，是否成功或失败，以及是否存在并发的修改。某块文件区域是一致的（<code>consistent</code>）表示不管客户端从哪个 <code>chunkserver</code> 读取这块的数据，每个客户端看到的内容始终是相同的。某块文件区域是已定义的（<code>defined</code>）表示经过一次修改后，首先这块文件区域是一致的，并且客户端读到的数据就是自己修改的。如果对某块文件区域的修改不受其他并发请求的客户端的影响，那么这块文件区域在修改成功后是已定义的（也是一致的），所有的客户端都能看到对应的修改。而如果有多个客户端并发的对同一块文件区域修改，那么最终的结果是未定义的，但是是一致的，所有的客户端读到的数据都是相同的，但是并不知道读到的数据是哪个客户端修改的，一般来说，最后这块文件区域的内容很可能是多个客户端并发修改后混合的结果。一次失败的修改会使得这块文件区域不一致（因此也是未定义的），不同的客户端在不同时间读取到的数据是不同的，因为多个 <code>chunkserver</code> 上保存的数据不一致。</p>
<p>对文件的修改包括在随机位置的写和在文件末尾的追加写两种，随机写指的是数据会写入到应用程序指定的某个文件偏移的位置，追加写指的是在并发的情况下能保证数据会原子性的至少一次追加写入到文件末尾，不过最后数据写入的实际偏移位置是由 <code>GFS</code> 来决定（相反的，一次常规的追加写指的是在客户端以为是文件末尾的地方写入数据。）。追加写完成后，系统会返回相应的偏移量给客户端，这个偏移量表示了某块已定义的文件区域的开始，对于当前客户端来说，这块区域的数据就是自己写入的，对于其他客户端来说，它们读到的数据也始终是相同的。不过，<code>GFS</code> 有可能在修改 <code>chunk</code> 时插入对齐数据或者重复的数据，例如现在有个追加写操作，<code>chunkserver1</code> 需要写入的偏移量位置是100，并且写入完成了，<code>chunkserver2</code> 需要写入的偏移量位置也是100，但是 <code>chunkserver2</code> 写入失败了，客户端会进行重试，那么 <code>chunkserver1</code> 会在偏移量101的地方再次写入相同的数据，但是由于之前 <code>chunkserver2</code> 没有写入成功，它的偏移量位置还是100，那么为了保证所有客户端根据偏移量读取到的数据是相同的，就需要先补齐 <code>chunkserver2</code> 偏移量位置100的数据，然后在偏移量位置101写入新的数据，如果这次所有 <code>chunkserver</code> 都写入成功了，那么就会把偏移量101返回给客户端，而不是100。所以我们看到，客户端想要保存的原始文件，和最终从 <code>GFS</code> 取回的文件数据并不是每个比特都一致的，存在某些文件区域数据不一致的情况。</p>
<p>当一系列文件修改执行完成后，被修改的文件区域保证了是已定义的，并且包含了最后一次修改的内容。<code>GFS</code> 通过两个方面来保证这一点：第一，对 <code>chunk</code> 的所有修改在所有 <code>chunkserver</code> 上的执行顺序是一样的；第二，通过 <code>chunk</code> 的版本号来识别所有版本落后的 <code>chunkserver</code>，这些 <code>chunkserver</code> 有可能是因为在失联期间 <code>chunk</code> 内容发生了修改。对于版本号落后的 <code>chunkserver</code> 则不会参与 <code>chunk</code> 的修改并且主节点也不会将其作为有效的 <code>chunkserver</code> 返回给客户端，这些 <code>chunkserver</code> 所持有的 <code>chunk</code> 会尽早的被垃圾回收。</p>
<p>然而在前面提到，客户端从主节点获取某个 <code>chunk</code> 的 <code>chunkserver</code> 列表后，会在一段时间内将其缓存，如果这段时间内某个 <code>chunkserver</code> 包含的 <code>chunk</code> 的版本号落后了，那么客户端读到的数据会存在短暂的不一致，这个时间段就取决于客户端缓存的过期时间，以及客户端下次打开文件的时间，因为重新打开文件时客户端会从主节点重新拉取 <code>chunkserver</code> 的信息从而刷新了缓存。然而，鉴于大部分文件的修改是追加写，这里我理解可能在流式读的场景下，读完 <code>chunk</code> 的某段内容后会返回新的偏移量，而对于版本号落后的 <code>chunkserver</code> 来说，它返回的偏移量可能已经读过了，所以此时客户端知道这个 <code>chunkserver</code> 可能过期了，所以会尝试再次和主节点通信，从而获取新的 <code>chunkserver</code> 列表。</p>
<p>然而即使对文件的修改执行成功了，系统其他组件如硬件的异常也有可能导致文件损坏或摧毁。<code>GFS</code> 通过主节点和 <code>chunkserver</code> 的心跳来监测异常的机器，并通过校验和来验证 <code>chunk</code> 的数据是否损坏。如果出现了数据损坏或者某台 <code>chunkserver</code> 失联了，那么 <code>GFS</code> 会从其他有效的 <code>chunkserver</code> 那重新复制数据，来保证副本数量维持在指定的阈值上。所以，只有当某个 <code>chunk</code> 对应的 <code>chunkserver</code> 全都异常了，同时 <code>GFS</code> 还没来得及反应过来（一般来说是几分钟内）复制数据，<code>chunk</code> 才会发生永久丢失。即使在这种情况下，对于应用程序来说它看到的是不可用的状态，而不是数据损坏：应用程序收到的是明确的错误信息而不是损坏的数据。</p>
<p>最后，来看一下每种操作下的一致性保证：</p>
<ul>
<li>随机写<ul>
<li>串行执行成功：已定义（<code>defined</code>），在串行执行场景下，某个时间只会有一个客户端执行写入，客户端在执行成功后读取到的就是自己刚刚写入的，所以是已定义的。</li>
<li>并行执行成功：一致但是未定义（<code>consistent but undefined</code>），某个时间在并发场景下多个客户端写入的偏移范围可能重合，最后文件区域的数据可能是多个客户端写入混合后的产物，每个客户端写入后读取到的不一定是自己写入的，所以是未定义的，但由于写入成功，所以是一致的。</li>
<li>异常：不一致（<code>inconsistent</code>），写入失败时，有的 <code>chunkserver</code> 可能写入成功，有的未成功，不同客户端读取同一个偏移量的数据就会不一致。</li>
</ul>
</li>
<li>追加写<ul>
<li>串行执行成功：已定义但是穿插着不一致（<code>defined, interspersed with inconsistent</code>），由于追加写至少写入一次的保证，一次追加写的成功背后可能包含着重试，所以在某些偏移量下各个 <code>chunkserver</code> 上的 <code>chunk</code> 的数据会不一致，但是追加写返回的是一个写入成功的偏移量，不会把不一致的偏移量返回，这个新的偏移量下的数据就是客户端自己写入的，且每个客户端读取这个偏移量的内容都是相同的，所以是已定义的。</li>
<li>并行执行成功：同上，追加写返回的偏移量是由 <code>GFS</code> 决定的，所以多个并发客户端追加写入不会相互覆盖。</li>
<li>异常：不一致（<code>inconsistent</code>），同随机写。</li>
</ul>
</li>
</ul>
<h4 id="对应用程序的影响"><a href="#对应用程序的影响" class="headerlink" title="对应用程序的影响"></a>对应用程序的影响</h4><p>借助追加写、检查点、可自我校验和识别的数据写入，<code>GFS</code> 的应用可以较好的适应 <code>GFS</code> 的弱一致性模型。</p>
<p>在实际应用场景中，<code>GFS</code> 的应用基本是通过追加写而不是覆盖写来修改文件。在某个典型应用中，某个文件的内容完全由某个生产者写入，在写入完成后能原子性的将其重命名，或者周期性的为至今写入成功的数据建立检查点。检查点同时也可包含应用程序级别的校验和（不同于 <code>GFS</code> 的校验和，应用程序保存的数据对于 <code>GFS</code> 来说是一致的，但对于应用来说可能是不完整的）。对于读文件的消费者来说，它们只需要读取到最新的检查点即可，因为到最新的检查点的数据可以认为是已定义的（<code>defined</code>），并可以通过校验和来验证文件数据的完整性。如上节所属，虽然追加写在失败时也会存在不一致的情况，但结合检查点的方式已经能较好的满足实际的业务场景。另外，追加写也远比随机写高效和健壮。借助检查点生产者可以在崩溃恢复后从检查点开始继续写入，消费者只根据检查点来读取数据，而不会越过检查点读到以应用的视角来说不完整的数据。</p>
<p>在另一个例子中，多个生产者并发的向某个文件进行追加写，追加写至少写入成功一次的保证使得每个生产者的写入都得以保留。不过，消费者在读取文件时就需要额外处理对齐数据和重复数据，每一条追加写可以看做一条记录，生产者在写入时可以附加一个校验和用于消费者校验数据的完整性，对于对齐数据或不完整的数据来说，它们必然没有校验和，因此消费者就可以跳过这些异常记录。另外，每一条记录都有一个唯一的标识符，重复的记录有着相同的标识符，如果消费者不能容忍重复的数据（例如消费者的代码没有做到幂等），则可以通过这个标识符来对数据去重。除了去重以外，上述的功能都已包含在客户端库中，无需应用端自行实现，从而使得在应用层面每一个消费者都能读到同一份数据（当然，可能会有一点重复数据）。</p>
<h2 id="系统交互"><a href="#系统交互" class="headerlink" title="系统交互"></a>系统交互</h2><p>在单主节点的设计下，<code>GFS</code> 尽可能的让主节点少参与到数据交互中，本节主要描述了客户端、主节点和 <code>chunkserver</code> 在数据修改、原子性的追加写和快照中的实现。</p>
<h3 id="租约和修改的执行顺序"><a href="#租约和修改的执行顺序" class="headerlink" title="租约和修改的执行顺序"></a>租约和修改的执行顺序</h3><p>一次修改（<code>mutation</code>）指的是修改了某个 <code>chunk</code> 的元数据或者内容。每个修改都会应用到该 <code>chunk</code> 的所有 <code>chunkserver</code> 上。在修改前，主节点会挑选某个 <code>chunkserver</code> 作为 <code>primary</code>，为其分配一个租约（<code>lease</code>）。对某个 <code>chunk</code> 的修改可能同时会有多个，所以 <code>primary</code> 会对所有的修改安排一个执行顺序，所有其他的 <code>chunkserver</code> 都会按照这个执行顺序应用修改。因此，从全局来看，对同一个 <code>chunk</code> 的修改的顺序首先取决于主节点分配租约的顺序，在某个租约有效期内的执行顺序，则取决于 <code>primary</code> 安排的执行顺序。</p>
<p>租约的设计是为了减少主节点协调的负担。一个租约初始设置了60秒的过期时间，不过如果在过期前该 <code>chunk</code> 还未完成修改，<code>primary</code> 可向主节点申请租约续期。这个申请可以附带在 <code>chunkserver</code> 和主节点的心跳监测中，而无需额外通信。不过有时候主节点可能会在租约有效期内回收租约（例如，某个文件正在重命名时主节点需要暂停所有的修改）。即使主节点和当前的 <code>primary</code> 失联了，主节点依然可以在该租约过期后安全的重新分配一个租约给新的 <code>primary</code>。</p>
<p>同时，租约也避免了脑裂的问题，如果没有租约的时间限制，主节点先指定了一个 <code>primary</code>，然而由于网络原因主节点认为这个 <code>primary</code> 失联了，就会重新分配一个 <code>primary</code>，此时就有两个 <code>chunkserver</code> 认为自己是 <code>primary</code>。而在租约过期前，主节点不会分配新的 <code>primary</code> 就保证了同一时间只有一个 <code>primary</code>。</p>
<p>下图描述了一次随机写的执行流程：</p>
<p><img src="/images/gfs-2.png" alt="alt"></p>
<ol>
<li>客户端询问主节点当前持有租约的 <code>primary</code> 和其他的 <code>chunkserver</code> 的地址，如果当前没有一台 <code>chunkserver</code> 持有租约，则主节点会挑选一个 <code>primary</code> 并分配租约。</li>
<li>主节点返回当前的 <code>primary</code> 和其他的 <code>chunkserver</code>（<code>secondary</code>） 的地址。客户端收到响应后将其缓存供后续修改时使用，只有当 <code>primary</code> 无响应或者 <code>primary</code> 回复不再持有租约时，客户端才会再次询问主节点 <code>primary</code> 的信息。</li>
<li>客户端将需要写入的数据发送给所有的 <code>chunkserver</code>，这一步没有执行顺序的要求。每个 <code>chunkserver</code> 收到数据后会将其暂存在一个 <code>LRU</code> 缓存中直到该数据被取出或过期。从控制流角度来说，是由 <code>primary</code> 向 <code>secondary</code> 发送指令，而发送待写入的数据不关心谁是 <code>primary</code>，下文会说到这里 <code>GFS</code> 将数据流和控制流进行解耦，基于网络的拓扑结构来实现数据的最优传输。</li>
<li>当所有 <code>chunkserver</code> 都收到了客户端的数据后，客户端会向 <code>primary</code> 发送一个写入请求，这个请求中标记了之前发送的那份数据。<code>primary</code> 将该时段收到的所有修改请求设置一个串行的执行顺序，并对每一个修改分配一个执行编号，然后将修改写入到本地的 <code>chunk</code> 中。</li>
<li><code>primary</code> 将写请求分发给其他所有的 <code>secondary</code>，<code>secondary</code> 收到请求后根据执行编号同样将修改按照和 <code>primary</code> 一样的执行顺序写入到自己的 <code>chunk</code> 中。</li>
<li>每个 <code>secondary</code> 写入完成后会回复 <code>primary</code> 告知写入完成。</li>
<li>当 <code>primary</code> 收到所有 <code>secondary</code> 的回复后，再回复给客户端告知写入成功。如果途中任何一个 <code>chunkserver</code> 出现异常都会通知客户端，当发生异常时，可能 <code>primary</code> 已经写入成功了，但是某个 <code>secondary</code> 只写入成功一部分（如果是 <code>primary</code> 写入异常，就不会分发写请求给其他所有的 <code>secondary</code>）。因此此次客户端请求就认为写入失败，多个 <code>chunkserver</code> 间出现了数据不一致，此时客户端会进行重试，它会尝试重新执行第3步到第7步，如果尝试几次后依然失败，则会重新从第1步开始执行。</li>
</ol>
<p>如果客户端一次随机写的数据量较大或者当前 <code>chunk</code> 的剩余空间无法容纳全部的数据则需要跨 <code>chunk</code> 写入，<code>GFS</code> 的客户端库会将一次写请求拆分为多个写请求处理。每个单独的写请求都遵循上述的流程执行，不过由于存在并发客户端写的问题，最终写入的数据有可能多个客户端间相互覆盖，不过由于每个 <code>secondary</code> 都按照相同的执行顺序写入成功，最终各个 <code>chunkserver</code> 上的数据都是相同的，这个也就是之前描述的一致但是未定义状态（<code>consistent but undefined</code>）。</p>
<h3 id="数据流"><a href="#数据流" class="headerlink" title="数据流"></a>数据流</h3><p>在前面提到，在第3步发送数据时，<code>GFS</code> 解耦了控制流和数据流以更有效率的利用网络。在控制流下，请求从客户端发送到 <code>primary</code>，再由 <code>primary</code> 分发给其他所有的 <code>secondary</code>，在数据流下，<code>GFS</code> 会选择最优的路径以接力的方式在各个 <code>chunkserver</code> 间传递数据。目的是为了充分的利用每台机器的网络带宽，同时避免网络瓶颈和高延迟的链路，从而降低推送全部数据的整体延迟。</p>
<p>为了尽可能的充分利用每台机器的网络带宽，<code>GFS</code> 选择了线性接力的方式在各个 <code>chunkserver</code> 间传递数据，而不是其他的分发方式（如树状分发，例如以一台机器发送给3台机器为例，发送的机器的出口带宽就被3台机器共享）。</p>
<p>为了尽可能的避免网络瓶颈和高延迟的链路，每个机器会选择在网络结构下距离最近且还未收到数据的机器传递数据。假设现在客户端需要推送数据给 <code>S1</code> 到 <code>S4</code> 四台机器，客户端先将数据发送给距离最近的机器，比如说是 <code>S1</code>，<code>S1</code> 同样将数据转发给距离 <code>S1</code> 最近的机器，比如说是 <code>S2</code>，依此类推，最后数据到达 <code>S4</code>。<code>Google</code> 内部的网络拓扑结构足够简单使得两台机器间的距离能够较准确的根据 <code>IP</code> 地址估算出来。</p>
<p>最后，<code>GFS</code> 在 <code>TCP</code> 连接上构建了一条流式接力传递的数据流来降低数据传递的延迟。每个 <code>chunkserver</code> 一旦收到一定大小的数据后就马上将其转发给距离最近的其他 <code>chunkserver</code>。收到数据就马上转发并不会降低收数据的速率。在没有网络拥堵的情况下，假设 <code>T</code> 是当前网络的吞吐，<code>L</code> 是两台机器间的传输速度，那么要传输 <code>B</code> 字节给 <code>R</code> 个 <code>chunkserver</code> 需要的时间为 <code>B / T + RL</code>，其中 <code>B / T</code> 是完整传输 <code>B</code> 个字节给第一台机器的时间，<code>RL</code> 表示第一台机器发送的第一个字节经过接力到达最后一台机器所需要的时间。在 <code>Google</code> 的网络环境下，<code>T</code> 为 <code>100 Mbps</code>，<code>L</code> 远小于 <code>1 ms</code>，所以 <code>RL</code> 的时间基本可以忽略，因此传输 <code>1 MB</code> 的数据理论上约需要 <code>80 ms</code>（(1 * 1024 * 1024 * 8 / 100 * 1000000) * 1000 ms）。</p>
<h3 id="原子性追加写"><a href="#原子性追加写" class="headerlink" title="原子性追加写"></a>原子性追加写</h3><p><code>GFS</code> 提供了原子性的追加写操作（<code>record append</code>）。在传统的随机写操作下，客户端会指定要写入的文件偏移位置，在多个客户端并发写的情况下，最终该文件区域的内容为多个客户端并发修改后混合的结果。而在追加写场景下，客户端不指定偏移位置，只提供需要写入的数据，<code>GFS</code> 能够原子性的至少一次将其追加到文件中，并指定一个数据写入后的偏移位置，然后将其返回给客户端。这个操作类似于在 <code>Unix</code> 下多个进程以 <code>O_APPEND</code> 的模式并发写入文件而无需担心竞争问题。</p>
<p>追加写在 <code>Google</code> 的分布式应用中使用的非常频繁，多个客户端会并发的对同一个文件追加写入。在传统的随机写操作下，客户端需要额外的同步机制例如实现一个分布式锁来保证写入的线程安全性。在 <code>Google</code> 的应用场景下，文件一般是作为一个多生产者/单消费者的缓冲队列或者包含了多客户端合并后的结果，所以追加写已经能满足需求。</p>
<p>追加写也是一种修改，它的执行流程和随机写基本是相同的，不过对于 <code>primary</code> 来说多了些额外的处理逻辑。首先客户端将需要追加写的数据发送给对应文件的最后一个 <code>chunk</code> 的所有 <code>chunkserver</code>，然后发送写请求给 <code>primary</code>。<code>primary</code> 会检查当次写入是否会造成最后 <code>chunk</code> 的大小超过 <code>64 MB</code>。如果会超过，那么 <code>primary</code> 会先将当前 <code>chunk</code> 使用特殊字符或其他手段填充到 <code>64 MB</code>，同时告知所有其他的 <code>secondary</code> 也进行同样的操作，接着回复客户端要求针对下一个新的 <code>chunk</code> 发送同样的追加写请求（追加写要求追加写入的偏移位置不超过 <code>chunk</code> 最大大小的四分之一处，从而使得由于数据填充带来的 <code>chunk</code> 碎片在一个可接受的水平。）。如果追加写入不会造成超过 <code>chunk</code> 的最大大小，那么 <code>primary</code> 会先将数据追加到自己的 <code>chunk</code> 中，产生一个新的偏移量，然后再通知其他所有的 <code>secondary</code>，将数据追加到相同的偏移量上，最后全部 <code>chunkserver</code> 执行成功后再将新的偏移量返回给客户端。</p>
<p>如果任何一个 <code>chunkserver</code> 追加写失败了，客户端会发起重试。因此，多个 <code>chunkserver</code> 下同一个 <code>chunk</code> 的数据可能会有数据不一致的情况，某条记录有可能全部或者部分重复。<code>GFS</code> 并不保证每个 <code>chunkserver</code> 上的 <code>chunk</code> 的每个字节都一样。它只保证了每个追加写能原子性的至少写入一次，因为一次追加写执行成功依赖于所有的 <code>chunkserver</code> 成功写入到指定的偏移位置上。此外，在一次追加写执行成功后，后续的追加写必然是写在一个更高的文件偏移位置或者新的 <code>chunk</code> 上，即使主节点挑选了一个新的 <code>primary</code>。在 <code>GFS</code> 的一致性模型下，成功执行了追加写所在的偏移区域的数据是已定义的（也是一致的），而剩下存在写失败的偏移区域的数据是不一致的，前文有提到过这些不一致的区域可以通过校验和来剔除。</p>
<h3 id="快照"><a href="#快照" class="headerlink" title="快照"></a>快照</h3><p>快照能够几乎瞬时的复制一个文件或者文件夹，而尽可能的降低对进行中的修改的影响。在实际应用中，一般使用快照对某个巨大的数据集快速创建一个分支（以及分支的分支），或者为当前的文件状态创建一个检查点，方便对当前的文件进行验证修改，在修改完成后提交或回滚。</p>
<p>类似于 <code>AFS</code>，<code>GFS</code> 也采用了写时复制（<code>copy-on-write</code>）的技术来实现快照。当主节点收到某个文件的快照请求时，会首先回收所有该文件的 <code>chunk</code> 所关联的租约，这就保证了后续所有对这些 <code>chunk</code> 的修改都需要先申请租约。这就给了主节点有复制一份 <code>chunk</code> 的机会。</p>
<p>当所有涉及的租约被回收或者过期后，主节点首先将快照操作写入操作日志，然后修改内存元数据，复制一份新的目标文件或文件夹的元数据，这个新复制的元数据和源文件的元数据都指向相同的 <code>chunk</code>。</p>
<p>在快照生成后，假设此时有一个客户端希望写入 <code>chunk C</code>，它会先询问主节点返回当前持有租约的 <code>primary</code>，主节点发现要修改的 <code>chunk C</code> 有多于1个的元数据引用，说明这个 <code>chunk</code> 发生了快照。主节点首先会为这个文件创建一个新的 <code>chunk C&#39;</code>，然后要求所有当前持有 <code>chunk C</code> 的 <code>chunkserver</code> 在本地创建一个文件作为 <code>chunk C&#39;</code>。因为创建 <code>chunk C&#39;</code> 的机器和持有 <code>chunk C</code> 的机器是同一台机器，所以创建 <code>chunk C&#39;</code> 只是单纯的复制 <code>chunk C</code>，不涉及任何网络 <code>IO</code>（<code>Google</code> 使用的磁盘 <code>IO</code> 速度大概是3倍于 <code>100 MB</code> 的网络 <code>IO</code> 速度）。接下来的操作就和前面描述的客户端申请写入某个 <code>chunk</code> 一样了，主节点返回 <code>chunk C&#39;</code> 的 <code>chunk handle</code> 和对应持有租约的 <code>primary</code> 给客户端，客户端并不会感知这个返回的 <code>chunk</code> 是从某个已有的 <code>chunk</code> 复制而来的。</p>
<h2 id="主节点操作"><a href="#主节点操作" class="headerlink" title="主节点操作"></a>主节点操作</h2><p>所有涉及命名空间的操作都由主节点执行。另外，主节点还负责 <code>chunk</code> 的管理：每个 <code>chunk</code> 应该放在哪台服务器上、创建新的 <code>chunk</code> 及其副本、协调各 <code>chunkserver</code> 保证每个 <code>chunk</code> 的副本数量满足阈值、保证 <code>chunkserver</code> 访问的负载均衡、回收不再使用的 <code>chunk</code> 等等。</p>
<h3 id="命名空间管理和锁"><a href="#命名空间管理和锁" class="headerlink" title="命名空间管理和锁"></a>命名空间管理和锁</h3><p>很多主节点的操作需要较长的时间完成，例如，一次快照操作就需要先回收所有 <code>chunk</code> 涉及的租约。同时，又需要降低不同的主节点操作间的影响。因此，<code>GFS</code> 允许不同的主节点操作并发执行，并通过命名空间加锁来保证线程安全。</p>
<p>和传统的文件系统不同，<code>GFS</code> 的文件夹不支持列出该文件夹内的所有文件，以及不支持文件或文件夹的别名（例如 <code>Unix</code> 的硬链接和软链接）。<code>GFS</code> 通过命名空间作为文件到元数据的映射索引，借助前缀压缩使得整个命名空间映射可以高效的保存在内存中。命名空间下的每个节点（代表一个文件名或者文件夹名）都持有一把读写锁。</p>
<p>主节点每次执行操作前都会先获取一批锁，例如如果当前操作涉及命名空间 <code>/d1/d2/.../dn/leaf</code>，主节点会先获取文件夹 <code>/d1</code>，<code>/d1/d2</code>，…，<code>/d1/d2/.../dn</code> 的读锁，以及 <code>/d1/d2/.../dn/leaf</code> 的读锁或写锁。这里的 <code>leaf</code> 对应的是某个文件或文件夹。</p>
<p>下面来举例说明 <code>GFS</code> 的锁机制如何保证当创建 <code>/home/user</code> 的快照到 <code>/save/user</code> 时避免同时创建文件 /<code>home/user/foo</code>。首先快照操作会获取 <code>/home</code> 和 <code>/save</code> 的读锁，以及 <code>home/user</code> 和 <code>/save/user</code> 的写锁。而文件创建的操作会获取 <code>/home</code> 和 <code>/home/user</code> 的读锁，以及 <code>/home/user/foo</code> 的写锁。可以看到，一个获取 <code>home/user</code> 的写锁，另一个获取 <code>home/user</code> 的读锁，所以这两个操作是互斥的，不会同时发生。写文件并不需要获取文件的父目录的写锁，因为文件夹对于 <code>GFS</code> 来说是个逻辑上的概念，背后没有类似 <code>inode</code> 的数据结构需要共享保护。获取文件夹的读锁已经足够保证不会有另一个客户端能够获取文件夹的写锁从而删除了这个文件夹。</p>
<p>这种锁机制的一个优点是允许对同一个文件夹下的不同文件进行并发修改。例如，可以在同一个文件夹下并发创建不同的文件，每一个创建操作都会获取该文件夹的读锁以及所要创建的文件的写锁。文件夹的读锁能有效避免这个文件夹被删除、重命名或者创建快照。文件的写锁则避免了同一个文件被创建两次。</p>
<p>因为整个命名空间会包含很多的节点，所以每个节点的读写锁采用了懒汉初始化的方式，并且当不需要的时候能及时删除。另外，锁的获取会按照命名空间的层级顺序以及同一层级下节点名称的字母顺序来获取，从而避免死锁。</p>
<h3 id="副本的放置"><a href="#副本的放置" class="headerlink" title="副本的放置"></a>副本的放置</h3><p>一个 <code>GFS</code> 集群一般由多个机柜上的几百台机器组成，这些 <code>chunkserver</code> 又会被几百台在同一个或不同机柜上的客户端访问，如果是两台不同机柜上的机器间的通信可能会经过1个或多个网络交换机。另外不同机柜间的入口和出口带宽一般会小于同机柜内的机器间的带宽。这种多级的分布式对分发数据的扩展性，可靠性和可用性提出了挑战。</p>
<p>所以副本的放置选择策略主要出于两个目的：最大化数据的可靠性和可用性，最大化利用网络带宽。为了实现这两点，仅仅将所有的副本分发到不同的机器上还不够，因为这只对磁盘或者机器异常进行了容错，以及最大化利用了每台机器的带宽，但是还缺少可用性的考虑。因此，需要将副本分发到不同机柜的不同机器上，这样如果当某台机柜上的机器都因故下线了（例如一些共享资源如网络交换机或者供电设备发生异常），系统仍然是可用的。所以在跨机柜的访问下，某个 <code>chunk</code> 的读操作会充分利用不同机柜间的带宽，另一方面 <code>chunk</code> 的写操作就需要跨机柜访问，这也是跨机柜存储的权衡利弊的一个体现。</p>
<h3 id="副本的创建、重复制和负载均衡"><a href="#副本的创建、重复制和负载均衡" class="headerlink" title="副本的创建、重复制和负载均衡"></a>副本的创建、重复制和负载均衡</h3><p><code>chunk</code> 的副本的创建出于三个目的：创建一个新的 <code>chunk</code> 时需要保证存储的可靠性，保证每个 <code>chunk</code> 的副本数量满足阈值，保证 <code>chunkserver</code> 访问的负载是均衡的。</p>
<p>当主节点创建一个 <code>chunk</code> 时，它会决定在哪个 <code>chunkserver</code> 上创建空的 <code>chunk</code>，它主要基于以下几点考虑：</p>
<ol>
<li>选择磁盘空间利用率低于平均值的 <code>chunkserver</code>，这保证了长期来看每台 <code>chunkserver</code> 的磁盘空间利用率维持在一个相近的水平。</li>
<li>避免每台 <code>chunkserver</code> 存在过多最近创建的 <code>chunk</code>。虽然创建一个 <code>chunk</code> 比较简单，但是可以预见的是随之而来这个 <code>chunk</code> 会面临大量的写入。</li>
<li>如前面所述，每个 <code>chunk</code> 的副本需要跨机柜存储。</li>
</ol>
<p>当某个 <code>chunk</code> 的副本数量低于用户指定的阈值时，主节点会复制一个 <code>chunk</code> 到某台 <code>chunkserver</code> 上。这种情况一般有以下几种原因：</p>
<ol>
<li>某台 <code>chunkserver</code> 异常失联了。</li>
<li><code>chunkserver</code> 反馈其保存的 <code>chunk</code> 数据已损坏。</li>
<li><code>chunkserver</code> 的某个磁盘由于异常被禁用了。</li>
<li>用户要求增加副本的数量。</li>
</ol>
<p>主节点在复制一个 <code>chunk</code> 时会按照优先级处理。其中一个是距离满足指定的副本数量阈值还差多少。例如如果某个 <code>chunk</code> 还差两个副本就比只差一个副本的 <code>chunk</code> 有着更高的复制优先级。另外，复制还在使用的 <code>chunk</code> 的优先级高于复制最近被删除的 <code>chunk</code>。最后，为了避免缺少副本带给应用的影响，主节点会优先处理那些阻塞了客户端执行的 <code>chunk</code>。</p>
<p>主节点根据优先级决定复制哪个 <code>chunk</code> 之后，会选择几台 <code>chunkserver</code> 让其直接从拥有完整副本的 <code>chunkserver</code> 上复制数据。这种情况下新的 <code>chunk</code> 副本的存放位置的选择和前文所述类似：尽量保证每台 <code>chunkserver</code> 的磁盘空间利用率维持在一个相近的水平，避免副本的复制集中在一台 <code>chunkserver</code> 上，尽量保证副本跨机柜存储。为了避免副本复制的流量压垮客户端的流量，主节点会限制整个集群以及每台 <code>chunkserver</code> 同一时间执行的副本复制操作的数量。另外，每台 <code>chunkserver</code> 也会限制花在副本复制上的带宽。</p>
<p>最后，主节点会定期检查各 <code>chunkserver</code> 的流量或者磁盘空间利用率是否均衡：如果发现了不均衡则会在各 <code>chunkserver</code> 间移动某些 <code>chunk</code> 以达到磁盘空间利用率或流量的均衡。根据上述的策略，对于一台新加入集群的 <code>chunkserver</code> 来说，主节点会逐渐的将其填满 <code>chunk</code> 而不是马上让其承接大量新的 <code>chunk</code> 然后伴随着大量的写流量。另外，主节点也需要决定从哪台 <code>chunkserver</code> 上移除 <code>chunk</code>，一般的，主节点会优先选择磁盘可用空间率低于平均水平的 <code>chunkserver</code>，从而使得每台 <code>chunkserver</code> 的磁盘空间利用率维持在一个相近的水平。</p>
<h3 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h3><p>当某个文件被删除后，<code>GFS</code> 不会马上释放它所占用的空间，而是在常规的垃圾回收期间在文件和 <code>chunk</code> 层面延迟删除，这种策略使得系统更加简洁和可靠。</p>
<h4 id="机制"><a href="#机制" class="headerlink" title="机制"></a>机制</h4><p>当某个文件被应用删除后，和其他修改一样主节点首先会将该操作记录到操作日志中。不过，<code>GFS</code> 并不会马上删除该文件，而是将该文件重命名为一个隐藏的名字，同时这个隐藏的名字包含了文件删除时的时间戳。在主节点常规扫描系统的命名空间的期间，它会删除命名空间中已经存在了3天（时间间隔可配置）的隐藏文件。在这之前，这个文件依然是可以通过重命名后的名字读取的，或者将其重命名回原来的名字来撤销删除。一旦主节点将其从命名空间中删除，其对应的元数据也会一并删除，从而切断了这个文件和任何 <code>chunk</code> 的关联。</p>
<p>类似的，主节点也会定期扫描所有 <code>chunk</code> 的命名空间，一旦发现没有任何关联的 <code>chunk</code>（即没有被任何文件引用的 <code>chunk</code>），主节点就会删除这些 <code>chunk</code> 的元数据。之后，在主节点和 <code>chunkserver</code> 的心跳监测中，<code>chunkserver</code> 会向主节点反馈其所拥有的 <code>chunk</code>，主节点会返回所有已经不在元数据中的 <code>chunk</code> 标识，<code>chunkserver</code> 收到回复后就可以删除掉这些不需要的 <code>chunk</code>。</p>
<h4 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h4><p>虽然分布式的垃圾回收是个困难的问题，往往需要一套特定编程语言下的复杂解决方案，在 <code>GFS</code> 的实现中却很简单。主节点可以轻易的标识出所有 <code>chunk</code> 的引用，它们全都保存在文件到 <code>chunk</code> 的映射中。同时主节点也可轻易的标识出所有的 <code>chunk</code> 副本，它们都是 <code>chunkserver</code> 上某个指定文件夹下的 <code>Linux</code> 文件。任何一个不在主节点中记录的副本都可以被回收。</p>
<p><code>GFS</code> 采用的延迟垃圾回收的方式相比于立即删除的方式存在以下几个优点：</p>
<ol>
<li>对于组件异常经常发生的大型分布式系统来说，延迟删除更为简洁和可靠。创建 <code>chunk</code> 时有可能在部分 <code>chunkserver</code> 上成功在部分 <code>chunkserver</code> 上失败，这就导致创建 <code>chunk</code> 的这次操作整体是失败的，从而使得主节点不会记录该 <code>chunk</code>，后续也无法知晓这些部分创建成功的 <code>chunk</code>。如果采用立即删除的方式，需要由主节点向 <code>chunkserver</code> 发送删除 <code>chunk</code> 的消息并需要对发送失败的消息进行重发。而延迟垃圾回收的方式提供了一个统一、可靠的方式来删除无用的 <code>chunk</code>。</li>
<li>主节点将 <code>chunk</code> 的删除合并到了主节点的日常维护中进行，例如对命名空间的定期扫描以及主节点和 <code>chunkserver</code> 的心跳监测。这就可以将 <code>chunk</code> 的删除进行批量处理从而摊薄了运行的成本。另外，垃圾回收只会在主节点较为空闲的时候进行，主节点会更优先的响应客户端的请求。</li>
<li>延迟删除给不可逆转的误删除提供了缓冲。</li>
</ol>
<p>在实际应用中，延迟删除的主要缺点是不利于磁盘剩余空间紧张的 <code>chunkserver</code> 释放磁盘空间。尤其是对不停的创建然后删除临时文件的应用来说，不能够马上利用那些不需要的临时文件的空间。<code>GFS</code> 通过加快回收某个重复删除的文件来缓解这个问题。同时，<code>GFS</code> 也允许用户针对不同的命名空间设置不同的副本数量阈值和垃圾回收策略。例如，用户可指定某个目录下的所有文件的 <code>chunk</code> 都不需要额外的副本，以及当文件删除时系统会立即直接删除。</p>
<h3 id="过期的副本检测"><a href="#过期的副本检测" class="headerlink" title="过期的副本检测"></a>过期的副本检测</h3><p>在某个 <code>chunk</code> 所属的 <code>chunkserver</code> 异常下线期间，其他 <code>chunkserver</code> 上的 <code>chunk</code> 发生了修改，则该 <code>chunkserver</code> 再次上线后该 <code>chunk</code> 的数据版本已落后。主节点会记录每一个 <code>chunk</code> 最新的版本号，从而能识别过期的副本。</p>
<p>每当主节点为某一个 <code>chunk</code> 分配一个租约时，它会先更新 <code>chunk</code> 的版本号，并通知所有当前持有最新版本 <code>chunk</code> 的 <code>chunkserver</code> 也更新版本号。主节点和 <code>chunkserver</code> 都会将这个版本号持久化，这里的持久化顺序应该是 <code>chunkserver</code> 先，主节点后，否则主节点先持久化版本号然后此时下线了，再次上线后造成没有任何一个 <code>chunkserver</code> 持有最新的版本号。只有当持久化完成之后，主节点才会将 <code>primary</code> 发送给客户端，之后才能开始写入。如果此时某个 <code>chunkserver</code> 下线了，那么它所持有的 <code>chunk</code> 的版本号就无法更新，当这个 <code>chunkserver</code> 再次上线时，它会向主节点报告所持有的 <code>chunk</code> 以及对应的版本号，主节点就能知道这个 <code>chunkserver</code> 持有了过期的副本。如果主节点发现某个 <code>chunk</code> 的副本的版本号大于自己内存中的版本号，那么主节点就知道自己在更新版本号期间下线了，直接使用 <code>chunkserver</code> 的版本号作为最新的版本号。</p>
<p>主节点会在垃圾回收期间删除过期的副本。在这之前，如果有客户端请求该 <code>chunk</code> 的信息，主节点会忽视持有过期副本的 <code>chunkserver</code>，不会将其返回给客户端。其次，主节点在返回给客户端的 <code>chunk</code> 信息中也包含了版本号，在要求某台 <code>chunkserver</code> 去复制另一台 <code>chunkserver</code> 上的 <code>chunk</code> 数据时也同样附带了版本号，这样客户端或者 <code>chunkserver</code> 在读取 <code>chunk</code> 数据时就能比较读到的数据是否是最近的版本。</p>
<h2 id="容错和诊断"><a href="#容错和诊断" class="headerlink" title="容错和诊断"></a>容错和诊断</h2><p><code>Google</code> 在设计 <code>GFS</code> 遇到的最大问题之一是如何应对频繁的组件异常。各组件的质量和数量决定了异常是经常出现而不是偶尔出现：既不能完全信任机器，也不能完全信任磁盘。组件异常可能造成系统不可用，或者更糟的是数据损坏。本节主要描述 <code>GFS</code> 如何面对这些挑战以及开发了哪些工具来定位问题。</p>
<h3 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h3><p>一个 <code>GFS</code> 集群由数百台机器组成，某些机器必然会在任意时间发生异常。<code>GFS</code> 通过快速恢复和副本这两个手段实现了系统的高可用。</p>
<h4 id="快速恢复"><a href="#快速恢复" class="headerlink" title="快速恢复"></a>快速恢复</h4><p>主节点和 <code>chunkserver</code> 都是设计成不管以什么样的方式终止，都能在数秒内恢复终止前的状态。实际上，<code>GFS</code> 并不会刻意区分正常的终止还是异常的终止，各服务器本身日常就会通过杀死进程来终止。客户端或其他服务器会因此经历一段短暂的中断，因为进行中的请求会因为服务器的终止而超时，继而客户端会对重启后的服务器进行重连，然后重试。</p>
<h4 id="chunk-副本"><a href="#chunk-副本" class="headerlink" title="chunk 副本"></a><code>chunk</code> 副本</h4><p>如之前描述，每个 <code>chunk</code> 会备份到不同机柜上的不同机器。用户可指定不同命名空间配置不同的备份策略。默认每个 <code>chunk</code> 有3个副本。主节点通过 <code>chunkserver</code> 间复制 <code>chunk</code> 来保证每个 <code>chunk</code> 有足够的副本数量，避免 <code>chunkserver</code> 由于下线或者数据损坏造成 <code>chunk</code> 副本数量不足。虽然依靠副本已经能较好的支撑需求，<code>Google</code> 的工程师也在考虑跨服务器的冗余手段如奇偶校验或者 <code>erasure code</code> 来应对日渐增长的只读存储需求。在当前这个松耦合的系统中实现更复杂的冗余策略会更有挑战性，不过也是可控的，因为大部分的文件操作都是追加写和读而不是随机写。</p>
<h4 id="主节点副本"><a href="#主节点副本" class="headerlink" title="主节点副本"></a>主节点副本</h4><p><code>GFS</code> 通过备份主节点的状态来实现可靠性。主节点的操作日志和检查点都会备份到多台机器上。一次文件的修改只有当主节点和所有备份服务器都写入到本地日志之后才认为是已提交的。从实现的简单性考虑，只会有一个主节点机器负责所有的元数据修改以及在后台运行的一些日常维护活动来修改主节点的内部状态，例如垃圾回收。当这个主节点发生异常时，它几乎能马上重启。如果整个机器或者磁盘发生异常，<code>GFS</code> 之外的监控设施能在其他机器上借助操作日志副本重新启动一个新的主节点进程。对于客户端来说不需要有任何改动，因为客户端和主节点的通信是通过主机名进行的（例如 <code>gfs-test</code>），其对应的是一个 <code>DNS</code> 别名，当主节点更换机器时，同步修改别名映射即可。</p>
<p>另外，即使主节点异常了，还有影子主节点也提供对本地文件系统的只读访问功能。因为这个影子主节点不是严格的镜像节点，所以它们的文件系统上的数据可能会略落后于主节点，一般来说是几秒。影子主节点依然能够提供当前未在修改的文件查询功能，或者应用程序不介意查询到一部分过期的数据，例如 <code>chunkserver</code> 列表小于实际运行的数量。因为文件的内容是通过 <code>chunkserver</code> 读取，和主节点无关，所以客户端不会读取到过期的数据。可能过期的数据是元数据，例如文件夹的内容或者权限控制信息。</p>
<p>为了保证自己持续更新，影子主节点会不断从某台操作日志副本机器读取日志，并以和主节点一样的执行顺序将其重放到内存中的数据结构中。和主节点一样，影子主节点在启动时也会拉取 <code>chunkserver</code> 信息来定位 <code>chunk</code> 的副本位置，并定期与其进行心跳监测。它依赖主节点来更新 <code>chunk</code> 副本的位置，如主节点创建和删除副本。</p>
<h3 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h3><p>每台 <code>chunkserver</code> 使用校验和来检测损坏的数据。鉴于一个 <code>GFS</code> 集群有几千个磁盘运行在几百台机器上，磁盘异常经常会导致文件读写时数据损坏或丢失。虽然可以从其他 <code>chunkserver</code> 恢复，但是通过比较多个 <code>chunkserver</code> 上的副本来检测数据损坏显然是不实际的。另外，在一致性模型中提到，每个副本的数据不是每个比特都一致的，由于追加写的原子性保证造成异常重试的存在，每个副本的数据很大可能是不同的。所以，每台 <code>chunkserver</code> 必须能够自己独立通过校验和来检测数据是否损坏。</p>
<p>每个 <code>chunk</code> 切分为每块大小为 <code>64 KB</code> 的数据块，每个数据块对应一个32位的校验和。和其他元数据一样，校验和也保存在内存中，并随日志持久化到本地磁盘中，且和用户数据分开存放。</p>
<p>对于读操作，<code>chunkserver</code> 会根据读取的字节范围先根据校验和校验所覆盖的数据块的数据是否完整，然后再返回数据给客户端或其他的 <code>chunkserver</code>。因此，每台 <code>chunkserver</code> 不会把损坏的数据同步到其他 <code>chunkserver</code> 上。如果 <code>chunkserver</code> 发现某个数据块发生了损坏，则会返回一个异常给调用方，并向主节点报告。调用方收到回复后会重新选择一个 <code>chunkserver</code> 进行读操作，而主节点则会安排这个 <code>chunkserver</code> 从其他 <code>chunkserver</code> 那复制 <code>chunk</code>。当新的 <code>chunk</code> 就绪后，主节点会通知这个 <code>chunkserver</code> 删除损坏的 <code>chunk</code>。</p>
<p>校验和对读操作带来的影响较小，主要有几个方面的原因。大部分的一次读操作只涉及了几个数据块，所以需要读取和验证的校验和数量也较少。另外，客户端在读取时，会尽量保证正好在数据块的边界读取，而不是跨数据块。最后，校验和验证不涉及 <code>IO</code>，以及校验和的计算可以和数据读取 <code>IO</code> 同时进行。</p>
<p>针对追加写，校验和计算对此做了大量的优化，因为追加写是主要的写场景。计算校验和时，<code>GFS</code> 会复用当前最后一个数据块的校验和，一个数据块的大小是 <code>64 KB</code>，可能目前只写入了 <code>30 KB</code>，那么本次往剩下的 <code>34 KB</code> 写入时，则无需从头开始计算这个数据块的校验和。对于新填充的数据块依然还是需要完整计算校验和。即使当前这个写入了 <code>30 KB</code> 的数据块损坏了，计算校验和时无法知晓，最终这个数据块的校验和与保存的数据已经不匹配，这个损坏也会在下一次读操作中被调用方感知。</p>
<p>相反的，对于随机写操作，如果当前的随机写覆盖了某个 <code>chunk</code> 已有的数据，则必须先对所覆盖的数据块进行校验和验证，确保当前的数据未损坏，然后再执行写入和计算新的校验和。如果不进校校验就写入，那么有可能在写入之外的地方已经发生了数据损坏。因为一次随机写可能写不满最后一个数据块，假设这个数据块是损坏的，如果写前校验就能发现这个问题。不过这里并没有理解为什么不能将完整性校验推迟到后续的读操作时，可能是因为追加写和流式读是 <code>GFS</code> 应用的主要方式，完整性校验推迟到读操作也能很快发现问题，而随机写是很少量的场景，写时校验能更快的发现问题。</p>
<p>在系统空闲的时候，<code>chunkserver</code> 会扫描和校验所有不活跃的 <code>chunk</code>，从而能够发现那些不经常被访问但是已损坏的 <code>chunk</code>（因为数据完整性检测发生在读写操作时）。一旦发现这样的 <code>chunk</code>，主节点会创建一个新的 <code>chunk</code> 然后指示 <code>chunkserver</code> 复制一份有效的 <code>chunk</code>，最后再删除损坏的 <code>chunk</code>。这就避免因为一些不活跃但已损坏的副本使得主节点认为这些 <code>chunk</code> 依然满足副本数量阈值的要求。</p>
<h3 id="诊断工具"><a href="#诊断工具" class="headerlink" title="诊断工具"></a>诊断工具</h3><p>通过详尽的诊断日志能有效的帮助问题隔离，调试，性能分析，而且带来的成本较小。如果没有日志，则很难理解各机器间无法复现的交互。针对某些重要的事件（如 <code>chunkserver</code> 的下线和上线） <code>GFS</code> 会生成相应的诊断日志，同时还会记录所有的 <code>RPC</code> 请求和响应。诊断日志的删除对系统的正确性没有任何影响。然而在磁盘空间允许的情况下，还是会尽可能保留多的诊断日志。</p>
<p>除了正在读取或写入的文件，诊断日志记录了其他所有 <code>RPC</code> 的请求和响应。通过比对各机器上的 <code>RPC</code> 日志，就能重现当时系统交互的场景，从而进行问题定位。同时这些日志也能辅助压力测试和性能分析。</p>
<p>诊断日志带来的性能影响很小（在其带来的价值面前不值一提），因为采用了异步顺序的方式写入日志。同时，最近某段时间内的事件也会保存在内存中用于线上的持续监控。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/papers/gfs.pdf">The Google File System</a></li>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/notes/l-gfs.txt">6.824 2022 Lecture 3: GFS</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/20485173">GFS，一致性模型里，“已定义”和“不一致”具体表示的什么含义？</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://frederick-s.github.io/2022/04/10/mit-6.824-lab1-implementation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaodan Mao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Übung macht den Meister">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/10/mit-6.824-lab1-implementation/" class="post-title-link" itemprop="url">MIT 6.824 - Lab 1 (5): 实现</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-10 00:00:00" itemprop="dateCreated datePublished" datetime="2022-04-10T00:00:00+08:00">2022-04-10</time>
            </span>

          
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>3.1k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p><code>Lab 1</code> 需要我们实现一个单机多线程、多进程的 <code>MapReduce</code> 程序，通过 <code>test-mr.sh</code> 可以看到该实验会先启动一个主节点进程，然后再启动多个工作节点进程。</p>
<h2 id="主节点"><a href="#主节点" class="headerlink" title="主节点"></a>主节点</h2><p>主节点的入口是 <code>mrcoordinator.go</code>，通过 <code>go run mrcoordinator.go pg*.txt</code> 可运行一个主节点程序，其中 <code>pg*.txt</code> 是本次 <code>MapReduce</code> 程序的输入数据。同时，主节点和工作节点间通过 <code>RPC</code> 进行交互，本实验需要我们实现以下 <code>RPC</code> 接口：</p>
<ol>
<li>实验中主节点的任务分发采用的是拉模式，工作节点会定期向主节点请求一个任务，这个任务可以是 <code>map</code> 任务也可以是 <code>reduce</code> 任务，由主节点根据当前任务的状态决定应该推送 <code>map</code> 任务还是 <code>reduce</code> 任务。由于 <code>reduce</code> 任务会不间断的拉取中间结果数据，这里为了方便处理，当某个工作节点正在处理 <code>reduce</code> 任务时，在向主节点请求任务时可指明要求分发对应的 <code>reduce</code> 任务</li>
<li><code>map</code> 任务完成后需要通知主节点生成的中间结果文件地址，主节点收到请求后需要将中间结果文件地址保存到内部数据结构中，供后续发送给对应 <code>reduce</code> 任务</li>
<li><code>reduce</code> 任务完成后同样需要通知主节点任务完成，否则主节点无法知晓所有 <code>reduce</code> 任务是否已完成，从而无法退出主节点</li>
</ol>
<h3 id="任务管理"><a href="#任务管理" class="headerlink" title="任务管理"></a>任务管理</h3><p>主节点需要维护所有 <code>map</code> 任务和 <code>reduce</code> 任务的状态，由于工作节点有可能失败，所以主节点需要同时记录每个任务已执行了多久，如果超过一定时间还没有收到任务完成的通知，则认为执行这个任务的工作节点已失联，然后需要将该任务重新分配给其他工作节点，在本实验中主节点等待每个任务完成的时间为10秒。</p>
<h2 id="工作节点"><a href="#工作节点" class="headerlink" title="工作节点"></a>工作节点</h2><p>工作节点的入口是 <code>mrworker.go</code>，通过 <code>go run mrworker.go wc.so</code> 可运行一个工作节点程序，其中 <code>wc.so</code> 是本次 <code>MapReduce</code> 程序用到的用户自定义 <code>map</code> 和 <code>reduce</code> 函数。工作节点只有一个 <code>Worker</code> 主方法，需要不断向主节点轮询请求任务，那么工作节点什么时候结束轮询？根据实验建议有两种方法，一种是当本次 <code>MapReduce</code> 任务完成后，主节点进程退出，当工作节点再次请求主节点任务时，<code>RPC</code> 请求必然失败，此时工作节点可认为本次任务已完成主节点已退出，从而结束轮询并退出；另一种是当主节点完成后，在一定时间内，在收到工作节点新的任务请求时，返回一个要求工作节点退出的标识（或者也可抽象为一种任务类型），工作节点收到 <code>RPC</code> 响应后退出，主节点在等待时间到了之后也进行退出。</p>
<h3 id="空任务"><a href="#空任务" class="headerlink" title="空任务"></a>空任务</h3><p>当工作节点向主节点申请任务，而此时主节点没有可分发的任务时，主节点可返回一个自定义任务类型的任务，来表示空任务，工作节点收到响应后则直接睡眠等待下次唤醒。</p>
<h3 id="map-任务"><a href="#map-任务" class="headerlink" title="map 任务"></a>map 任务</h3><p><code>map</code> 任务负责调用用户自定义的 <code>map</code> 函数，生成一组中间结果数据，然后将中间结果数据保存为文件，实验建议的文件名是 <code>mr-X-Y</code>，其中 <code>X</code> 表示 <code>map</code> 任务的编号，<code>Y</code> 表示 <code>reduce</code> 任务的编号，<code>map</code> 任务编号的范围可简单使用 <code>[1, 输入文件的数量]</code> 来表示，<code>reduce</code> 任务编号的范围为 <code>[1, nReduce]</code>。每个 <code>map</code> 任务会生成 <code>R</code> 个中间结果文件，实验已提供了分片函数，对中间结果的每个键使用 <code>ihash(key) % nReduce</code> 来决定写入到哪个中间结果文件中。</p>
<p>从数据结构角度来说，所有的中间结果数据是一个 <code>nReduce * nMap</code> 的二维矩阵，每一行对应一个 <code>reduce</code> 任务。</p>
<p>由于 <code>map</code> 节点有可能执行失败，为避免 <code>reduce</code> 节点读取到的是未写入完成的中间结果文件，在写入中间结果文件时可以先写入一个临时文件，在写入完成后再重命名为最终的文件名。在本实验中，可以使用 <code>ioutil.TempFile</code> 来创建临时文件，以及使用 <code>os.Rename</code> 来原子性的重命名文件。</p>
<p>在当前 <code>map</code> 任务的中间结果文件写入完成后，需要通过 <code>RPC</code> 请求通知主节点所有中间结果文件的文件地址。在本实验中，各个工作节点运行在同一台机器上，实验要求保存 <code>map</code> 任务的中间结果文件到当前文件夹，这样 <code>reduce</code> 任务就能通过中间结果文件的文件名读取中间结果数据。</p>
<p>对中间结果数据的写文件可以借助 <code>Go</code> 的 <code>encoding/json</code> 模块，例如：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">enc := json.NewEncoder(file)</span><br><span class="line"><span class="keyword">for</span> _, kv := ... &#123;</span><br><span class="line">    err := enc.Encode(&amp;kv)</span><br></pre></td></tr></table></figure>

<p>然后 <code>reduce</code> 任务读取文件：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dec := json.NewDecoder(file)</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">    <span class="keyword">var</span> kv KeyValue</span><br><span class="line">    <span class="keyword">if</span> err := dec.Decode(&amp;kv); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    &#125;</span><br><span class="line">    kva = <span class="built_in">append</span>(kva, kv)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="reduce-任务"><a href="#reduce-任务" class="headerlink" title="reduce 任务"></a>reduce 任务</h3><p><code>reduce</code> 任务负责将所有中间结果数据按照键排序后应用用户自定义的 <code>reduce</code> 函数，并生成最终输出文件，文件格式为 <code>mr-out-X</code>，其中 <code>X</code> 表示 <code>reduce</code> 任务的编号。文件中的每一行对应一个 <code>reduce</code> 函数调用的结果，需要按照 <code>Go</code> 的 <code>%v %v</code> 形式对键值对进行格式化。</p>
<p>在收到所有的中间结果数据之前，<code>reduce</code> 任务无法开始执行，所以在这个期间当工作节点请求 <code>reduce</code> 任务时，如果主节点暂时没有新的 <code>reduce</code> 任务可分发，则可返回一个空任务，工作节点收到响应后则暂时等待一段时间再重新请求任务。<code>reduce</code> 每次收到中间结果数据后会暂存在内存中，如果暂存的中间结果数据的数量等于 <code>map</code> 任务的数量（这个值可以放在 <code>RPC</code> 响应中），则说明所有中间结果数据已经接收完毕，可以开始执行 <code>reduce</code> 任务。</p>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p><code>mrworker.go</code> 中注释描述通过 <code>go run mrworker.go wc.so</code> 来运行工作节点，不过如果构建 <code>wc.so</code> 时开启了竞争检测（<code>-race</code>），则运行 <code>mrworker.go</code> 时也同样需要开启竞争检测，否则会提示 <code>cannot load plugin wc.so</code>，如果打印 <code>err</code> 则会显示 <code>plugin.Open(&quot;wc&quot;): plugin was built with a different version of package runtime</code>。</p>
<p>同样的，如果使用 <code>GoLand</code> 调试程序，由于在调试模式下 <code>GoLand</code> 编译 <code>Go</code> 文件时会增加 <code>-gcflags=&quot;all=-N -l&quot;</code> 参数，所以也需要在打包 <code>wc.so</code> 时增加相同的参数。</p>
<h2 id="活锁"><a href="#活锁" class="headerlink" title="活锁"></a>活锁</h2><p>在最后的 <code>crash test</code> 遇到个类似活锁的问题，在前文提到，如果某个工作节点开始了 <code>reduce</code> 任务但是还没有接收全部的中间结果数据，则该节点下次申请任务时会继续申请该 <code>reduce</code> 任务（普通工作节点对申请的任务类型没有要求）。在 <code>crash test</code> 下，工作节点在执行用户自定义的 <code>map</code> 或 <code>reduce</code> 函数时有一定概率结束进程，假设现在有4个工作节点，其中一个在执行 <code>map</code> 任务，另外三个各自在执行 <code>reduce</code> 任务（轮询获取中间结果数据），不幸的是<br>这个时候 <code>map</code> 节点挂了，此时 <code>test-mr.sh</code> 会自动再启动一个工作节点，然后更不幸的是挂掉的 <code>map</code> 任务在主节点看来还没有到超时时间，所以主节点此时不会分配 <code>map</code> 任务给新的节点（假设没有其他 <code>map</code> 任务），会再分配一个 <code>reduce</code> 任务给新的节点，至此所有工作节点都在执行 <code>reduce</code> 任务，又都在等待中间结果数据完成，却又不可能完成。</p>
<p>造成这个问题的主要原因是任务分配顺序，上述问题下的任务分配顺序是：</p>
<ol>
<li>指定的 <code>reduce</code> 任务</li>
<li>空闲或超时的 <code>map</code> 任务</li>
<li>空闲或超时的 <code>reduce</code> 任务</li>
</ol>
<p>解决方法就是把前两个换下顺序即可，即：</p>
<ol>
<li>空闲或超时的 <code>map</code> 任务</li>
<li>指定的 <code>reduce</code> 任务</li>
<li>空闲或超时的 <code>reduce</code> 任务</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/labs/lab-mr.html">6.824 Lab 1: MapReduce</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://frederick-s.github.io/2022/04/04/mit-6.824-lab1-go-rpc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaodan Mao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Übung macht den Meister">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/04/mit-6.824-lab1-go-rpc/" class="post-title-link" itemprop="url">MIT 6.824 - Lab 1 (4): Go RPC</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-04 00:00:00" itemprop="dateCreated datePublished" datetime="2022-04-04T00:00:00+08:00">2022-04-04</time>
            </span>

          
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>2.8k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><code>Lab 1</code> 虽然是个单机器多线程、多进程的程序，但主节点和工作节点的交互依然通过 <code>RPC</code> 实现，<code>Go</code> 本身也提供了开箱即用的 <code>RPC</code> 功能，下面将通过一个简单的求和服务来了解在 <code>Go</code> 中如何实现一个 <code>RPC</code> 服务。</p>
<h2 id="定义请求体和响应体"><a href="#定义请求体和响应体" class="headerlink" title="定义请求体和响应体"></a>定义请求体和响应体</h2><p>请求体和响应体都非常简单，<code>SumRequest</code> 中包含要求和的两个数字，<code>SumReply</code> 中存放求和的结果：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> pb</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> SumRequest <span class="keyword">struct</span> &#123;</span><br><span class="line">	A <span class="keyword">int</span></span><br><span class="line">	B <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> SumReply <span class="keyword">struct</span> &#123;</span><br><span class="line">	Result <span class="keyword">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h2><p>首先定义服务类 <code>SumService</code> 和提供的方法：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> SumService <span class="keyword">struct</span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(sumService *SumService)</span> <span class="title">Sum</span><span class="params">(sumRequest *pb.SumRequest, sumReplay *pb.SumReply)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	sumReplay.Result = sumRequest.A + sumRequest.B</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>SumService</code> 只有一个 <code>Sum</code> 方法，接收 <code>SumRequest</code> 和 <code>SumReply</code> 两个参数，求和后将结果放回到 <code>SumReply</code> 中（<code>Sum</code> 的方法签名必须是这样的形式，即两个入参和一个 <code>error</code> 类型的出参，具体规则见下文描述）。</p>
<p>然后进行服务注册：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sumService := &amp;SumService&#123;&#125;</span><br><span class="line">rpc.Register(sumService)</span><br><span class="line">rpc.HandleHTTP()</span><br></pre></td></tr></table></figure>

<p>通过 <code>rpc.Register</code> 这个方法可以知道，一个服务类及其提供的方法必须满足以下条件才能注册成功：</p>
<ol>
<li>服务类必须是公共的</li>
<li>服务类提供的方法必须是公共的</li>
<li>服务类提供的方法入参必须是两个，一个表示请求，一个表示响应（从编码的角度来说方法入参是两个，但是实际代码是判断是否等于3个，因为在这种场景下定义的方法的第一个入参类似于 <code>Java</code> 中的 <code>this</code>）</li>
<li>服务类提供的方法的第一个参数类型必须是公共的或者是 <code>Go</code> 内置的数据类型</li>
<li>服务类提供的方法的第二个参数类型也必须是公共的或者是 <code>Go</code> 内置的数据类型，且必须是指针类型</li>
<li>服务类提供的方法的出参个数只能是1个</li>
<li>服务类提供的方法的出参类型必须是 <code>error</code></li>
</ol>
<p>而 <code>rpc.HandleHTTP()</code> 表示通过 <code>HTTP</code> 作为客户端和服务端间的通信协议，当客户端发起一个 <code>RPC</code> 调用时，本质上是将要调用的方法和参数包装成一个 <code>HTTP</code> 请求，服务端收到 <code>HTTP</code> 请求后，解码出要调用的本地方法名称和入参，然后调用本地方法，在本地方法调用完成后再将结果写入到 <code>HTTP</code> 响应中，客户端收到响应后，再解析出远程调用的结果。</p>
<p><code>rpc.HandleHTTP()</code> 本质上是个 <code>HTTP</code> 路由注册，实际上是调用 <code>Handle(pattern string, handler Handler)</code> 方法，当请求路由匹配 <code>pattern</code> 时，会调用对应的 <code>handler</code> 执行，对于 <code>Go RPC</code> 来说，固定路由路径是 <code>/_goRPC_</code>。</p>
<p>所以，在完成 <code>HTTP</code> 路由注册后，还需要配合开启一个 <code>HTTP</code> 服务，这样才能接受远程服务调用：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">listener, err := net.Listen(<span class="string">&quot;tcp&quot;</span>, <span class="string">&quot;:1234&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">	log.Fatal(<span class="string">&quot;listen error:&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fmt.Println(<span class="string">&quot;Listening on port 1234&quot;</span>)</span><br><span class="line">http.Serve(listener, <span class="literal">nil</span>)</span><br></pre></td></tr></table></figure>

<p><code>http.Serve</code> 方法中对于每一个客户端的连接，最终会分配一个 <code>goroutine</code> 来调用 <code>Handler</code> 的 <code>ServeHTTP(ResponseWriter, *Request)</code> 方法来处理，对于 <code>Go</code> 的 <code>RPC</code> 包来说，则可以实现该方法来处理 <code>RPC</code> 请求：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ServeHTTP implements an http.Handler that answers RPC requests.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(server *Server)</span> <span class="title">ServeHTTP</span><span class="params">(w http.ResponseWriter, req *http.Request)</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> req.Method != <span class="string">&quot;CONNECT&quot;</span> &#123;</span><br><span class="line">		w.Header().Set(<span class="string">&quot;Content-Type&quot;</span>, <span class="string">&quot;text/plain; charset=utf-8&quot;</span>)</span><br><span class="line">		w.WriteHeader(http.StatusMethodNotAllowed)</span><br><span class="line">		io.WriteString(w, <span class="string">&quot;405 must CONNECT\n&quot;</span>)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line">	conn, _, err := w.(http.Hijacker).Hijack()</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		log.Print(<span class="string">&quot;rpc hijacking &quot;</span>, req.RemoteAddr, <span class="string">&quot;: &quot;</span>, err.Error())</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line">	io.WriteString(conn, <span class="string">&quot;HTTP/1.0 &quot;</span>+connected+<span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line">	server.ServeConn(conn)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h2><p>对于客户端来说，发起远程方法调用前需要先和服务端建立连接：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">client, err := rpc.DialHTTP(<span class="string">&quot;tcp&quot;</span>, <span class="string">&quot;:1234&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">	log.Fatal(<span class="string">&quot;dialing:&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">defer</span> client.Close()</span><br></pre></td></tr></table></figure>

<p>该方法同时返回了一个 <code>RPC</code> 客户端类，内部同时负责对 <code>RPC</code> 请求的编码和解码。</p>
<p>然后通过 <code>client.Call</code> 来发起远程调用：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sumRequest := &amp;pb.SumRequest&#123;</span><br><span class="line">	A: <span class="number">1</span>,</span><br><span class="line">	B: <span class="number">2</span>,</span><br><span class="line">&#125;</span><br><span class="line">sumReply := &amp;pb.SumReply&#123;&#125;</span><br><span class="line">err = client.Call(<span class="string">&quot;SumService.Sum&quot;</span>, sumRequest, sumReply)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">	log.Fatal(<span class="string">&quot;call error:&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fmt.Println(<span class="string">&quot;Result:&quot;</span>, sumReply.Result)</span><br></pre></td></tr></table></figure>

<p>这里的调用一共有三个参数，第一个是被调用的远程方法名，需要是 <code>类名.方法名</code> 的形式，后两个则是远程方法的约定入参。</p>
<p>完整的代码可参考 <a target="_blank" rel="noopener" href="https://github.com/Frederick-S/go-rpc-demo">go-rpc-demo</a>。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/labs/lab-mr.html">6.824 Lab 1: MapReduce</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://frederick-s.github.io/2022/03/27/mit-6.824-lab1-sequential-mapreduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaodan Mao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Übung macht den Meister">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/27/mit-6.824-lab1-sequential-mapreduce/" class="post-title-link" itemprop="url">MIT 6.824 - Lab 1 (3): 示例程序</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-27 00:00:00" itemprop="dateCreated datePublished" datetime="2022-03-27T00:00:00+08:00">2022-03-27</time>
            </span>

          
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>2.9k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><code>Lab 1</code> 提供了一个串行化的示例 <code>MapReduce</code> 程序，整体分两部分，第一部分是用户自定义的 <code>map</code> 和 <code>reduce</code> 函数，第二部分是框架代码。</p>
<h2 id="用户自定义-map-和-reduce-函数"><a href="#用户自定义-map-和-reduce-函数" class="headerlink" title="用户自定义 map 和 reduce 函数"></a>用户自定义 map 和 reduce 函数</h2><p>以单词计数应用 <code>wc.go</code> 为例，对于 <code>map</code> 函数来说，它的输入键值对类型为 <code>&lt;string, string&gt;</code>，中间结果数据类型为框架定义的 <code>KeyValue</code> 类型，本质上也是个 <code>&lt;string, string&gt;</code> 类型。<code>map</code> 函数首先将文件内容拆分为单词，然后遍历每个单词，输出对应中间结果数据 <code>&lt;w, &quot;1&quot;&gt;</code>：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// The map function is called once for each file of input. The first</span></span><br><span class="line"><span class="comment">// argument is the name of the input file, and the second is the</span></span><br><span class="line"><span class="comment">// file&#x27;s complete contents. You should ignore the input file name,</span></span><br><span class="line"><span class="comment">// and look only at the contents argument. The return value is a slice</span></span><br><span class="line"><span class="comment">// of key/value pairs.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Map</span><span class="params">(filename <span class="keyword">string</span>, contents <span class="keyword">string</span>)</span> []<span class="title">mr</span>.<span class="title">KeyValue</span></span> &#123;</span><br><span class="line">	<span class="comment">// function to detect word separators.</span></span><br><span class="line">	ff := <span class="function"><span class="keyword">func</span><span class="params">(r <span class="keyword">rune</span>)</span> <span class="title">bool</span></span> &#123; <span class="keyword">return</span> !unicode.IsLetter(r) &#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// split contents into an array of words.</span></span><br><span class="line">	words := strings.FieldsFunc(contents, ff)</span><br><span class="line"></span><br><span class="line">	kva := []mr.KeyValue&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> _, w := <span class="keyword">range</span> words &#123;</span><br><span class="line">		kv := mr.KeyValue&#123;w, <span class="string">&quot;1&quot;</span>&#125;</span><br><span class="line">		kva = <span class="built_in">append</span>(kva, kv)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> kva</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>reduce</code> 函数的输出类型为 <code>string</code>，其逻辑较为简单，中间结果数组的长度就是单词的个数：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// The reduce function is called once for each key generated by the</span></span><br><span class="line"><span class="comment">// map tasks, with a list of all the values created for that key by</span></span><br><span class="line"><span class="comment">// any map task.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Reduce</span><span class="params">(key <span class="keyword">string</span>, values []<span class="keyword">string</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	<span class="comment">// return the number of occurrences of this word.</span></span><br><span class="line">	<span class="keyword">return</span> strconv.Itoa(<span class="built_in">len</span>(values))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="框架代码"><a href="#框架代码" class="headerlink" title="框架代码"></a>框架代码</h2><p>我们可以通过 <code>go run -race mrsequential.go wc.so pg*.txt</code> 来运行串行化的 <code>MapReduce</code> 程序，这里的 <code>wc.so</code> 内包含了用户自定义的 <code>map</code> 和 <code>reduce</code> 函数，<code>pg*.txt</code> 则是本次 <code>MapReduce</code> 程序的原始输入数据。</p>
<p>首先，根据入参提供的插件找到用户自定义的 <code>map</code> 和 <code>reduce</code> 函数：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mapf, reducef := loadPlugin(os.Args[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>接着，依次读取输入文件的内容，并调用用户自定义的 <code>map</code> 函数，生成一组中间结果数据：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// read each input file,</span></span><br><span class="line"><span class="comment">// pass it to Map,</span></span><br><span class="line"><span class="comment">// accumulate the intermediate Map output.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">intermediate := []mr.KeyValue&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> _, filename := <span class="keyword">range</span> os.Args[<span class="number">2</span>:] &#123;</span><br><span class="line">	file, err := os.Open(filename)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		log.Fatalf(<span class="string">&quot;cannot open %v&quot;</span>, filename)</span><br><span class="line">	&#125;</span><br><span class="line">	content, err := ioutil.ReadAll(file)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		log.Fatalf(<span class="string">&quot;cannot read %v&quot;</span>, filename)</span><br><span class="line">	&#125;</span><br><span class="line">	file.Close()</span><br><span class="line">	kva := mapf(filename, <span class="keyword">string</span>(content))</span><br><span class="line">	intermediate = <span class="built_in">append</span>(intermediate, kva...)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后，对所有中间结果数据按照键进行排序，而在 <code>MapReduce</code> 论文中，中间结果数据会经过分片函数分发给不同的 <code>reduce</code> 节点，由 <code>reduce</code> 节点自行排序处理：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// a big difference from real MapReduce is that all the</span></span><br><span class="line"><span class="comment">// intermediate data is in one place, intermediate[],</span></span><br><span class="line"><span class="comment">// rather than being partitioned into NxM buckets.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"></span><br><span class="line">sort.Sort(ByKey(intermediate))</span><br></pre></td></tr></table></figure>

<p>最后，遍历所有中间结果数据，对同键的中间结果数据调用用户自定义的 <code>reduce</code> 函数，并将结果写入到最终输出文件中，同样的，这里也只有一个最终输出文件而不是多个：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">oname := <span class="string">&quot;mr-out-0&quot;</span></span><br><span class="line">ofile, _ := os.Create(oname)</span><br><span class="line"></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// call Reduce on each distinct key in intermediate[],</span></span><br><span class="line"><span class="comment">// and print the result to mr-out-0.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">i := <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i &lt; <span class="built_in">len</span>(intermediate) &#123;</span><br><span class="line">	j := i + <span class="number">1</span></span><br><span class="line">	<span class="keyword">for</span> j &lt; <span class="built_in">len</span>(intermediate) &amp;&amp; intermediate[j].Key == intermediate[i].Key &#123;</span><br><span class="line">		j++</span><br><span class="line">	&#125;</span><br><span class="line">	values := []<span class="keyword">string</span>&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> k := i; k &lt; j; k++ &#123;</span><br><span class="line">		values = <span class="built_in">append</span>(values, intermediate[k].Value)</span><br><span class="line">	&#125;</span><br><span class="line">	output := reducef(intermediate[i].Key, values)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// this is the correct format for each line of Reduce output.</span></span><br><span class="line">	fmt.Fprintf(ofile, <span class="string">&quot;%v %v\n&quot;</span>, intermediate[i].Key, output)</span><br><span class="line"></span><br><span class="line">	i = j</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ofile.Close()</span><br></pre></td></tr></table></figure>

<p>至此，一个串行化的 <code>MapReduce</code> 程序就完成了。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/6.824/labs/lab-mr.html">6.824 Lab 1: MapReduce</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://frederick-s.github.io/2022/03/26/mit-6.824-lab1-go-buildmode-plugin/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaodan Mao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Übung macht den Meister">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/26/mit-6.824-lab1-go-buildmode-plugin/" class="post-title-link" itemprop="url">MIT 6.824 - Lab 1 (2): Go buildmode</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-26 00:00:00" itemprop="dateCreated datePublished" datetime="2022-03-26T00:00:00+08:00">2022-03-26</time>
            </span>

          
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>915</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><code>Lab 1</code> 中遇到的第一个命令是 <code>go build -race -buildmode=plugin ../mrapps/wc.go</code>，其中 <code>-buildmode=plugin</code> 表示以插件的形式打包源文件，这里的 <code>wc.go</code> 是用户实现的 <code>map</code> 和 <code>reduce</code> 方法，这体现了面向接口编程的思想，只要用户编写的 <code>map</code> 和 <code>reduce</code> 方法遵循统一的签名，则可以在不重新编译 <code>MapReduce</code> 框架代码的情况下，实时替换运行不同的用户应用。</p>
<p>假设有个 <code>sum.go</code> 文件，里面只有一个 <code>Sum</code> 方法：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Sum</span><span class="params">(a <span class="keyword">int</span>, b <span class="keyword">int</span>)</span> <span class="title">int</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> a + b</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对 <code>sum.go</code> 以插件形式编译：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go build -buildmode&#x3D;plugin sum.go</span><br></pre></td></tr></table></figure>

<p>会生成一个 <code>sum.so</code> 文件。</p>
<p>接着，在 <code>main.go</code> 中就可以通过 <code>plugin.Open</code> 读取 <code>sum.so</code>：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;log&quot;</span></span><br><span class="line">	<span class="string">&quot;plugin&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fileName := <span class="string">&quot;sum.so&quot;</span></span><br><span class="line">	p, err := plugin.Open(fileName)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		log.Fatalf(<span class="string">&quot;cannot load plugin %v&quot;</span>, fileName)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	sumSymbol, err := p.Lookup(<span class="string">&quot;Sum&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		log.Fatalf(<span class="string">&quot;cannot find Map in %v&quot;</span>, fileName)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	sum := sumSymbol.(<span class="function"><span class="keyword">func</span><span class="params">(<span class="keyword">int</span>, <span class="keyword">int</span>)</span> <span class="title">int</span>)</span></span><br><span class="line"></span><br><span class="line">	fmt.Println(<span class="string">&quot;1 + 2 is&quot;</span>, sum(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后通过 <code>Lookup</code> 根据方法名找到 <code>Sum</code> 方法，按照指定方法签名转换后即可进行调用。而如果需要换一个 <code>Sum</code> 的实现，则无需重新编译 <code>main.go</code>。</p>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://pkg.go.dev/cmd/go#hdr-Build_modes">Build modes</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Xiaodan Mao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">41</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiaodan Mao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
